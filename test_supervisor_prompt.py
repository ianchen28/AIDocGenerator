#!/usr/bin/env python3
"""
æµ‹è¯• supervisor_router çš„ exact prompt
"""

import sys
import os

# æ·»åŠ  service ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'service'))

from service.core.env_loader import setup_environment
from service.src.doc_agent.llm_clients import get_llm_client


def test_supervisor_prompt():
    """æµ‹è¯• supervisor_router çš„ exact prompt"""
    print("ğŸ” å¼€å§‹æµ‹è¯• supervisor_router çš„ exact prompt...")

    # è®¾ç½®ç¯å¢ƒ
    setup_environment()

    # åˆ›å»ºå®¢æˆ·ç«¯
    llm_client = get_llm_client("qwen_2_5_235b_a22b")

    # æ¨¡æ‹Ÿ supervisor_router çš„ prompt
    topic = "äººå·¥æ™ºèƒ½åœ¨ä¸­å›½ç”µåŠ›è¡Œä¸šçš„åº”ç”¨è¶‹åŠ¿å’Œæ”¿ç­–æ”¯æŒ"
    gathered_data = "=== æœç´¢æŸ¥è¯¢ 1: äººå·¥æ™ºèƒ½ ç”µåŠ›è¡Œä¸š åº”ç”¨è¶‹åŠ¿ ä¸­å›½ ===\n\nçŸ¥è¯†åº“æœç´¢ç»“æœ:\næœç´¢æŸ¥è¯¢: äººå·¥æ™ºèƒ½ ç”µåŠ›è¡Œä¸š åº”ç”¨è¶‹åŠ¿ ä¸­å›½\næ‰¾åˆ° 75 ä¸ªç›¸å…³æ–‡æ¡£:\n\n1. [standard_index_base] æ™ºæ…§åŸå¸‚ äººå·¥æ™ºèƒ½æŠ€æœ¯åº”ç”¨åœºæ™¯åˆ†ç±»æŒ‡å—\n   è¯„åˆ†: 1.595\n   åŸå§‹å†…å®¹: *7.6 æ™ºæ…§èƒ½æº äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨æ™ºæ…§èƒ½æºé¢†åŸŸçš„åº”ç”¨ä¸»è¦æ˜¯ä¸ºèƒ½æºå…¨å‘¨æœŸä¾›åº”é“¾æä¾›æ™ºèƒ½åŒ–æœåŠ¡, åŒ…æ‹¬ç”Ÿäº§ã€æœåŠ¡å’Œç®¡ç†çš„æ™ºèƒ½åŒ–ç­‰,åº”ç”¨åœºæ™¯åˆ†ç±»åŠæè¿°è§è¡¨ 13 ã€‚ *è¡¨ 13 æ™ºæ…§èƒ½æºåº”ç”¨åœºæ™¯åˆ†ç±»åŠæè¿°" + "è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„ç ”ç©¶èµ„æ–™å†…å®¹ã€‚" * 2000  # æ¨¡æ‹Ÿé•¿æ•°æ®

    prompt = f"""**è§’è‰²ï¼š** ä½ æ˜¯ä¸€ä½ç ”ç©¶ä¸»ç®¡ï¼Œéœ€è¦åˆ¤æ–­ä¸‹æ–¹èµ„æ–™æ˜¯å¦è¶³å¤Ÿæ’°å†™å®Œæ•´æ–‡æ¡£ã€‚

**ä¸»é¢˜ï¼š**ã€Œ{topic}ã€

**å·²æ”¶é›†çš„ç ”ç©¶èµ„æ–™ï¼š**
{gathered_data}

**è¯„åˆ¤æ ‡å‡†ï¼ˆå®½æ¾ç‰ˆï¼‰ï¼š**
- å¦‚æœèµ„æ–™åŒ…å«3æ¡æˆ–ä»¥ä¸Šè¯¦ç»†æ£€ç´¢ç»“æœï¼Œæ¯æ¡éƒ½æœ‰å…·ä½“å†…å®¹ï¼Œä¸”æ€»å­—æ•°è¶…è¿‡500å­—ï¼Œåˆ™è§†ä¸º"å……è¶³"
- å¦‚æœåªæœ‰1-2æ¡æ£€ç´¢ï¼Œæˆ–å†…å®¹è¿‡äºç®€å•ï¼ˆåªæœ‰å®šä¹‰ï¼‰ï¼Œåˆ™è§†ä¸º"ä¸å……è¶³"

**é‡è¦æç¤ºï¼š**
- åªè¦èµ„æ–™å†…å®¹ä¸°å¯Œã€æœ‰å¤šä¸ªæ–¹é¢ã€æœ‰å…·ä½“ä¿¡æ¯ï¼Œå°±åº”è¯¥è¿”å›FINISH
- ä¸è¦è¿‡äºè‹›åˆ»ï¼Œèµ„æ–™ä¸éœ€è¦å®Œç¾æ— ç¼º
- é‡ç‚¹çœ‹æ˜¯å¦æœ‰è¶³å¤Ÿçš„å†…å®¹æ¥å†™æ–‡æ¡£

**ä½ çš„å†³ç­–ï¼š**
è¯·æ ¹æ®ä¸Šè¿°æ ‡å‡†åˆ¤æ–­ï¼Œå½“å‰èµ„æ–™èƒ½å¦ç”¨äºæ’°å†™æ–‡æ¡£ï¼Ÿåªèƒ½å›ç­”ä¸€ä¸ªå•è¯ï¼š
- "FINISH" = èµ„æ–™å……è¶³ï¼Œå¯ä»¥å†™æ–‡æ¡£
- "CONTINUE" = èµ„æ–™ä¸è¶³ï¼Œéœ€è¦æ›´å¤šç ”ç©¶

è¯·ç›´æ¥è¾“å‡ºï¼šFINISH æˆ– CONTINUE"""

    print(f"ğŸ“‹ Prompt ä¿¡æ¯:")
    print(f"   Topic: {topic}")
    print(f"   Gathered data é•¿åº¦: {len(gathered_data)} å­—ç¬¦")
    print(f"   Total prompt é•¿åº¦: {len(prompt)} å­—ç¬¦")
    print(f"   Prompt é¢„è§ˆ: {prompt[:200]}...")

    # æµ‹è¯•ä¸åŒçš„ max_tokens å€¼
    max_tokens_values = [10, 100, 1000]

    for max_tokens in max_tokens_values:
        print(f"\nğŸ§ª æµ‹è¯• max_tokens={max_tokens}:")
        try:
            response = llm_client.invoke(prompt,
                                         temperature=0,
                                         max_tokens=max_tokens)
            print(f"âœ… æˆåŠŸ: {response}")
        except Exception as e:
            print(f"âŒ å¤±è´¥: {e}")
            # å¦‚æœæ˜¯ 400 é”™è¯¯ï¼Œå°è¯•å‡å°‘ prompt é•¿åº¦
            if "400" in str(e):
                print("ğŸ’¡ å°è¯•å‡å°‘ prompt é•¿åº¦...")
                shorter_prompt = prompt[:5000] + "\n\nè¯·ç›´æ¥è¾“å‡ºï¼šFINISH æˆ– CONTINUE"
                try:
                    response = llm_client.invoke(shorter_prompt,
                                                 temperature=0,
                                                 max_tokens=max_tokens)
                    print(f"âœ… çŸ­ prompt æˆåŠŸ: {response}")
                except Exception as e2:
                    print(f"âŒ çŸ­ prompt ä¹Ÿå¤±è´¥: {e2}")


if __name__ == "__main__":
    test_supervisor_prompt()
