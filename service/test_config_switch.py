#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é…ç½®åˆ‡æ¢æµ‹è¯•è„šæœ¬
æ¼”ç¤ºå¦‚ä½•ä¿®æ”¹é…ç½®æ¥ä½¿ç”¨ä¸åŒçš„LLMæœåŠ¡
"""

import sys
import os
from pathlib import Path

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['PYTHONPATH'] = '/Users/chenyuyang/git/AIDocGenerator/service'

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_file = Path(__file__)
service_dir = current_file.parent
if str(service_dir) not in sys.path:
    sys.path.insert(0, str(service_dir))

from core.env_loader import setup_environment
from core.config import settings
from src.doc_agent.llm_clients import get_llm_client


def test_different_models():
    """æµ‹è¯•ä¸åŒæ¨¡å‹çš„é…ç½®åˆ‡æ¢"""
    print("ğŸ”„ æµ‹è¯•ä¸åŒæ¨¡å‹é…ç½®åˆ‡æ¢")
    print("=" * 80)

    # æµ‹è¯•çš„æ¨¡å‹åˆ—è¡¨
    test_configs = [{
        "name": "Moonshot K2",
        "model_key": "moonshot_k2_0711_preview",
        "description": "é«˜æ€§èƒ½æ¨ç†æ¨¡å‹"
    }, {
        "name": "Gemini 1.5 Pro",
        "model_key": "gemini_1_5_pro",
        "description": "Googleé«˜è´¨é‡æ¨¡å‹"
    }, {
        "name": "DeepSeek Chat",
        "model_key": "deepseek_v3",
        "description": "DeepSeeké€šç”¨æ¨¡å‹"
    }, {
        "name": "Qwen 235B",
        "model_key": "qwen_2_5_235b_a22b",
        "description": "ä¼ä¸šå†…ç½‘æ¨ç†æ¨¡å‹"
    }]

    for config in test_configs:
        print(f"\nğŸ” æµ‹è¯• {config['name']} ({config['description']})")
        print("-" * 60)

        try:
            # è·å–æ¨¡å‹é…ç½®
            model_config = settings.get_model_config(config['model_key'])
            if not model_config:
                print(f"âŒ æ¨¡å‹é…ç½®æœªæ‰¾åˆ°: {config['model_key']}")
                continue

            print(f"ğŸ“ æ¨¡å‹åç§°: {model_config.name}")
            print(f"ğŸ”— APIåœ°å€: {model_config.url}")
            print(f"ğŸ§  æ¨ç†æ¨¡å¼: {model_config.reasoning}")

            # åˆ›å»ºå®¢æˆ·ç«¯
            client = get_llm_client(config['model_key'])
            print(f"âœ… å®¢æˆ·ç«¯ç±»å‹: {type(client).__name__}")

            # æµ‹è¯•è°ƒç”¨
            test_prompt = "è¯·ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"
            print(f"ğŸ§ª æµ‹è¯•é—®é¢˜: {test_prompt}")

            response = client.invoke(test_prompt,
                                     temperature=0.7,
                                     max_tokens=100)
            print(f"ğŸ“¤ å“åº”: {response}")
            print(f"âœ… {config['name']} æµ‹è¯•æˆåŠŸ")

        except Exception as e:
            print(f"âŒ {config['name']} æµ‹è¯•å¤±è´¥: {str(e)}")


def demonstrate_container_switch():
    """æ¼”ç¤ºå¦‚ä½•åˆ‡æ¢å®¹å™¨ä¸­çš„æ¨¡å‹"""
    print("\nğŸ”§ æ¼”ç¤ºå®¹å™¨æ¨¡å‹åˆ‡æ¢")
    print("=" * 80)

    try:
        from core.container import container

        print("ğŸ“‹ å½“å‰å®¹å™¨é…ç½®:")
        print(f"  - LLMå®¢æˆ·ç«¯: {type(container.llm_client).__name__}")
        print(f"  - æ¨¡å‹åç§°: {container.llm_client.model_name}")

        # æ¼”ç¤ºå¦‚ä½•åˆ›å»ºä¸åŒæ¨¡å‹çš„å®¹å™¨
        print("\nğŸ”„ æ¼”ç¤ºåˆ›å»ºä¸åŒæ¨¡å‹çš„å®¹å™¨:")

        models_to_test = [
            "gemini_1_5_pro", "deepseek_v3", "moonshot_k2_0711_preview"
        ]

        for model_key in models_to_test:
            try:
                print(f"\n  ğŸ” æµ‹è¯• {model_key}:")

                # åˆ›å»ºæ–°çš„LLMå®¢æˆ·ç«¯
                new_client = get_llm_client(model_key)
                print(f"    âœ… å®¢æˆ·ç«¯åˆ›å»º: {type(new_client).__name__}")

                # æµ‹è¯•è°ƒç”¨
                response = new_client.invoke("ä½ å¥½",
                                             temperature=0,
                                             max_tokens=20)
                print(f"    ğŸ“¤ å“åº”: {response[:50]}...")

            except Exception as e:
                print(f"    âŒ å¤±è´¥: {str(e)}")

    except Exception as e:
        print(f"âŒ å®¹å™¨åˆ‡æ¢æ¼”ç¤ºå¤±è´¥: {str(e)}")


def show_configuration_guide():
    """æ˜¾ç¤ºé…ç½®æŒ‡å—"""
    print("\nğŸ“– é…ç½®ä¿®æ”¹æŒ‡å—")
    print("=" * 80)

    print("1. ä¿®æ”¹å®¹å™¨é»˜è®¤æ¨¡å‹:")
    print("   æ–‡ä»¶: core/container.py")
    print("   ä¿®æ”¹: self.llm_client = get_llm_client(model_key='your_model')")
    print()

    print("2. ä¿®æ”¹Agentç»„ä»¶é…ç½®:")
    print("   æ–‡ä»¶: core/config.yaml")
    print("   ä¿®æ”¹: agent_config ä¸‹çš„ provider å’Œ model å­—æ®µ")
    print()

    print("3. æ·»åŠ æ–°æ¨¡å‹:")
    print("   æ–‡ä»¶: core/config.yaml")
    print("   åœ¨ supported_models ä¸‹æ·»åŠ æ–°é…ç½®")
    print()

    print("4. ç¯å¢ƒå˜é‡é…ç½®:")
    print("   æ–‡ä»¶: .env")
    print("   æ·»åŠ : MODEL_API_KEY=your_api_key")
    print()

    print("5. å¯ç”¨çš„æ¨¡å‹é”®å:")
    for model_key in settings.supported_models.keys():
        config = settings.supported_models[model_key]
        print(f"   - {model_key}: {config.name} ({config.type})")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ é…ç½®åˆ‡æ¢æµ‹è¯•å¼€å§‹")
    print("=" * 80)

    # è®¾ç½®ç¯å¢ƒ
    setup_environment()

    # è¿è¡Œæµ‹è¯•
    test_different_models()
    demonstrate_container_switch()
    show_configuration_guide()

    print("\nâœ… é…ç½®åˆ‡æ¢æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    main()
