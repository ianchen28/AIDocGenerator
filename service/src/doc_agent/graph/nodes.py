# service/src/doc_agent/graph/nodes.py
from .state import ResearchState
from ..llm_clients.base import LLMClient
from ..tools.web_search import WebSearchTool
from ..tools.es_search import ESSearchTool
from ..llm_clients.providers import EmbeddingClient

# æ·»åŠ é…ç½®å¯¼å…¥
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_file = Path(__file__)
service_dir = None
for parent in current_file.parents:
    if parent.name == 'service':
        service_dir = parent
        break

if service_dir and str(service_dir) not in sys.path:
    sys.path.insert(0, str(service_dir))

from core.config import settings

# ä¿®å¤ç›¸å¯¹å¯¼å…¥
try:
    from ..utils import parse_planner_response
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥
    try:
        from src.doc_agent.utils import parse_planner_response
    except ImportError:
        # å¦‚æœéƒ½å¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„è§£æå‡½æ•°
        def parse_planner_response(response):
            """ç®€å•çš„å“åº”è§£æå‡½æ•°"""
            import json
            try:
                data = json.loads(response)
                return data.get("research_plan",
                                ""), data.get("search_queries", [])
            except:
                return "é»˜è®¤ç ”ç©¶è®¡åˆ’", ["é»˜è®¤æœç´¢æŸ¥è¯¢"]


def planner_node(state: ResearchState, llm_client: LLMClient) -> dict:
    """
    èŠ‚ç‚¹1: è§„åˆ’ç ”ç©¶æ­¥éª¤
    
    ä»çŠ¶æ€ä¸­è·å– topicï¼Œåˆ›å»º prompt è°ƒç”¨ LLM ç”Ÿæˆç ”ç©¶è®¡åˆ’å’Œæœç´¢æŸ¥è¯¢
    
    Args:
        state: ç ”ç©¶çŠ¶æ€ï¼ŒåŒ…å« topic
        llm_client: LLMå®¢æˆ·ç«¯å®ä¾‹
        
    Returns:
        dict: åŒ…å« research_plan å’Œ search_queries çš„å­—å…¸
    """
    topic = state.get("topic", "")
    if not topic:
        raise ValueError("Topic is required in state")

    # è·å–ä»»åŠ¡è§„åˆ’å™¨é…ç½®
    task_planner_config = settings.get_agent_component_config("task_planner")
    if not task_planner_config:
        raise ValueError("Task planner configuration not found")

    # åˆ›å»ºç ”ç©¶è®¡åˆ’ç”Ÿæˆçš„ promptï¼Œè¦æ±‚ JSON æ ¼å¼å“åº”
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶è§„åˆ’ä¸“å®¶ã€‚è¯·ä¸ºä»¥ä¸‹ä¸»é¢˜åˆ¶å®šè¯¦ç»†çš„ç ”ç©¶è®¡åˆ’å’Œæœç´¢ç­–ç•¥ã€‚

ä¸»é¢˜: {topic}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–å†…å®¹ï¼š

{{
    "research_plan": "è¯¦ç»†çš„ç ”ç©¶è®¡åˆ’ï¼ŒåŒ…æ‹¬ï¼š1. éœ€è¦äº†è§£çš„æ ¸å¿ƒæ¦‚å¿µ 2. éœ€è¦æ”¶é›†çš„ä¿¡æ¯ç±»å‹ 3. ç ”ç©¶çš„æ—¶é—´å®‰æ’ 4. å¯èƒ½é‡åˆ°çš„æŒ‘æˆ˜å’Œè§£å†³æ–¹æ¡ˆ",
    "search_queries": ["å…·ä½“çš„æœç´¢æŸ¥è¯¢1", "å…·ä½“çš„æœç´¢æŸ¥è¯¢2", "å…·ä½“çš„æœç´¢æŸ¥è¯¢3", "å…·ä½“çš„æœç´¢æŸ¥è¯¢4", "å…·ä½“çš„æœç´¢æŸ¥è¯¢5"]
}}

è¦æ±‚ï¼š
1. research_plan åº”è¯¥æ˜¯ä¸€ä¸ªè¯¦ç»†çš„æ­¥éª¤è®¡åˆ’ï¼ŒåŒ…å«å…·ä½“çš„ç ”ç©¶æ­¥éª¤å’Œç­–ç•¥
2. search_queries åº”è¯¥åŒ…å«3-5ä¸ªå…·ä½“çš„æœç´¢æŸ¥è¯¢ï¼Œæ¯ä¸ªæŸ¥è¯¢è¦é’ˆå¯¹æ€§å¼ºä¸”è¦†ç›–ä¸»é¢˜çš„ä¸åŒæ–¹é¢
3. å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼è¾“å‡ºï¼Œç¡®ä¿ JSON æ ¼å¼æ­£ç¡®
4. æœç´¢æŸ¥è¯¢åº”è¯¥ä½¿ç”¨é€šç”¨å…³é”®è¯ï¼Œé¿å…è¿‡äºå…·ä½“çš„æœ¯è¯­ï¼Œç¡®ä¿èƒ½åœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç›¸å…³å†…å®¹
5. æœç´¢æŸ¥è¯¢åº”è¯¥åŒ…å«ä¸»é¢˜çš„æ ¸å¿ƒè¯æ±‡ï¼Œæ¯”å¦‚"æ°´ç”µç«™"ã€"ç”µåŠ›"ã€"èƒ½æº"ç­‰é€šç”¨æœ¯è¯­
"""

    try:
        # è°ƒç”¨ LLM ç”Ÿæˆç ”ç©¶è®¡åˆ’
        response = llm_client.invoke(
            prompt,
            temperature=task_planner_config.temperature,
            max_tokens=task_planner_config.max_tokens,
            **task_planner_config.extra_params)

        # è§£æ JSON å“åº”
        research_plan, search_queries = parse_planner_response(response)

        return {
            "research_plan": research_plan,
            "search_queries": search_queries
        }

    except Exception as e:
        # å¦‚æœ LLM è°ƒç”¨å¤±è´¥ï¼Œè¿”å›é»˜è®¤è®¡åˆ’
        print(f"Planner node error: {str(e)}")
        return {
            "research_plan": f"é»˜è®¤ç ”ç©¶è®¡åˆ’ï¼šå¯¹ä¸»é¢˜ '{topic}' è¿›è¡Œæ·±å…¥ç ”ç©¶ï¼Œæ”¶é›†ç›¸å…³ä¿¡æ¯å¹¶æ•´ç†æˆæ–‡æ¡£ã€‚",
            "search_queries":
            [f"{topic} ä»‹ç»", f"{topic} æœ€æ–°å‘å±•", f"{topic} åº”ç”¨æ¡ˆä¾‹"]
        }


def researcher_node(state: ResearchState,
                    web_search_tool: WebSearchTool) -> dict:
    raise NotImplementedError("è¯·ä½¿ç”¨ async_researcher_node")


async def async_researcher_node(state: ResearchState,
                                web_search_tool: WebSearchTool) -> dict:
    """
    å¼‚æ­¥èŠ‚ç‚¹2: æ‰§è¡Œæœç´¢ç ”ç©¶
    
    ä»çŠ¶æ€ä¸­è·å– search_queriesï¼Œä½¿ç”¨æœç´¢å·¥å…·æ”¶é›†ç›¸å…³ä¿¡æ¯
    ä¼˜å…ˆä½¿ç”¨å‘é‡æ£€ç´¢ï¼Œå¦‚æœå¤±è´¥åˆ™å›é€€åˆ°æ–‡æœ¬æœç´¢
    
    Args:
        state: ç ”ç©¶çŠ¶æ€ï¼ŒåŒ…å« search_queries
        web_search_tool: ç½‘ç»œæœç´¢å·¥å…·
        
    Returns:
        dict: åŒ…å« gathered_data çš„å­—å…¸
    """
    search_queries = state.get("search_queries", [])
    if not search_queries:
        return {"gathered_data": "æ²¡æœ‰æœç´¢æŸ¥è¯¢éœ€è¦æ‰§è¡Œ"}

    all_results = []

    # è·å–embeddingé…ç½®
    embedding_config = settings.supported_models.get("gte_qwen")
    embedding_client = None
    if embedding_config:
        try:
            embedding_client = EmbeddingClient(
                base_url=embedding_config.url,
                api_key=embedding_config.api_key)
            print("âœ… Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸  Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            embedding_client = None
    else:
        print("âŒ æœªæ‰¾åˆ° embedding é…ç½®ï¼Œå°†ä½¿ç”¨æ–‡æœ¬æœç´¢")

    # ç”¨é…ç½®åˆå§‹åŒ–ESå·¥å…·
    es_config = settings.elasticsearch_config
    async with ESSearchTool(hosts=es_config.hosts,
                            username=es_config.username,
                            password=es_config.password,
                            index_prefix=es_config.index_prefix,
                            timeout=es_config.timeout) as es_search_tool:

        for i, query in enumerate(search_queries, 1):
            print(f"æ‰§è¡Œæœç´¢æŸ¥è¯¢ {i}/{len(search_queries)}: {query}")

            # ç½‘ç»œæœç´¢
            web_results = ""
            try:
                web_results = web_search_tool.search(query)
                if "æ¨¡æ‹Ÿ" in web_results or "mock" in web_results.lower():
                    print(f"ç½‘ç»œæœç´¢è¿”å›æ¨¡æ‹Ÿç»“æœï¼Œè·³è¿‡: {query}")
                    web_results = ""
            except Exception as e:
                print(f"ç½‘ç»œæœç´¢å¤±è´¥: {str(e)}")
                web_results = ""

            # ESæœç´¢ - ä¼˜å…ˆå‘é‡æ£€ç´¢ï¼Œå¤±è´¥åˆ™å›é€€åˆ°æ–‡æœ¬æœç´¢
            es_results = ""
            try:
                if embedding_client:
                    # å°è¯•å‘é‡æ£€ç´¢
                    try:
                        embedding_response = embedding_client.invoke(query)
                        import json
                        try:
                            embedding_data = json.loads(embedding_response)
                            if isinstance(embedding_data, list):
                                if len(embedding_data) > 0 and isinstance(
                                        embedding_data[0], list):
                                    query_vector = embedding_data[0]
                                else:
                                    query_vector = embedding_data
                            elif isinstance(embedding_data,
                                            dict) and 'data' in embedding_data:
                                query_vector = embedding_data['data']
                            else:
                                print(
                                    f"âš ï¸  æ— æ³•è§£æembeddingå“åº”æ ¼å¼: {type(embedding_data)}"
                                )
                                query_vector = None
                        except json.JSONDecodeError:
                            print(f"âš ï¸  JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬æœç´¢")
                            query_vector = None

                        if query_vector and len(query_vector) == 1536:
                            print(
                                f"âœ… å‘é‡ç»´åº¦: {len(query_vector)}ï¼Œå‰5: {query_vector[:5]}"
                            )
                            # ä½¿ç”¨æ–°çš„ search æ–¹æ³•ï¼Œä¼ å…¥æ­£ç¡®çš„å‚æ•°
                            # å¦‚æœ query ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤æŸ¥è¯¢å­—ç¬¦ä¸²
                            search_query = query if query.strip() else "ç›¸å…³æ–‡æ¡£"
                            es_results = await es_search_tool.search(
                                query=search_query,
                                query_vector=query_vector,
                                top_k=3)
                            print(f"âœ… å‘é‡æ£€ç´¢æ‰§è¡ŒæˆåŠŸï¼Œç»“æœé•¿åº¦: {len(es_results)}")
                        else:
                            print(f"âŒ å‘é‡ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬æœç´¢")
                            # å›é€€åˆ°æ–‡æœ¬æœç´¢
                            es_results = await es_search_tool.search(
                                query=query, query_vector=None, top_k=3)
                            print(f"âœ… æ–‡æœ¬æœç´¢æ‰§è¡ŒæˆåŠŸï¼Œç»“æœé•¿åº¦: {len(es_results)}")
                    except Exception as e:
                        print(f"âŒ å‘é‡æ£€ç´¢å¼‚å¸¸: {str(e)}ï¼Œä½¿ç”¨æ–‡æœ¬æœç´¢")
                        # å›é€€åˆ°æ–‡æœ¬æœç´¢
                        es_results = await es_search_tool.search(
                            query=query, query_vector=None, top_k=3)
                        print(f"âœ… æ–‡æœ¬æœç´¢æ‰§è¡ŒæˆåŠŸï¼Œç»“æœé•¿åº¦: {len(es_results)}")
                else:
                    # æ²¡æœ‰embeddingå®¢æˆ·ç«¯ï¼Œç›´æ¥ä½¿ç”¨æ–‡æœ¬æœç´¢
                    print("ğŸ“ ä½¿ç”¨æ–‡æœ¬æœç´¢")
                    es_results = await es_search_tool.search(query=query,
                                                             query_vector=None,
                                                             top_k=3)
                    print(f"âœ… æ–‡æœ¬æœç´¢æ‰§è¡ŒæˆåŠŸï¼Œç»“æœé•¿åº¦: {len(es_results)}")

            except Exception as e:
                print(f"âŒ ESæœç´¢å¤±è´¥: {str(e)}")
                es_results = f"ESæœç´¢å¤±è´¥: {str(e)}"

            # èšåˆç»“æœ
            query_results = f"=== æœç´¢æŸ¥è¯¢ {i}: {query} ===\n\n"
            if web_results:
                query_results += f"ç½‘ç»œæœç´¢ç»“æœ:\n{web_results}\n\n"
            if es_results:
                query_results += f"çŸ¥è¯†åº“æœç´¢ç»“æœ:\n{es_results}\n\n"
            if not web_results and not es_results:
                query_results += "æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœ\n\n"
            all_results.append(query_results)

    # åˆå¹¶æ‰€æœ‰æœç´¢ç»“æœ
    if all_results:
        gathered_data = "\\n\\n".join(all_results)
        print(f"âœ… æ”¶é›†åˆ° {len(all_results)} æ¡æœç´¢ç»“æœ")
        print(f"ğŸ“Š æ€»æ•°æ®é•¿åº¦: {len(gathered_data)} å­—ç¬¦")
        # åªæ˜¾ç¤ºå‰200å­—ç¬¦ä½œä¸ºé¢„è§ˆï¼Œé¿å…æ—¥å¿—è¿‡é•¿
        preview = gathered_data[:200] + "..." if len(
            gathered_data) > 200 else gathered_data
        print(f"ğŸ“ æ•°æ®é¢„è§ˆ: {preview}")
    else:
        gathered_data = "æœªæ”¶é›†åˆ°ä»»ä½•æœç´¢ç»“æœ"
        print("âŒ æœªæ”¶é›†åˆ°ä»»ä½•æœç´¢ç»“æœ")

    return {"gathered_data": gathered_data}


def writer_node(state: ResearchState, llm_client: LLMClient) -> dict:
    """
    èŠ‚ç‚¹3: æ’°å†™æ–‡æ¡£
    
    åŸºäºæ”¶é›†çš„ç ”ç©¶æ•°æ®å’Œè®¡åˆ’ï¼Œç”Ÿæˆé«˜è´¨é‡çš„æœ€ç»ˆæ–‡æ¡£
    
    Args:
        state: ç ”ç©¶çŠ¶æ€ï¼ŒåŒ…å« topic, research_plan, gathered_data
        llm_client: LLMå®¢æˆ·ç«¯å®ä¾‹
        
    Returns:
        dict: åŒ…å« final_document çš„å­—å…¸
    """
    topic = state.get("topic", "")
    research_plan = state.get("research_plan", "")
    gathered_data = state.get("gathered_data", "")

    if not topic:
        raise ValueError("Topic is required in state")

    if not gathered_data:
        return {"final_document": f"# {topic}\n\nç”±äºæ²¡æœ‰æ”¶é›†åˆ°ç›¸å…³æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆå®Œæ•´æ–‡æ¡£ã€‚"}

    # è·å–æ–‡æ¡£ç”Ÿæˆå™¨é…ç½®
    document_writer_config = settings.get_agent_component_config(
        "document_writer")
    if not document_writer_config:
        # ä½¿ç”¨é»˜è®¤é…ç½®
        temperature = 0.7
        max_tokens = 4000
        extra_params = {}
    else:
        temperature = document_writer_config.temperature
        max_tokens = document_writer_config.max_tokens
        extra_params = document_writer_config.extra_params

    # æ„å»ºé«˜è´¨é‡çš„æç¤ºè¯
    prompt = f"""
**Role:** You are a professional researcher and expert writer, tasked with creating a comprehensive and well-structured document.

**Topic:** {topic}

**Original Research Plan:**
{research_plan}

**Raw Data & Research Findings:**
{gathered_data}

**Your Task:**
Based *exclusively* on the provided "Raw Data & Research Findings", write a comprehensive document on the specified "Topic". Follow these instructions carefully:

1. **Synthesize, Do Not Copy:** Do not simply list the raw data. You must synthesize, analyze, and organize the information into a coherent narrative.

2. **Structure:** Structure the document logically with a clear introduction, body, and conclusion. Use headings, subheadings, bullet points, and bold text to improve readability. The output format must be Markdown.

3. **Adhere to the Plan:** Use the "Original Research Plan" as a guide for the document's structure, but feel free to adapt it if the data suggests a better flow.

4. **Be Objective:** Stick to the facts and information presented in the raw data. Do not introduce any external knowledge or personal opinions.

5. **Completeness:** Ensure all key aspects from the gathered data are covered in the final document.

6. **Professional Quality:** Write in a professional, academic style that would be suitable for technical documentation or research reports.

7. **Markdown Formatting:** Use proper Markdown syntax including:
   - Headers (# ## ###)
   - Lists (both bulleted and numbered)
   - Bold and italic text for emphasis
   - Tables where appropriate
   - Code blocks for technical specifications
   - Blockquotes for important notes

Begin writing the document now.
"""

    # é™åˆ¶ prompt é•¿åº¦ï¼Œé¿å…è¶…è¿‡æ¨¡å‹è¾“å…¥é™åˆ¶
    max_prompt_length = 25000  # 25K å­—ç¬¦é™åˆ¶ï¼Œç»™ writer æ›´å¤šç©ºé—´
    if len(prompt) > max_prompt_length:
        print(
            f"âš ï¸  Writer prompt é•¿åº¦ {len(prompt)} è¶…è¿‡é™åˆ¶ {max_prompt_length}ï¼Œè¿›è¡Œæˆªæ–­"
        )
        # ä¿ç•™å¼€å¤´å’Œç»“å°¾çš„é‡è¦ä¿¡æ¯ï¼Œæˆªæ–­ä¸­é—´çš„ç ”ç©¶èµ„æ–™
        header = prompt[:prompt.find("**Raw Data & Research Findings:**") +
                        len("**Raw Data & Research Findings:**")]
        footer = prompt[prompt.find("**Your Task:**"):]
        # ä» gathered_data ä¸­å–å‰ 15000 å­—ç¬¦
        gathered_data_preview = gathered_data[:15000] + "\n\n... (å†…å®¹å·²æˆªæ–­ï¼Œä¿ç•™å‰15000å­—ç¬¦)"
        prompt = header + "\n\n" + gathered_data_preview + "\n\n" + footer
        print(f"ğŸ“ æˆªæ–­å writer prompt é•¿åº¦: {len(prompt)} å­—ç¬¦")

    try:
        # è°ƒç”¨LLMç”Ÿæˆæ–‡æ¡£
        response = llm_client.invoke(prompt,
                                     temperature=temperature,
                                     max_tokens=max_tokens,
                                     **extra_params)

        # ç¡®ä¿å“åº”æ˜¯æœ‰æ•ˆçš„Markdownæ ¼å¼
        if not response.strip():
            response = f"# {topic}\n\næ— æ³•ç”Ÿæˆæ–‡æ¡£å†…å®¹ã€‚"
        elif not response.startswith("#"):
            # å¦‚æœæ²¡æœ‰æ ‡é¢˜ï¼Œæ·»åŠ é»˜è®¤æ ‡é¢˜
            response = f"# {topic}\n\n{response}"

        return {"final_document": response}

    except Exception as e:
        # å¦‚æœLLMè°ƒç”¨å¤±è´¥ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
        print(f"Writer node error: {str(e)}")
        error_document = f"""# {topic}

## æ–‡æ¡£ç”Ÿæˆé”™è¯¯

ç”±äºæŠ€æœ¯åŸå› ï¼Œæ— æ³•ç”Ÿæˆå®Œæ•´çš„æ–‡æ¡£ã€‚

**é”™è¯¯ä¿¡æ¯:** {str(e)}

**åŸå§‹æ•°æ®æ‘˜è¦:**
{gathered_data[:500]}{"..." if len(gathered_data) > 500 else ""}

è¯·æ£€æŸ¥ç³»ç»Ÿé…ç½®æˆ–ç¨åé‡è¯•ã€‚
"""
        return {"final_document": error_document}
