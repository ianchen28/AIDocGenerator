# service/src/doc_agent/graph/fast_nodes.py
"""
å¿«é€Ÿç‰ˆæœ¬çš„èŠ‚ç‚¹å®ç°
ä½¿ç”¨ç®€åŒ–çš„promptsï¼Œç›®æ ‡ï¼š3-5åˆ†é’Ÿå†…å®Œæˆæ–‡æ¡£ç”Ÿæˆ
"""

import json
import pprint

from loguru import logger

from doc_agent.core.config import settings
from doc_agent.fast_prompts import (
    FAST_OUTLINE_GENERATION_PROMPT,
    FAST_PLANNER_PROMPT,
)
from doc_agent.graph.state import ResearchState
from doc_agent.llm_clients.base import LLMClient
from doc_agent.llm_clients.providers import EmbeddingClient
from doc_agent.tools.es_search import ESSearchTool
from doc_agent.tools.reranker import RerankerTool
from doc_agent.tools.web_search import WebSearchTool
from doc_agent.utils.search_utils import search_and_rerank


async def fast_initial_research_node(state: ResearchState,
                                     web_search_tool: WebSearchTool,
                                     es_search_tool: ESSearchTool,
                                     reranker_tool: RerankerTool = None,
                                     llm_client: LLMClient = None) -> dict:
    """
    å¿«é€Ÿåˆå§‹ç ”ç©¶èŠ‚ç‚¹ - ç®€åŒ–ç‰ˆæœ¬
    æ‰§è¡Œå¿«é€Ÿçš„ç ”ç©¶ï¼Œæ”¶é›†å…³äºä¸»é¢˜çš„æ¦‚è§ˆä¿¡æ¯
    """
    topic = state.get("topic", "")
    if not topic:
        raise ValueError("ä¸»é¢˜ä¸èƒ½ä¸ºç©º")

    logger.info(f"ğŸ” å¼€å§‹å¿«é€Ÿåˆå§‹ç ”ç©¶: {topic}")

    # ç”Ÿæˆç®€åŒ–çš„åˆå§‹æœç´¢æŸ¥è¯¢ - åªä½¿ç”¨2ä¸ªæŸ¥è¯¢
    initial_queries = [f"{topic} æ¦‚è¿°", f"{topic} ä¸»è¦å†…å®¹"]

    all_results = []

    # è·å–embeddingé…ç½®
    embedding_config = settings.supported_models.get("gte_qwen")
    embedding_client = None
    if embedding_config:
        try:
            embedding_client = EmbeddingClient(
                base_url=embedding_config.url,
                api_key=embedding_config.api_key)
            logger.info("âœ… Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸  Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            embedding_client = None

    # æ‰§è¡Œæœç´¢
    for i, query in enumerate(initial_queries, 1):
        logger.info(f"æ‰§è¡Œå¿«é€Ÿåˆå§‹æœç´¢ {i}/{len(initial_queries)}: {query}")

        # ç½‘ç»œæœç´¢
        web_results = ""
        try:
            # ä½¿ç”¨å¼‚æ­¥æœç´¢æ–¹æ³•
            web_results = await web_search_tool.search_async(query)
            if "æ¨¡æ‹Ÿ" in web_results or "mock" in web_results.lower():
                web_results = ""
        except Exception as e:
            logger.error(f"ç½‘ç»œæœç´¢å¤±è´¥: {str(e)}")
            web_results = ""

        # ESæœç´¢ - ä½¿ç”¨ç®€åŒ–çš„æœç´¢
        es_results = ""
        try:
            if embedding_client:
                # å°è¯•å‘é‡æ£€ç´¢
                try:
                    embedding_response = embedding_client.invoke(query)
                    embedding_data = json.loads(embedding_response)

                    # è§£æå‘é‡
                    if isinstance(embedding_data, list):
                        query_vector = embedding_data[0] if len(
                            embedding_data) > 0 and isinstance(
                                embedding_data[0], list) else embedding_data
                    elif isinstance(embedding_data,
                                    dict) and 'data' in embedding_data:
                        query_vector = embedding_data['data']
                    else:
                        query_vector = None

                    if query_vector:
                        # ä½¿ç”¨å‘é‡æ£€ç´¢
                        es_results = await search_and_rerank(
                            query, es_search_tool, reranker_tool, query_vector)
                        logger.info(f"âœ… å‘é‡æ£€ç´¢+é‡æ’åºæ‰§è¡ŒæˆåŠŸï¼Œç»“æœé•¿åº¦: {len(es_results)}")
                    else:
                        # å›é€€åˆ°æ–‡æœ¬æœç´¢
                        es_results = es_search_tool.search(query)
                        logger.info(f"âœ… æ–‡æœ¬æœç´¢æ‰§è¡ŒæˆåŠŸï¼Œç»“æœé•¿åº¦: {len(es_results)}")

                except Exception as e:
                    logger.warning(f"âš ï¸  å‘é‡æ£€ç´¢å¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬æœç´¢: {str(e)}")
                    es_results = es_search_tool.search(query)
            else:
                # ç›´æ¥ä½¿ç”¨æ–‡æœ¬æœç´¢
                es_results = es_search_tool.search(query)

        except Exception as e:
            logger.error(f"ESæœç´¢å¤±è´¥: {str(e)}")
            es_results = ""

        # åˆå¹¶ç»“æœ
        combined_results = f"=== æŸ¥è¯¢: {query} ===\n"
        if web_results:
            combined_results += f"ç½‘ç»œæœç´¢ç»“æœ:\n{web_results}\n\n"
        if es_results:
            combined_results += f"çŸ¥è¯†åº“æœç´¢ç»“æœ:\n{es_results}\n\n"

        all_results.append(combined_results)

    # åˆå¹¶æ‰€æœ‰ç»“æœ
    raw_initial_gathered_data = "\n".join(all_results)
    logger.info(f"âœ… å¿«é€Ÿåˆå§‹ç ”ç©¶å®Œæˆï¼Œæ”¶é›†åˆ° {len(raw_initial_gathered_data)} å­—ç¬¦çš„åŸå§‹æ•°æ®")

    # ç®€åŒ–æ•°æ®å¤„ç† - å¦‚æœæ•°æ®é‡è¿‡å¤§ï¼Œè¿›è¡Œç®€å•æˆªæ–­
    if len(raw_initial_gathered_data) > 5000:  # é™ä½é˜ˆå€¼
        logger.info("ğŸ“Š æ•°æ®é‡è¾ƒå¤§ï¼Œè¿›è¡Œç®€å•æˆªæ–­...")
        truncated_data = raw_initial_gathered_data[:5000] + "\n\n... (å†…å®¹å·²æˆªæ–­)"
        return {"initial_gathered_data": truncated_data}

    return {"initial_gathered_data": raw_initial_gathered_data}


def fast_outline_generation_node(state: ResearchState,
                                 llm_client: LLMClient) -> dict:
    """
    å¿«é€Ÿå¤§çº²ç”ŸæˆèŠ‚ç‚¹ - ç®€åŒ–ç‰ˆæœ¬
    
    åŸºäºåˆå§‹ç ”ç©¶æ•°æ®ç”Ÿæˆç®€åŒ–çš„æ–‡æ¡£å¤§çº²
    """
    topic = state.get("topic", "")
    initial_gathered_data = state.get("initial_gathered_data", "")

    if not topic:
        raise ValueError("ä¸»é¢˜ä¸èƒ½ä¸ºç©º")

    if not initial_gathered_data:
        raise ValueError("åˆå§‹ç ”ç©¶æ•°æ®ä¸èƒ½ä¸ºç©º")

    logger.info(f"ğŸ“‹ å¼€å§‹ç”Ÿæˆå¿«é€Ÿæ–‡æ¡£å¤§çº²: {topic}")

    # è·å–é…ç½®
    outline_config = settings.get_agent_component_config("task_planner")
    if not outline_config:
        temperature = 0.3
        max_tokens = 1000  # å‡å°‘tokenæ•°é‡
        extra_params = {}
    else:
        temperature = outline_config.temperature * 0.7
        max_tokens = min(outline_config.max_tokens, 1000)  # é™åˆ¶æœ€å¤§token
        extra_params = outline_config.extra_params

    # æ„å»ºæç¤ºè¯
    prompt = FAST_OUTLINE_GENERATION_PROMPT.format(
        topic=topic,
        initial_gathered_data=initial_gathered_data[:4000]  # å‡å°‘è¾“å…¥é•¿åº¦
    )

    logger.debug(
        f"Invoking LLM with fast outline generation prompt:\n{pprint.pformat(prompt)}"
    )

    try:
        # è°ƒç”¨LLMç”Ÿæˆå¤§çº²
        response = llm_client.invoke(prompt,
                                     temperature=temperature,
                                     max_tokens=max_tokens,
                                     **extra_params)

        # è§£æJSONå“åº”
        try:
            # æ¸…ç†å“åº”ï¼Œç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
            cleaned_response = response.strip()
            if cleaned_response.startswith("```json"):
                cleaned_response = cleaned_response[7:]
            if cleaned_response.startswith("```"):
                cleaned_response = cleaned_response[3:]
            if cleaned_response.endswith("```"):
                cleaned_response = cleaned_response[:-3]

            document_outline = json.loads(cleaned_response.strip())

            # éªŒè¯å¤§çº²ç»“æ„
            if "chapters" not in document_outline:
                raise ValueError("å¤§çº²ç¼ºå°‘chapterså­—æ®µ")

            logger.info(
                f"âœ… æˆåŠŸç”Ÿæˆå¿«é€Ÿå¤§çº²ï¼ŒåŒ…å« {len(document_outline['chapters'])} ä¸ªç« èŠ‚")

        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSONè§£æå¤±è´¥: {str(e)}")
            # è¿”å›é»˜è®¤å¤§çº²
            document_outline = {
                "title":
                topic,
                "summary":
                f"å…³äº{topic}çš„ç®€åŒ–æ–‡æ¡£",
                "chapters": [{
                    "chapter_number": 1,
                    "chapter_title": "æ¦‚è¿°ä¸èƒŒæ™¯",
                    "description": f"ä»‹ç»{topic}çš„åŸºæœ¬æ¦‚å¿µå’Œé‡è¦æ€§",
                    "key_points": ["åŸºæœ¬å®šä¹‰", "é‡è¦æ€§"],
                    "estimated_sections": 2
                }, {
                    "chapter_number": 2,
                    "chapter_title": "æ ¸å¿ƒå†…å®¹åˆ†æ",
                    "description": f"åˆ†æ{topic}çš„æ ¸å¿ƒè¦ç´ ",
                    "key_points": ["ä¸»è¦ç‰¹å¾", "å…³é”®è¦ç´ "],
                    "estimated_sections": 2
                }],
                "total_chapters":
                2,
                "estimated_total_words":
                5000
            }

        return {"document_outline": document_outline}

    except Exception as e:
        logger.error(f"âŒ å¿«é€Ÿå¤§çº²ç”Ÿæˆå¤±è´¥: {str(e)}")
        # è¿”å›åŸºç¡€å¤§çº²
        return {
            "document_outline": {
                "title":
                topic,
                "summary":
                f"å…³äº{topic}çš„æ–‡æ¡£",
                "chapters": [{
                    "chapter_number": 1,
                    "chapter_title": "å¼•è¨€",
                    "description": f"ä»‹ç»{topic}çš„åŸºæœ¬ä¿¡æ¯",
                    "key_points": ["æ¦‚è¿°"],
                    "estimated_sections": 2
                }],
                "total_chapters":
                1,
                "estimated_total_words":
                3000
            }
        }


def fast_planner_node(state: ResearchState, llm_client: LLMClient) -> dict:
    """
    å¿«é€Ÿè§„åˆ’èŠ‚ç‚¹ - ç®€åŒ–ç‰ˆæœ¬
    ä»çŠ¶æ€ä¸­è·å– topic å’Œå½“å‰ç« èŠ‚ä¿¡æ¯ï¼Œåˆ›å»ºç®€åŒ–çš„ç ”ç©¶è®¡åˆ’
    """
    topic = state.get("topic", "")
    current_chapter_index = state.get("current_chapter_index", 0)
    chapters_to_process = state.get("chapters_to_process", [])

    if not topic:
        raise ValueError("Topic is required in state")

    # è·å–å½“å‰ç« èŠ‚ä¿¡æ¯
    current_chapter = None
    if current_chapter_index < len(chapters_to_process):
        current_chapter = chapters_to_process[current_chapter_index]

    chapter_title = current_chapter.get("chapter_title",
                                        "") if current_chapter else ""
    chapter_description = current_chapter.get("description",
                                              "") if current_chapter else ""

    logger.info(f"ğŸ“‹ å¿«é€Ÿè§„åˆ’ç« èŠ‚ç ”ç©¶: {chapter_title}")

    # è·å–ä»»åŠ¡è§„åˆ’å™¨é…ç½®
    task_planner_config = settings.get_agent_component_config("task_planner")
    if not task_planner_config:
        raise ValueError("Task planner configuration not found")

    # åˆ›å»ºç®€åŒ–çš„ç ”ç©¶è®¡åˆ’ç”Ÿæˆçš„ prompt
    prompt = FAST_PLANNER_PROMPT.format(
        topic=topic,
        chapter_title=chapter_title,
        chapter_description=chapter_description)

    logger.debug(
        f"Invoking LLM with fast planner prompt:\n{pprint.pformat(prompt)}")

    try:
        # è°ƒç”¨ LLM ç”Ÿæˆç ”ç©¶è®¡åˆ’
        response = llm_client.invoke(
            prompt,
            temperature=task_planner_config.temperature,
            max_tokens=min(task_planner_config.max_tokens, 800),  # å‡å°‘token
            **task_planner_config.extra_params)

        logger.debug(f"ğŸ” LLMåŸå§‹å“åº”: {repr(response)}")

        # è§£æ JSON å“åº”
        research_plan, search_queries = parse_planner_response(response)

        logger.info(f"âœ… ç”Ÿæˆå¿«é€Ÿç ”ç©¶è®¡åˆ’: {len(search_queries)} ä¸ªæœç´¢æŸ¥è¯¢")
        for i, query in enumerate(search_queries, 1):
            logger.debug(f"  {i}. {query}")

        return {
            "research_plan": research_plan,
            "search_queries": search_queries
        }

    except Exception as e:
        # å¦‚æœ LLM è°ƒç”¨å¤±è´¥ï¼Œè¿”å›é»˜è®¤è®¡åˆ’
        logger.error(f"Fast planner node error: {str(e)}")
        default_queries = [
            f"{topic} {chapter_title} æ¦‚è¿°", f"{topic} {chapter_title} ä¸»è¦å†…å®¹"
        ]
        logger.warning(f"âš ï¸  ä½¿ç”¨é»˜è®¤å¿«é€Ÿæœç´¢æŸ¥è¯¢: {len(default_queries)} ä¸ª")

        return {
            "research_plan": f"å¿«é€Ÿç ”ç©¶è®¡åˆ’ï¼šå¯¹ç« èŠ‚ {chapter_title} è¿›è¡Œç®€è¦ç ”ç©¶ï¼Œæ”¶é›†å…³é”®ä¿¡æ¯ã€‚",
            "search_queries": default_queries
        }


async def fast_researcher_node(state: ResearchState,
                               web_search_tool: WebSearchTool,
                               es_search_tool: ESSearchTool,
                               reranker_tool: RerankerTool = None) -> dict:
    """
    å¿«é€Ÿç ”ç©¶èŠ‚ç‚¹ - ç®€åŒ–ç‰ˆæœ¬
    ä»çŠ¶æ€ä¸­è·å– search_queriesï¼Œä½¿ç”¨æœç´¢å·¥å…·æ”¶é›†ç›¸å…³ä¿¡æ¯
    """
    search_queries = state.get("search_queries", [])

    if not search_queries:
        logger.warning("âŒ æ²¡æœ‰æœç´¢æŸ¥è¯¢ï¼Œè¿”å›é»˜è®¤æ¶ˆæ¯")
        return {"gathered_data": "æ²¡æœ‰æœç´¢æŸ¥è¯¢éœ€è¦æ‰§è¡Œ"}

    all_results = []

    # è·å–embeddingé…ç½®
    embedding_config = settings.supported_models.get("gte_qwen")
    embedding_client = None
    if embedding_config:
        try:
            embedding_client = EmbeddingClient(
                base_url=embedding_config.url,
                api_key=embedding_config.api_key)
            logger.info("âœ… Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸  Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            embedding_client = None

    # æ‰§è¡Œæœç´¢ - åªå¤„ç†å‰2ä¸ªæŸ¥è¯¢
    limited_queries = search_queries[:2]  # é™åˆ¶æŸ¥è¯¢æ•°é‡
    for i, query in enumerate(limited_queries, 1):
        logger.info(f"æ‰§è¡Œå¿«é€Ÿæœç´¢æŸ¥è¯¢ {i}/{len(limited_queries)}: {query}")

        # ç½‘ç»œæœç´¢
        web_results = ""
        try:
            # ä½¿ç”¨å¼‚æ­¥æœç´¢æ–¹æ³•
            web_results = await web_search_tool.search_async(query)
            if "æ¨¡æ‹Ÿ" in web_results or "mock" in web_results.lower():
                web_results = ""
        except Exception as e:
            logger.error(f"ç½‘ç»œæœç´¢å¤±è´¥: {str(e)}")
            web_results = ""

        # ESæœç´¢ - ä½¿ç”¨ç®€åŒ–çš„æœç´¢
        es_results = ""
        try:
            if embedding_client:
                # å°è¯•å‘é‡æ£€ç´¢
                try:
                    embedding_response = embedding_client.invoke(query)
                    embedding_data = json.loads(embedding_response)

                    # è§£æå‘é‡
                    if isinstance(embedding_data, list):
                        query_vector = embedding_data[0] if len(
                            embedding_data) > 0 and isinstance(
                                embedding_data[0], list) else embedding_data
                    elif isinstance(embedding_data,
                                    dict) and 'data' in embedding_data:
                        query_vector = embedding_data['data']
                    else:
                        query_vector = None

                    if query_vector:
                        # ä½¿ç”¨å‘é‡æ£€ç´¢
                        es_results = await search_and_rerank(
                            query, es_search_tool, reranker_tool, query_vector)
                        logger.info(f"âœ… å‘é‡æ£€ç´¢+é‡æ’åºæ‰§è¡ŒæˆåŠŸï¼Œç»“æœé•¿åº¦: {len(es_results)}")
                    else:
                        # å›é€€åˆ°æ–‡æœ¬æœç´¢
                        es_results = es_search_tool.search(query)
                        logger.info(f"âœ… æ–‡æœ¬æœç´¢æ‰§è¡ŒæˆåŠŸï¼Œç»“æœé•¿åº¦: {len(es_results)}")

                except Exception as e:
                    logger.warning(f"âš ï¸  å‘é‡æ£€ç´¢å¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬æœç´¢: {str(e)}")
                    es_results = es_search_tool.search(query)
            else:
                # ç›´æ¥ä½¿ç”¨æ–‡æœ¬æœç´¢
                es_results = es_search_tool.search(query)

        except Exception as e:
            logger.error(f"ESæœç´¢å¤±è´¥: {str(e)}")
            es_results = ""

        # åˆå¹¶ç»“æœ
        combined_results = f"=== æŸ¥è¯¢: {query} ===\n"
        if web_results:
            combined_results += f"ç½‘ç»œæœç´¢ç»“æœ:\n{web_results}\n\n"
        if es_results:
            combined_results += f"çŸ¥è¯†åº“æœç´¢ç»“æœ:\n{es_results}\n\n"

        all_results.append(combined_results)

    # åˆå¹¶æ‰€æœ‰ç»“æœ
    gathered_data = "\n".join(all_results)
    logger.info(f"âœ… æ”¶é›†åˆ° {len(limited_queries)} æ¡å¿«é€Ÿæœç´¢ç»“æœ")
    logger.info(f"ğŸ“Š æ€»æ•°æ®é•¿åº¦: {len(gathered_data)} å­—ç¬¦")

    return {"gathered_data": gathered_data}


def fast_writer_node(state: ResearchState, llm_client: LLMClient) -> dict:
    """
    å¿«é€Ÿç« èŠ‚å†™ä½œèŠ‚ç‚¹ - ç®€åŒ–ç‰ˆæœ¬
    åŸºäºå½“å‰ç« èŠ‚çš„ç ”ç©¶æ•°æ®ï¼Œç”Ÿæˆç®€æ´çš„ç« èŠ‚å†…å®¹
    """
    # è·å–åŸºæœ¬ä¿¡æ¯
    topic = state.get("topic", "")
    current_chapter_index = state.get("current_chapter_index", 0)
    chapters_to_process = state.get("chapters_to_process", [])
    completed_chapters_content = state.get("completed_chapters_content", [])

    # éªŒè¯å½“å‰ç« èŠ‚ç´¢å¼•
    if current_chapter_index >= len(chapters_to_process):
        raise ValueError(f"ç« èŠ‚ç´¢å¼• {current_chapter_index} è¶…å‡ºèŒƒå›´")

    # è·å–å½“å‰ç« èŠ‚ä¿¡æ¯
    current_chapter = chapters_to_process[current_chapter_index]
    chapter_title = current_chapter.get("chapter_title", "")
    chapter_description = current_chapter.get("description", "")

    # ä»çŠ¶æ€ä¸­è·å–ç ”ç©¶æ•°æ®
    gathered_data = state.get("gathered_data", "")

    if not chapter_title:
        raise ValueError("ç« èŠ‚æ ‡é¢˜ä¸èƒ½ä¸ºç©º")

    if not gathered_data:
        return {
            "final_document": f"## {chapter_title}\n\nç”±äºæ²¡æœ‰æ”¶é›†åˆ°ç›¸å…³æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆç« èŠ‚å†…å®¹ã€‚"
        }

    # è·å–æ–‡æ¡£ç”Ÿæˆå™¨é…ç½®
    document_writer_config = settings.get_agent_component_config(
        "document_writer")
    if not document_writer_config:
        temperature = 0.7
        max_tokens = 2000  # å‡å°‘tokenæ•°é‡
        extra_params = {}
    else:
        temperature = document_writer_config.temperature
        max_tokens = min(document_writer_config.max_tokens, 2000)  # é™åˆ¶æœ€å¤§token
        extra_params = document_writer_config.extra_params

    # æ„å»ºå·²å®Œæˆç« èŠ‚çš„ä¸Šä¸‹æ–‡æ‘˜è¦ - ç®€åŒ–ç‰ˆæœ¬
    previous_chapters_context = ""
    if completed_chapters_content:
        previous_chapters_context = "\n\n".join([
            f"ç¬¬{i+1}ç« æ‘˜è¦:\n{content[:200]}..."  # å‡å°‘æ‘˜è¦é•¿åº¦
            for i, content in enumerate(completed_chapters_content)
        ])

    # å¯¼å…¥å¿«é€Ÿæç¤ºè¯æ¨¡æ¿
    from ..fast_prompts import FAST_WRITER_PROMPT

    # æ„å»ºç®€åŒ–çš„æç¤ºè¯
    prompt = FAST_WRITER_PROMPT.format(
        topic=topic,
        chapter_title=chapter_title,
        chapter_description=chapter_description,
        chapter_number=current_chapter_index + 1,
        total_chapters=len(chapters_to_process),
        previous_chapters_context=previous_chapters_context
        if previous_chapters_context else "è¿™æ˜¯ç¬¬ä¸€ç« ï¼Œæ²¡æœ‰å‰ç½®å†…å®¹ã€‚",
        gathered_data=gathered_data[:8000]  # é™åˆ¶è¾“å…¥é•¿åº¦
    )

    logger.debug(
        f"Invoking LLM with fast writer prompt:\n{pprint.pformat(prompt)}")

    try:
        # è°ƒç”¨LLMç”Ÿæˆç« èŠ‚å†…å®¹
        response = llm_client.invoke(prompt,
                                     temperature=temperature,
                                     max_tokens=max_tokens,
                                     **extra_params)

        # ç¡®ä¿å“åº”æ ¼å¼æ­£ç¡®
        if not response.strip():
            response = f"## {chapter_title}\n\næ— æ³•ç”Ÿæˆç« èŠ‚å†…å®¹ã€‚"
        elif not response.startswith("##"):
            # å¦‚æœæ²¡æœ‰äºŒçº§æ ‡é¢˜ï¼Œæ·»åŠ ç« èŠ‚æ ‡é¢˜
            response = f"## {chapter_title}\n\n{response}"

        # è¿”å›å½“å‰ç« èŠ‚çš„å†…å®¹
        return {"final_document": response}

    except Exception as e:
        # å¦‚æœLLMè°ƒç”¨å¤±è´¥ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
        logger.error(f"Fast writer node error: {str(e)}")
        error_content = f"""## {chapter_title}

### ç« èŠ‚ç”Ÿæˆé”™è¯¯

ç”±äºæŠ€æœ¯åŸå› ï¼Œæ— æ³•ç”Ÿæˆæœ¬ç« èŠ‚çš„å†…å®¹ã€‚

**é”™è¯¯ä¿¡æ¯:** {str(e)}

**ç« èŠ‚æè¿°:** {chapter_description}

è¯·æ£€æŸ¥ç³»ç»Ÿé…ç½®æˆ–ç¨åé‡è¯•ã€‚
"""
        return {"final_document": error_content}


def fast_supervisor_router(state: ResearchState, llm_client: LLMClient) -> str:
    """
    å¿«é€Ÿç›‘ç£å™¨è·¯ç”± - ç®€åŒ–ç‰ˆæœ¬
    è¯„ä¼°æ”¶é›†çš„ç ”ç©¶æ•°æ®æ˜¯å¦è¶³å¤Ÿæ’°å†™æ–‡æ¡£ï¼Œé™ä½è¦æ±‚
    """
    logger.info("ğŸš€ ====== è¿›å…¥å¿«é€Ÿ supervisor_router è·¯ç”±èŠ‚ç‚¹ ======")

    # ä»çŠ¶æ€ä¸­æå– topic å’Œ gathered_data
    topic = state.get("topic", "")
    gathered_data = state.get("gathered_data", "")

    if not topic:
        logger.warning("âŒ æ²¡æœ‰ä¸»é¢˜ï¼Œè¿”å› rerun_researcher")
        return "rerun_researcher"

    if not gathered_data:
        logger.warning("âŒ æ²¡æœ‰æ”¶é›†åˆ°æ•°æ®ï¼Œè¿”å› rerun_researcher")
        return "rerun_researcher"

    # é¢„åˆ†ææ­¥éª¤ï¼šè®¡ç®—å…ƒæ•°æ®
    num_sources = gathered_data.count("===")
    total_length = len(gathered_data)

    logger.info(f"ğŸ“‹ Topic: {topic}")
    logger.info(f"ğŸ“Š Gathered data é•¿åº¦: {total_length} å­—ç¬¦")
    logger.info(f"ğŸ” æ¥æºæ•°é‡: {num_sources}")

    # å¯¼å…¥å¿«é€Ÿæç¤ºè¯æ¨¡æ¿
    from ..fast_prompts import FAST_SUPERVISOR_PROMPT

    # æ„å»ºç®€åŒ–çš„è¯„ä¼°æç¤ºè¯
    prompt = FAST_SUPERVISOR_PROMPT.format(topic=topic,
                                           num_sources=num_sources,
                                           total_length=total_length)

    logger.debug(
        f"Invoking LLM with fast supervisor prompt:\n{pprint.pformat(prompt)}")

    try:
        # è°ƒç”¨ LLM å®¢æˆ·ç«¯
        max_tokens = 10

        logger.info("ğŸ¤– è°ƒç”¨ LLM è¿›è¡Œå¿«é€Ÿå†³ç­–åˆ¤æ–­...")
        logger.debug(f"ğŸ“ Prompt é•¿åº¦: {len(prompt)} å­—ç¬¦")

        # æ·»åŠ é‡è¯•æœºåˆ¶
        max_retries = 2  # å‡å°‘é‡è¯•æ¬¡æ•°
        for attempt in range(max_retries):
            try:
                response = llm_client.invoke(prompt,
                                             temperature=0,
                                             max_tokens=max_tokens)
                break
            except Exception as e:
                if "400" in str(e) and attempt < max_retries - 1:
                    logger.warning(
                        f"âš ï¸  ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥ (400é”™è¯¯)ï¼Œæ­£åœ¨é‡è¯•...")
                    import time
                    time.sleep(1)  # å‡å°‘ç­‰å¾…æ—¶é—´
                    continue
                else:
                    raise e

        # è§£æå“åº”
        decision = response.strip().upper()
        clean_response = response

        # å¦‚æœå“åº”è¢«æˆªæ–­æˆ–åŒ…å«æ¨ç†è¿‡ç¨‹ï¼Œå°è¯•æå–å†³ç­–å…³é”®è¯
        if "FINISH" not in decision and "CONTINUE" not in decision:
            import re
            clean_response = re.sub(r'<think>.*',
                                    '',
                                    response,
                                    flags=re.IGNORECASE)
            clean_response = re.sub(r'<THINK>.*',
                                    '',
                                    clean_response,
                                    flags=re.IGNORECASE)
            decision = clean_response.strip().upper()

        logger.debug(f"ğŸ” LLMåŸå§‹å“åº”: '{response}'")
        logger.debug(f"ğŸ” å¤„ç†åå†³ç­–: '{decision}'")

        # æ ¹æ®å“åº”å†³å®šè·¯ç”±
        if "FINISH" in decision:
            logger.info("âœ… å†³ç­–: FINISH -> continue_to_writer")
            return "continue_to_writer"
        else:
            logger.info("âœ… å†³ç­–: CONTINUE/å…¶ä»– -> rerun_researcher")
            return "rerun_researcher"

    except Exception as e:
        # å¦‚æœ LLM è°ƒç”¨å¤±è´¥ï¼Œé»˜è®¤ç»§ç»­ç ”ç©¶ä»¥ç¡®ä¿å®‰å…¨
        logger.error(f"âŒ Fast supervisor router error: {str(e)}")
        return "rerun_researcher"


def parse_planner_response(response: str) -> tuple:
    """
    è§£æè§„åˆ’å™¨å“åº” - ç®€åŒ–ç‰ˆæœ¬
    """
    try:
        # æ¸…ç†å“åº”
        cleaned_response = response.strip()
        if cleaned_response.startswith("```json"):
            cleaned_response = cleaned_response[7:]
        if cleaned_response.startswith("```"):
            cleaned_response = cleaned_response[3:]
        if cleaned_response.endswith("```"):
            cleaned_response = cleaned_response[:-3]

        # è§£æJSON
        data = json.loads(cleaned_response.strip())

        research_plan = data.get("research_plan", "")
        search_queries = data.get("search_queries", [])

        # ç¡®ä¿search_queriesæ˜¯åˆ—è¡¨
        if isinstance(search_queries, str):
            search_queries = [search_queries]

        return research_plan, search_queries

    except Exception as e:
        logger.error(f"è§£æè§„åˆ’å™¨å“åº”å¤±è´¥: {str(e)}")
        logger.debug(f"åŸå§‹å“åº”: {repr(response)}")

        # è¿”å›é»˜è®¤å€¼
        return "å¿«é€Ÿç ”ç©¶è®¡åˆ’", ["æ¦‚è¿°", "ä¸»è¦å†…å®¹"]
