# service/src/doc_agent/graph/main_orchestrator/builder.py
from loguru import logger
import pprint
from typing import Dict, Any
from langgraph.graph import StateGraph, END
from ..state import ResearchState
from . import nodes


def create_chapter_processing_node(chapter_workflow_graph):
    """
    åˆ›å»ºç« èŠ‚å¤„ç†èŠ‚ç‚¹çš„å·¥å‚å‡½æ•°
    
    Args:
        chapter_workflow_graph: ç¼–è¯‘åçš„ç« èŠ‚å·¥ä½œæµå›¾
        
    Returns:
        ç« èŠ‚å¤„ç†èŠ‚ç‚¹å‡½æ•°
    """

    async def chapter_processing_node(state: ResearchState) -> dict:
        """
        ç« èŠ‚å¤„ç†èŠ‚ç‚¹
        
        è°ƒç”¨ç« èŠ‚å­å·¥ä½œæµå¤„ç†å½“å‰ç« èŠ‚ï¼Œå¹¶æ›´æ–°çŠ¶æ€
        
        Args:
            state: ç ”ç©¶çŠ¶æ€
            
        Returns:
            dict: æ›´æ–°åçš„çŠ¶æ€å­—æ®µ
        """
        # è·å–å½“å‰çŠ¶æ€ä¿¡æ¯
        current_chapter_index = state.get("current_chapter_index", 0)
        chapters_to_process = state.get("chapters_to_process", [])
        completed_chapters_content = state.get("completed_chapters_content",
                                               [])
        topic = state.get("topic", "")

        # éªŒè¯ç´¢å¼•
        if current_chapter_index >= len(chapters_to_process):
            raise ValueError(f"ç« èŠ‚ç´¢å¼• {current_chapter_index} è¶…å‡ºèŒƒå›´")

        # è·å–å½“å‰ç« èŠ‚
        current_chapter = chapters_to_process[current_chapter_index]
        chapter_title = current_chapter.get("chapter_title", "")

        logger.info(
            f"\nğŸ“– å¼€å§‹å¤„ç†ç¬¬ {current_chapter_index + 1}/{len(chapters_to_process)} ç« : {chapter_title}"
        )

        # å‡†å¤‡å­å·¥ä½œæµçš„è¾“å…¥çŠ¶æ€
        # å…³é”®ï¼šä¼ é€’å·²å®Œæˆç« èŠ‚çš„å†…å®¹ä»¥ä¿æŒè¿è´¯æ€§
        chapter_workflow_input = {
            "topic": topic,
            "current_chapter_index": current_chapter_index,
            "chapters_to_process": chapters_to_process,
            "completed_chapters_content":
            completed_chapters_content,  # å…³é”®ï¼šä¼ é€’ä¸Šä¸‹æ–‡
            "search_queries": [],  # åˆå§‹åŒ–æœç´¢æŸ¥è¯¢ï¼ŒplannerèŠ‚ç‚¹ä¼šç”Ÿæˆ
            "research_plan": "",  # åˆå§‹åŒ–ç ”ç©¶è®¡åˆ’ï¼ŒplannerèŠ‚ç‚¹ä¼šç”Ÿæˆ
            "gathered_sources": [],  # åˆå§‹åŒ–æ”¶é›†çš„æºæ•°æ®ï¼ŒresearcherèŠ‚ç‚¹ä¼šå¡«å……
            "gathered_data": "",  # ä¿æŒå‘åå…¼å®¹
            "messages": []  # æ–°çš„æ¶ˆæ¯å†å²
        }

        logger.debug(
            f"Chapter workflow input state:\n{pprint.pformat(chapter_workflow_input)}"
        )

        try:
            # è°ƒç”¨ç« èŠ‚å·¥ä½œæµ
            logger.info(f"ğŸ”„ è°ƒç”¨ç« èŠ‚å·¥ä½œæµå¤„ç†: {chapter_title}")
            chapter_result = await chapter_workflow_graph.ainvoke(
                chapter_workflow_input)

            # è°ƒè¯•ï¼šæ‰“å°ç« èŠ‚å·¥ä½œæµçš„å®Œæ•´è¾“å‡º
            logger.info(f"ğŸ“Š ç« èŠ‚å·¥ä½œæµè¾“å‡ºé”®: {list(chapter_result.keys())}")
            logger.info(f"ğŸ“Š ç« èŠ‚å·¥ä½œæµè¾“å‡º: {chapter_result}")

            # ä»ç»“æœä¸­æå–ç« èŠ‚å†…å®¹å’Œå¼•ç”¨æº
            chapter_content = chapter_result.get("final_document", "")
            cited_sources_in_chapter = chapter_result.get(
                "cited_sources_in_chapter", [])

            if not chapter_content:
                logger.warning(f"âš ï¸  ç« èŠ‚å·¥ä½œæµæœªè¿”å›å†…å®¹ï¼Œä½¿ç”¨é»˜è®¤å†…å®¹")
                chapter_content = f"## {chapter_title}\n\nç« èŠ‚å†…å®¹ç”Ÿæˆå¤±è´¥ã€‚"

            logger.info(f"âœ… ç« èŠ‚å¤„ç†å®Œæˆï¼Œå†…å®¹é•¿åº¦: {len(chapter_content)} å­—ç¬¦")
            logger.info(f"ğŸ“š ç« èŠ‚å¼•ç”¨æºæ•°é‡: {len(cited_sources_in_chapter)}")

            # æ›´æ–°å·²å®Œæˆç« èŠ‚åˆ—è¡¨
            updated_completed_chapters = completed_chapters_content.copy()
            updated_completed_chapters.append(chapter_content)

            # æ›´æ–°ç« èŠ‚ç´¢å¼•
            updated_chapter_index = current_chapter_index + 1

            # æ›´æ–°å…¨å±€å¼•ç”¨æº
            current_cited_sources = state.get("cited_sources", {})
            updated_cited_sources = current_cited_sources.copy()

            # å°†ç« èŠ‚çš„å¼•ç”¨æºæ·»åŠ åˆ°å…¨å±€å¼•ç”¨æºä¸­
            for source in cited_sources_in_chapter:
                if hasattr(source, 'id'):
                    updated_cited_sources[source.id] = source
                    logger.debug(f"ğŸ“š æ·»åŠ å¼•ç”¨æºåˆ°å…¨å±€: [{source.id}] {source.title}")

            logger.info(
                f"ğŸ“Š è¿›åº¦: {updated_chapter_index}/{len(chapters_to_process)} ç« èŠ‚å·²å®Œæˆ"
            )
            logger.info(f"ğŸ“š å…¨å±€å¼•ç”¨æºæ€»æ•°: {len(updated_cited_sources)}")

            return {
                "completed_chapters_content": updated_completed_chapters,
                "current_chapter_index": updated_chapter_index,
                "cited_sources": updated_cited_sources
            }

        except Exception as e:
            logger.error(f"âŒ ç« èŠ‚å¤„ç†å¤±è´¥: {str(e)}")
            # å¤±è´¥æ—¶ä»ç„¶æ¨è¿›ç´¢å¼•ï¼Œé¿å…æ— é™å¾ªç¯
            return {
                "completed_chapters_content":
                completed_chapters_content +
                [f"## {chapter_title}\n\nç« èŠ‚å¤„ç†å¤±è´¥: {str(e)}"],
                "current_chapter_index":
                current_chapter_index + 1
            }

    return chapter_processing_node


def chapter_decision_function(state: ResearchState) -> str:
    """
    å†³ç­–å‡½æ•°ï¼šåˆ¤æ–­æ˜¯å¦è¿˜æœ‰ç« èŠ‚éœ€è¦å¤„ç†
    
    Args:
        state: ç ”ç©¶çŠ¶æ€
        
    Returns:
        str: "process_chapter" æˆ– "finalize_document"
    """
    current_chapter_index = state.get("current_chapter_index", 0)
    chapters_to_process = state.get("chapters_to_process", [])

    logger.info(
        f"\nğŸ¤” ç« èŠ‚å¤„ç†å†³ç­–: {current_chapter_index}/{len(chapters_to_process)}")

    if current_chapter_index < len(chapters_to_process):
        logger.info(f"â¡ï¸  ç»§ç»­å¤„ç†ç¬¬ {current_chapter_index + 1} ç« ")
        return "process_chapter"
    else:
        logger.info(f"âœ… æ‰€æœ‰ç« èŠ‚å·²å¤„ç†å®Œæˆ")
        return "finalize_document"


def finalize_document_node(state: ResearchState) -> dict:
    """
    æ–‡æ¡£æœ€ç»ˆåŒ–èŠ‚ç‚¹
    
    å°†æ‰€æœ‰ç« èŠ‚å†…å®¹åˆå¹¶ä¸ºæœ€ç»ˆæ–‡æ¡£ï¼Œå¹¶è¿›è¡Œæ ¼å¼æ¸…ç†
    
    Args:
        state: ç ”ç©¶çŠ¶æ€
        
    Returns:
        dict: åŒ…å« final_document çš„å­—å…¸
    """
    topic = state.get("topic", "")
    document_outline = state.get("document_outline", {})
    completed_chapters_content = state.get("completed_chapters_content", [])

    logger.info(f"\nğŸ“‘ å¼€å§‹ç”Ÿæˆæœ€ç»ˆæ–‡æ¡£")

    # è·å–æ–‡æ¡£æ ‡é¢˜å’Œæ‘˜è¦
    doc_title = document_outline.get("title", topic)
    doc_summary = document_outline.get("summary", "")

    # æ„å»ºæœ€ç»ˆæ–‡æ¡£
    final_document_parts = []

    # æ·»åŠ æ ‡é¢˜
    final_document_parts.append(f"# {doc_title}\n")

    # æ·»åŠ æ‘˜è¦
    if doc_summary:
        final_document_parts.append(f"## æ‘˜è¦\n\n{doc_summary}\n")

    # æ·»åŠ ç›®å½•
    final_document_parts.append("\n## ç›®å½•\n")
    chapters = document_outline.get("chapters", [])
    for i, chapter in enumerate(chapters):
        chapter_title = chapter.get("chapter_title", f"ç¬¬{i+1}ç« ")
        final_document_parts.append(f"{i+1}. {chapter_title}\n")

    final_document_parts.append("\n---\n")

    # æ·»åŠ æ‰€æœ‰ç« èŠ‚å†…å®¹ï¼ˆè¿›è¡Œæ ¼å¼æ¸…ç†ï¼‰
    for chapter_content in completed_chapters_content:
        cleaned_content = _clean_chapter_content(chapter_content)
        final_document_parts.append(f"\n{cleaned_content}\n")
        final_document_parts.append("\n---\n")

    # åˆå¹¶ä¸ºæœ€ç»ˆæ–‡æ¡£
    final_document = "\n".join(final_document_parts)

    logger.info(f"âœ… æœ€ç»ˆæ–‡æ¡£ç”Ÿæˆå®Œæˆï¼Œæ€»é•¿åº¦: {len(final_document)} å­—ç¬¦")
    logger.info(f"ğŸ“Š åŒ…å« {len(completed_chapters_content)} ä¸ªç« èŠ‚")

    return {"final_document": final_document}


def _clean_chapter_content(content: str) -> str:
    """
    æ¸…ç†ç« èŠ‚å†…å®¹æ ¼å¼
    
    Args:
        content: åŸå§‹ç« èŠ‚å†…å®¹
        
    Returns:
        str: æ¸…ç†åçš„å†…å®¹
    """
    if not content:
        return content

    # 1. ç§»é™¤ markdown ä»£ç å—æ ‡è®°
    # ç§»é™¤å¼€å¤´çš„ ```markdown æˆ– ``` æ ‡è®°
    content = content.strip()
    if content.startswith("```markdown"):
        content = content[11:]  # ç§»é™¤ ```markdown
    elif content.startswith("```"):
        content = content[3:]  # ç§»é™¤ ```

    # ç§»é™¤ç»“å°¾çš„ ``` æ ‡è®°
    if content.endswith("```"):
        content = content[:-3]

    # 2. è°ƒæ•´æ ‡é¢˜å±‚çº§
    lines = content.split('\n')
    cleaned_lines = []

    for line in lines:
        # å°†ä¸€çº§æ ‡é¢˜ (# æ ‡é¢˜) é™çº§ä¸ºäºŒçº§æ ‡é¢˜ (## æ ‡é¢˜)
        if line.startswith('# ') and not line.startswith('## '):
            # è¿™æ˜¯ä¸€çº§æ ‡é¢˜ï¼Œéœ€è¦é™çº§
            line = '#' + line  # æ·»åŠ ä¸€ä¸ª # å˜æˆäºŒçº§æ ‡é¢˜

        # å°†äºŒçº§æ ‡é¢˜ (## æ ‡é¢˜) é™çº§ä¸ºä¸‰çº§æ ‡é¢˜ (### æ ‡é¢˜)
        elif line.startswith('## ') and not line.startswith('### '):
            # è¿™æ˜¯äºŒçº§æ ‡é¢˜ï¼Œéœ€è¦é™çº§
            line = '#' + line  # æ·»åŠ ä¸€ä¸ª # å˜æˆä¸‰çº§æ ‡é¢˜

        # å°†ä¸‰çº§æ ‡é¢˜ (### æ ‡é¢˜) é™çº§ä¸ºå››çº§æ ‡é¢˜ (#### æ ‡é¢˜)
        elif line.startswith('### ') and not line.startswith('#### '):
            # è¿™æ˜¯ä¸‰çº§æ ‡é¢˜ï¼Œéœ€è¦é™çº§
            line = '#' + line  # æ·»åŠ ä¸€ä¸ª # å˜æˆå››çº§æ ‡é¢˜

        cleaned_lines.append(line)

    # é‡æ–°ç»„åˆå†…å®¹
    cleaned_content = '\n'.join(cleaned_lines)

    # 3. ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
    # å°†è¿ç»­çš„ç©ºè¡Œå‹ç¼©ä¸ºæœ€å¤šä¸¤ä¸ªç©ºè¡Œ
    import re
    cleaned_content = re.sub(r'\n{3,}', '\n\n', cleaned_content)

    return cleaned_content.strip()


def build_main_orchestrator_graph(initial_research_node,
                                  outline_generation_node,
                                  split_chapters_node,
                                  chapter_workflow_graph,
                                  finalize_document_node_func=None,
                                  bibliography_node_func=None):
    """
    æ„å»ºä¸»ç¼–æ’å™¨å›¾
    
    ä¸»å·¥ä½œæµç¨‹ï¼š
    1. åˆå§‹ç ”ç©¶ -> ç”Ÿæˆå¤§çº² -> æ‹†åˆ†ç« èŠ‚
    2. å¾ªç¯å¤„ç†æ¯ä¸ªç« èŠ‚ï¼ˆè°ƒç”¨ç« èŠ‚å­å·¥ä½œæµï¼‰
    3. æ‰€æœ‰ç« èŠ‚å®Œæˆåï¼Œç”Ÿæˆæœ€ç»ˆæ–‡æ¡£
    4. ç”Ÿæˆå‚è€ƒæ–‡çŒ®
    
    Args:
        initial_research_node: å·²ç»‘å®šä¾èµ–çš„åˆå§‹ç ”ç©¶èŠ‚ç‚¹
        outline_generation_node: å·²ç»‘å®šä¾èµ–çš„å¤§çº²ç”ŸæˆèŠ‚ç‚¹
        split_chapters_node: ç« èŠ‚æ‹†åˆ†èŠ‚ç‚¹
        chapter_workflow_graph: ç¼–è¯‘åçš„ç« èŠ‚å·¥ä½œæµå›¾
        finalize_document_node_func: å¯é€‰çš„æ–‡æ¡£æœ€ç»ˆåŒ–èŠ‚ç‚¹å‡½æ•°
        bibliography_node_func: å¯é€‰çš„å‚è€ƒæ–‡çŒ®ç”ŸæˆèŠ‚ç‚¹å‡½æ•°
        
    Returns:
        CompiledGraph: ç¼–è¯‘åçš„ä¸»ç¼–æ’å™¨å›¾
    """
    # åˆ›å»ºçŠ¶æ€å›¾
    workflow = StateGraph(ResearchState)

    # åˆ›å»ºç« èŠ‚å¤„ç†èŠ‚ç‚¹
    chapter_processing_node = create_chapter_processing_node(
        chapter_workflow_graph)

    # ä½¿ç”¨æä¾›çš„æˆ–é»˜è®¤çš„æ–‡æ¡£æœ€ç»ˆåŒ–èŠ‚ç‚¹
    if finalize_document_node_func is None:
        finalize_document_node_func = finalize_document_node

    # ä½¿ç”¨æä¾›çš„æˆ–é»˜è®¤çš„å‚è€ƒæ–‡çŒ®ç”ŸæˆèŠ‚ç‚¹
    if bibliography_node_func is None:
        bibliography_node_func = nodes.bibliography_node

    # æ³¨å†Œæ‰€æœ‰èŠ‚ç‚¹
    workflow.add_node("initial_research", initial_research_node)
    workflow.add_node("outline_generation", outline_generation_node)
    workflow.add_node("split_chapters", split_chapters_node)
    workflow.add_node("chapter_processing", chapter_processing_node)
    workflow.add_node("finalize_document", finalize_document_node_func)
    workflow.add_node("generate_bibliography", bibliography_node_func)

    # è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("initial_research")

    # æ·»åŠ é¡ºåºè¾¹
    workflow.add_edge("initial_research", "outline_generation")
    workflow.add_edge("outline_generation", "split_chapters")

    # ä» split_chapters åˆ°æ¡ä»¶å†³ç­–ç‚¹
    workflow.add_conditional_edges(
        "split_chapters", chapter_decision_function, {
            "process_chapter": "chapter_processing",
            "finalize_document": "finalize_document"
        })

    # ç« èŠ‚å¤„ç†å®Œæˆåï¼Œå›åˆ°æ¡ä»¶å†³ç­–ç‚¹ï¼ˆå½¢æˆå¾ªç¯ï¼‰
    workflow.add_conditional_edges(
        "chapter_processing",
        chapter_decision_function,
        {
            "process_chapter": "chapter_processing",  # ç»§ç»­å¤„ç†ä¸‹ä¸€ç« 
            "finalize_document": "finalize_document"  # æ‰€æœ‰ç« èŠ‚å®Œæˆ
        })

    # æœ€ç»ˆåŒ–åè¿›å…¥å‚è€ƒæ–‡çŒ®ç”Ÿæˆ
    workflow.add_edge("finalize_document", "generate_bibliography")

    # å‚è€ƒæ–‡çŒ®ç”Ÿæˆåç»“æŸ
    workflow.add_edge("generate_bibliography", END)

    # ç¼–è¯‘å¹¶è¿”å›å›¾
    logger.info("ğŸ—ï¸  ä¸»ç¼–æ’å™¨å›¾æ„å»ºå®Œæˆ")
    return workflow.compile()
