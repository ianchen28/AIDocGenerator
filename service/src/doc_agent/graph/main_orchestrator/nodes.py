# service/src/doc_agent/graph/main_orchestrator/nodes.py
from loguru import logger
import pprint
from typing import Dict, List
import json
from ..state import ResearchState
from ...llm_clients.base import LLMClient
from ...tools.web_search import WebSearchTool
from ...tools.es_search import ESSearchTool
from ...tools.reranker import RerankerTool
from ...llm_clients.providers import EmbeddingClient, RerankerClient
from ...common.prompt_selector import PromptSelector
from ...schemas import Source

# æ·»åŠ é…ç½®å¯¼å…¥
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_file = Path(__file__)
service_dir = None
for parent in current_file.parents:
    if parent.name == 'service':
        service_dir = parent
        break

if service_dir and str(service_dir) not in sys.path:
    sys.path.insert(0, str(service_dir))

from core.config import settings
from src.doc_agent.utils.search_utils import search_and_rerank


async def initial_research_node(state: ResearchState,
                                web_search_tool: WebSearchTool,
                                es_search_tool: ESSearchTool,
                                reranker_tool: RerankerTool = None,
                                llm_client: LLMClient = None) -> dict:
    """
    åˆå§‹ç ”ç©¶èŠ‚ç‚¹
    
    åŸºäºä¸»é¢˜è¿›è¡Œåˆå§‹ç ”ç©¶ï¼Œæ”¶é›†ç›¸å…³ä¿¡æ¯æº
    
    Args:
        state: ç ”ç©¶çŠ¶æ€ï¼ŒåŒ…å« topic
        web_search_tool: ç½‘ç»œæœç´¢å·¥å…·
        es_search_tool: ESæœç´¢å·¥å…·
        reranker_tool: é‡æ’åºå·¥å…·
        llm_client: LLMå®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        dict: åŒ…å« initial_sources çš„å­—å…¸ï¼ŒåŒ…å« Source å¯¹è±¡åˆ—è¡¨
    """
    topic = state.get("topic", "")
    if not topic:
        raise ValueError("ä¸»é¢˜ä¸èƒ½ä¸ºç©º")

    logger.info(f"ğŸ” å¼€å§‹åˆå§‹ç ”ç©¶: {topic}")

    # ä»é…ç½®ä¸­è¯»å–æœç´¢è½®æ•°
    from service.core.config import settings

    # ç”Ÿæˆåˆå§‹æœç´¢æŸ¥è¯¢ - æ›´é€šç”¨å’Œå¹¿æ³›çš„æŸ¥è¯¢
    # æ ¹æ®é…ç½®å†³å®šæŸ¥è¯¢æ•°é‡
    search_config = getattr(settings, 'search_config', None)
    if search_config and hasattr(search_config, 'max_search_rounds'):
        max_search_rounds = search_config.max_search_rounds
    else:
        max_search_rounds = 5  # é»˜è®¤5è½®

    # å®šä¹‰æ‰€æœ‰å¯èƒ½çš„æŸ¥è¯¢
    all_possible_queries = [
        f"{topic} æ¦‚è¿°",
        f"{topic} ä¸»è¦å†…å®¹",
        f"{topic} å…³é”®è¦ç‚¹",
        f"{topic} æœ€æ–°å‘å±•",
        f"{topic} é‡è¦æ€§",
    ]

    # æ ¹æ®é…ç½®é€‰æ‹©æŸ¥è¯¢æ•°é‡
    initial_queries = all_possible_queries[:max_search_rounds]

    logger.info(
        f"ğŸ“Š é…ç½®æœç´¢è½®æ•°: {max_search_rounds}ï¼Œå®é™…æ‰§è¡Œ: {len(initial_queries)} è½®")

    all_sources = []  # å­˜å‚¨æ‰€æœ‰ Source å¯¹è±¡
    source_id_counter = 1  # æºIDè®¡æ•°å™¨

    # è·å–embeddingé…ç½®
    embedding_config = settings.supported_models.get("gte_qwen")
    embedding_client = None
    if embedding_config:
        try:
            embedding_client = EmbeddingClient(
                base_url=embedding_config.url,
                api_key=embedding_config.api_key)
            logger.info("âœ… Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸  Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            embedding_client = None

    # æ‰§è¡Œæœç´¢
    for i, query in enumerate(initial_queries, 1):
        logger.info(f"æ‰§è¡Œåˆå§‹æœç´¢ {i}/{len(initial_queries)}: {query}")

        # ç½‘ç»œæœç´¢
        web_results = ""
        try:
            # ä½¿ç”¨å¼‚æ­¥æœç´¢æ–¹æ³•
            web_results = await web_search_tool.search_async(query)
            if "æ¨¡æ‹Ÿ" in web_results or "mock" in web_results.lower():
                web_results = ""
        except Exception as e:
            logger.error(f"ç½‘ç»œæœç´¢å¤±è´¥: {str(e)}")
            web_results = ""

        # ESæœç´¢ - ä½¿ç”¨æ–°çš„æœç´¢å’Œé‡æ’åºåŠŸèƒ½
        es_results = ""
        try:
            if embedding_client:
                # å°è¯•å‘é‡æ£€ç´¢
                try:
                    embedding_response = embedding_client.invoke(query)
                    embedding_data = json.loads(embedding_response)

                    # è§£æå‘é‡
                    if isinstance(embedding_data, list):
                        query_vector = embedding_data[0] if len(
                            embedding_data) > 0 and isinstance(
                                embedding_data[0], list) else embedding_data
                    elif isinstance(embedding_data,
                                    dict) and 'data' in embedding_data:
                        query_vector = embedding_data['data']
                    else:
                        query_vector = None

                    if query_vector and len(query_vector) == 1536:
                        # ä½¿ç”¨æ–°çš„æœç´¢å’Œé‡æ’åºåŠŸèƒ½
                        _, reranked_results, formatted_es_results = await search_and_rerank(
                            es_search_tool=es_search_tool,
                            query=query,
                            query_vector=query_vector,
                            reranker_tool=reranker_tool,
                            initial_top_k=8,  # åˆå§‹ç ”ç©¶è·å–æ›´å¤šç»“æœ
                            final_top_k=5  # é‡æ’åºåè¿”å›top5
                        )
                        es_results = formatted_es_results
                        logger.info(
                            f"âœ… å‘é‡æ£€ç´¢+é‡æ’åºæ‰§è¡ŒæˆåŠŸï¼Œç»“æœé•¿åº¦: {len(formatted_es_results)}"
                        )
                    else:
                        # å›é€€åˆ°æ–‡æœ¬æœç´¢
                        _, reranked_results, formatted_es_results = await search_and_rerank(
                            es_search_tool=es_search_tool,
                            query=query,
                            query_vector=None,
                            reranker_tool=reranker_tool,
                            initial_top_k=8,
                            final_top_k=5)
                        es_results = formatted_es_results
                        logger.info(
                            f"âœ… æ–‡æœ¬æœç´¢+é‡æ’åºæ‰§è¡ŒæˆåŠŸï¼Œç»“æœé•¿åº¦: {len(formatted_es_results)}"
                        )
                except Exception as e:
                    logger.error(f"å‘é‡æ£€ç´¢å¤±è´¥: {str(e)}")
                    # å›é€€åˆ°æ–‡æœ¬æœç´¢
                    _, reranked_results, formatted_es_results = await search_and_rerank(
                        es_search_tool=es_search_tool,
                        query=query,
                        query_vector=None,
                        reranker_tool=reranker_tool,
                        initial_top_k=8,
                        final_top_k=5)
                    es_results = formatted_es_results
                    logger.info(
                        f"âœ… æ–‡æœ¬æœç´¢+é‡æ’åºæ‰§è¡ŒæˆåŠŸï¼Œç»“æœé•¿åº¦: {len(formatted_es_results)}")
            else:
                # æ²¡æœ‰embeddingå®¢æˆ·ç«¯ï¼Œç›´æ¥ä½¿ç”¨æ–‡æœ¬æœç´¢
                _, reranked_results, formatted_es_results = await search_and_rerank(
                    es_search_tool=es_search_tool,
                    query=query,
                    query_vector=None,
                    reranker_tool=reranker_tool,
                    initial_top_k=8,
                    final_top_k=5)
                es_results = formatted_es_results
                logger.info(
                    f"âœ… æ–‡æœ¬æœç´¢+é‡æ’åºæ‰§è¡ŒæˆåŠŸï¼Œç»“æœé•¿åº¦: {len(formatted_es_results)}")

        except Exception as e:
            logger.error(f"ESæœç´¢å¤±è´¥: {str(e)}")
            es_results = f"ESæœç´¢å¤±è´¥: {str(e)}"

        # å¤„ç†ç½‘ç»œæœç´¢ç»“æœ
        if web_results and web_results.strip():
            try:
                # è§£æç½‘ç»œæœç´¢ç»“æœï¼Œåˆ›å»º Source å¯¹è±¡
                web_sources = _parse_web_search_results(
                    web_results, query, source_id_counter)
                all_sources.extend(web_sources)
                source_id_counter += len(web_sources)
                logger.info(f"âœ… ä»ç½‘ç»œæœç´¢ä¸­æå–åˆ° {len(web_sources)} ä¸ªæº")
            except Exception as e:
                logger.error(f"âŒ è§£æç½‘ç»œæœç´¢ç»“æœå¤±è´¥: {str(e)}")

        # å¤„ç†ESæœç´¢ç»“æœ
        if es_results and es_results.strip():
            try:
                # è§£æESæœç´¢ç»“æœï¼Œåˆ›å»º Source å¯¹è±¡
                es_sources = _parse_es_search_results(es_results, query,
                                                      source_id_counter)
                all_sources.extend(es_sources)
                source_id_counter += len(es_sources)
                logger.info(f"âœ… ä»ESæœç´¢ä¸­æå–åˆ° {len(es_sources)} ä¸ªæº")
            except Exception as e:
                logger.error(f"âŒ è§£æESæœç´¢ç»“æœå¤±è´¥: {str(e)}")

    # è¿”å›ç»“æ„åŒ–çš„æºåˆ—è¡¨
    logger.info(f"âœ… åˆå§‹ç ”ç©¶å®Œæˆï¼Œæ€»å…±æ”¶é›†åˆ° {len(all_sources)} ä¸ªä¿¡æ¯æº")
    for i, source in enumerate(all_sources[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ªæºä½œä¸ºé¢„è§ˆ
        logger.debug(
            f"  {i}. [{source.id}] {source.title} ({source.source_type})")

    return {"initial_sources": all_sources}


def outline_generation_node(state: ResearchState,
                            llm_client: LLMClient,
                            prompt_selector: PromptSelector,
                            genre: str = "default") -> dict:
    """
    å¤§çº²ç”ŸæˆèŠ‚ç‚¹
    
    åŸºäºåˆå§‹ç ”ç©¶æ•°æ®ç”Ÿæˆæ–‡æ¡£çš„ç»“æ„åŒ–å¤§çº²
    
    Args:
        state: ç ”ç©¶çŠ¶æ€ï¼ŒåŒ…å« topic å’Œ initial_gathered_data
        llm_client: LLMå®¢æˆ·ç«¯å®ä¾‹
        prompt_selector: PromptSelectorå®ä¾‹ï¼Œç”¨äºè·å–promptæ¨¡æ¿
        genre: genreç±»å‹ï¼Œé»˜è®¤ä¸º"default"
        
    Returns:
        dict: åŒ…å« document_outline çš„å­—å…¸
    """
    topic = state.get("topic", "")
    initial_sources = state.get("initial_sources", [])
    initial_gathered_data = state.get("initial_gathered_data", "")  # ä¿æŒå‘åå…¼å®¹

    if not topic:
        raise ValueError("ä¸»é¢˜ä¸èƒ½ä¸ºç©º")

    # å¦‚æœæ²¡æœ‰æ”¶é›†åˆ°æºæ•°æ®ï¼Œå°è¯•ä½¿ç”¨æ—§çš„ initial_gathered_data
    if not initial_sources and not initial_gathered_data:
        raise ValueError("åˆå§‹ç ”ç©¶æ•°æ®ä¸èƒ½ä¸ºç©º")

    # å¦‚æœæœ‰ initial_sourcesï¼Œå°†å…¶è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼
    if initial_sources:
        initial_gathered_data = _format_sources_to_text(initial_sources)
    elif not initial_gathered_data:
        initial_gathered_data = "æ²¡æœ‰æ”¶é›†åˆ°ç›¸å…³æ•°æ®"

    logger.info(f"ğŸ“‹ å¼€å§‹ç”Ÿæˆæ–‡æ¡£å¤§çº²: {topic}")

    # è·å–é…ç½®
    outline_config = settings.get_agent_component_config("task_planner")
    if not outline_config:
        temperature = 0.3  # å¤§çº²ç”Ÿæˆéœ€è¦æ›´ä½çš„æ¸©åº¦ä»¥ç¡®ä¿ç»“æ„æ€§
        max_tokens = 2000
        extra_params = {}
    else:
        temperature = outline_config.temperature * 0.7  # é™ä½æ¸©åº¦
        max_tokens = outline_config.max_tokens
        extra_params = outline_config.extra_params

    # è·å–éœ€æ±‚æ–‡æ¡£å†…å®¹
    requirements_content = state.get("requirements_content")

    # è·å–æ–‡æ¡£é…ç½®ä»¥ç¡®å®šç« èŠ‚æ•°
    doc_config = settings.get_document_config(fast_mode=True)
    target_chapter_count = doc_config.get('chapter_count', 5)
    logger.info(f"ğŸ“‹ ç›®æ ‡ç« èŠ‚æ•°: {target_chapter_count}")

    # ä½¿ç”¨ PromptSelector è·å– prompt æ¨¡æ¿
    try:
        # å¦‚æœæœ‰éœ€æ±‚æ–‡æ¡£ï¼Œéœ€è¦æ‰‹åŠ¨æŒ‡å®šä½¿ç”¨ v2_with_requirements ç‰ˆæœ¬
        if requirements_content and requirements_content.strip():
            # ç›´æ¥å¯¼å…¥æ¨¡å—å¹¶è·å–ç‰¹å®šç‰ˆæœ¬
            import importlib
            module = importlib.import_module(
                "src.doc_agent.prompts.outline_generation")
            if hasattr(module,
                       'PROMPTS') and "v2_with_requirements" in module.PROMPTS:
                prompt_template = module.PROMPTS["v2_with_requirements"]
                logger.info(f"âœ… ä½¿ç”¨ v2_with_requirements ç‰ˆæœ¬ï¼Œæ£€æµ‹åˆ°éœ€æ±‚æ–‡æ¡£")
            else:
                # å›é€€åˆ°é»˜è®¤ç‰ˆæœ¬
                prompt_template = prompt_selector.get_prompt(
                    "prompts", "outline_generation", genre)
                logger.warning(f"âš ï¸  v2_with_requirements ç‰ˆæœ¬ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤ç‰ˆæœ¬")
        else:
            prompt_template = prompt_selector.get_prompt(
                "prompts", "outline_generation", genre)
            logger.info(f"âœ… ä½¿ç”¨é»˜è®¤ç‰ˆæœ¬ï¼Œæœªæ£€æµ‹åˆ°éœ€æ±‚æ–‡æ¡£")

        logger.debug(f"âœ… æˆåŠŸè·å– outline_generation prompt æ¨¡æ¿ï¼Œgenre: {genre}")
    except Exception as e:
        logger.error(f"âŒ è·å– outline_generation prompt æ¨¡æ¿å¤±è´¥: {e}")
        # ä½¿ç”¨é»˜è®¤çš„ prompt æ¨¡æ¿ä½œä¸ºå¤‡ç”¨
        prompt_template = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£ç»“æ„è§„åˆ’ä¸“å®¶ã€‚è¯·åŸºäºæä¾›çš„ç ”ç©¶æ•°æ®ï¼Œä¸ºæŒ‡å®šä¸»é¢˜ç”Ÿæˆä¸€ä¸ªç»“æ„åŒ–çš„æ–‡æ¡£å¤§çº²ã€‚

**æ–‡æ¡£ä¸»é¢˜:** {topic}

**ç ”ç©¶æ•°æ®æ‘˜è¦:**
{initial_gathered_data}

**ä»»åŠ¡è¦æ±‚:**
1. åˆ†æç ”ç©¶æ•°æ®ï¼Œè¯†åˆ«ä¸»è¦ä¸»é¢˜å’Œå…³é”®ä¿¡æ¯
2. è®¾è®¡ä¸€ä¸ªé€»è¾‘æ¸…æ™°ã€ç»“æ„åˆç†çš„æ–‡æ¡£å¤§çº²
3. ç¡®ä¿å¤§çº²æ¶µç›–ä¸»é¢˜çš„å„ä¸ªæ–¹é¢
4. æ¯ä¸ªç« èŠ‚åº”è¯¥æœ‰æ˜ç¡®çš„æ ‡é¢˜å’Œæè¿°
5. è€ƒè™‘ç« èŠ‚ä¹‹é—´çš„é€»è¾‘å…³ç³»å’Œå±‚æ¬¡ç»“æ„

**è¾“å‡ºæ ¼å¼:**
è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- title: æ–‡æ¡£æ ‡é¢˜
- summary: æ–‡æ¡£æ‘˜è¦
- chapters: ç« èŠ‚åˆ—è¡¨ï¼Œæ¯ä¸ªç« èŠ‚åŒ…å«ï¼š
  - chapter_number: ç« èŠ‚ç¼–å·
  - chapter_title: ç« èŠ‚æ ‡é¢˜
  - description: ç« èŠ‚æè¿°
  - key_points: å…³é”®è¦ç‚¹åˆ—è¡¨
  - estimated_sections: é¢„ä¼°å°èŠ‚æ•°é‡

è¯·ç«‹å³å¼€å§‹ç”Ÿæˆæ–‡æ¡£å¤§çº²ã€‚
"""

    # æ„å»ºæç¤ºè¯
    if requirements_content and requirements_content.strip():
        # æ ¼å¼åŒ–éœ€æ±‚æ–‡æ¡£å†…å®¹
        formatted_requirements = f"""
**ç”¨æˆ·éœ€æ±‚æ–‡æ¡£:**
{requirements_content}

"""
        prompt = prompt_template.format(
            topic=topic,
            initial_gathered_data=initial_gathered_data[:8000],  # é™åˆ¶è¾“å…¥é•¿åº¦
            requirements_content=formatted_requirements,
            target_chapter_count=target_chapter_count)
        logger.info(f"ğŸ“‹ åŒ…å«éœ€æ±‚æ–‡æ¡£çš„å¤§çº²ç”Ÿæˆï¼Œéœ€æ±‚é•¿åº¦: {len(requirements_content)} å­—ç¬¦")
    else:
        # ä¸åŒ…å«éœ€æ±‚æ–‡æ¡£çš„ç‰ˆæœ¬
        prompt = prompt_template.format(
            topic=topic,
            initial_gathered_data=initial_gathered_data[:8000],  # é™åˆ¶è¾“å…¥é•¿åº¦
            target_chapter_count=target_chapter_count)
        logger.info(f"ğŸ“‹ æ ‡å‡†å¤§çº²ç”Ÿæˆï¼ŒæœªåŒ…å«éœ€æ±‚æ–‡æ¡£")

    logger.debug(
        f"Invoking LLM with outline generation prompt:\n{pprint.pformat(prompt)}"
    )

    try:
        # è°ƒç”¨LLMç”Ÿæˆå¤§çº²
        response = llm_client.invoke(prompt,
                                     temperature=temperature,
                                     max_tokens=max_tokens,
                                     **extra_params)

        # è§£æJSONå“åº”
        try:
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            logger.debug(f"ğŸ” LLMåŸå§‹å“åº”:\n{response}")

            # æ¸…ç†å“åº”ï¼Œç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
            cleaned_response = response.strip()
            if cleaned_response.startswith("```json"):
                cleaned_response = cleaned_response[7:]
            if cleaned_response.startswith("```"):
                cleaned_response = cleaned_response[3:]
            if cleaned_response.endswith("```"):
                cleaned_response = cleaned_response[:-3]

            logger.debug(f"ğŸ” æ¸…ç†åçš„å“åº”:\n{cleaned_response}")

            document_outline = json.loads(cleaned_response.strip())

            # éªŒè¯å¤§çº²ç»“æ„
            if "chapters" not in document_outline:
                raise ValueError("å¤§çº²ç¼ºå°‘chapterså­—æ®µ")

            logger.info(f"âœ… æˆåŠŸç”Ÿæˆå¤§çº²ï¼ŒåŒ…å« {len(document_outline['chapters'])} ä¸ªç« èŠ‚")

        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSONè§£æå¤±è´¥: {str(e)}")
            logger.error(f"ğŸ” å°è¯•è§£æçš„å“åº”: {cleaned_response}")
            # è¿”å›é»˜è®¤å¤§çº²
            document_outline = {
                "title":
                topic,
                "summary":
                f"å…³äº{topic}çš„ç»¼åˆæ€§æ–‡æ¡£",
                "chapters": [{
                    "chapter_number": 1,
                    "chapter_title": "æ¦‚è¿°ä¸èƒŒæ™¯",
                    "description": f"ä»‹ç»{topic}çš„åŸºæœ¬æ¦‚å¿µã€å†å²èƒŒæ™¯å’Œé‡è¦æ€§",
                    "key_points": ["åŸºæœ¬å®šä¹‰", "å†å²å‘å±•", "ç°å®æ„ä¹‰"],
                    "estimated_sections": 3
                }, {
                    "chapter_number": 2,
                    "chapter_title": "æ ¸å¿ƒå†…å®¹åˆ†æ",
                    "description": f"æ·±å…¥åˆ†æ{topic}çš„æ ¸å¿ƒè¦ç´ å’Œå…³é”®ç‰¹å¾",
                    "key_points": ["ä¸»è¦ç‰¹å¾", "å…³é”®è¦ç´ ", "æŠ€æœ¯ç»†èŠ‚"],
                    "estimated_sections": 4
                }, {
                    "chapter_number": 3,
                    "chapter_title": "åº”ç”¨ä¸å®è·µ",
                    "description": f"æ¢è®¨{topic}çš„å®é™…åº”ç”¨åœºæ™¯å’Œæœ€ä½³å®è·µ",
                    "key_points": ["åº”ç”¨åœºæ™¯", "æ¡ˆä¾‹åˆ†æ", "å®æ–½æ–¹æ³•"],
                    "estimated_sections": 3
                }, {
                    "chapter_number": 4,
                    "chapter_title": "æœªæ¥å±•æœ›",
                    "description": f"åˆ†æ{topic}çš„å‘å±•è¶‹åŠ¿å’Œæœªæ¥å¯èƒ½æ€§",
                    "key_points": ["å‘å±•è¶‹åŠ¿", "æŒ‘æˆ˜æœºé‡", "å‰æ™¯é¢„æµ‹"],
                    "estimated_sections": 2
                }],
                "total_chapters":
                4,
                "estimated_total_words":
                12000
            }

        return {"document_outline": document_outline}

    except Exception as e:
        logger.error(f"âŒ å¤§çº²ç”Ÿæˆå¤±è´¥: {str(e)}")
        # è¿”å›åŸºç¡€å¤§çº²
        return {
            "document_outline": {
                "title":
                topic,
                "summary":
                f"å…³äº{topic}çš„æ–‡æ¡£",
                "chapters": [{
                    "chapter_number": 1,
                    "chapter_title": "å¼•è¨€",
                    "description": f"ä»‹ç»{topic}çš„åŸºæœ¬ä¿¡æ¯",
                    "key_points": ["æ¦‚è¿°"],
                    "estimated_sections": 2
                }],
                "total_chapters":
                1,
                "estimated_total_words":
                3000
            }
        }


def split_chapters_node(state: ResearchState) -> dict:
    """
    ç« èŠ‚æ‹†åˆ†èŠ‚ç‚¹
    
    å°†æ–‡æ¡£å¤§çº²æ‹†åˆ†ä¸ºç‹¬ç«‹çš„ç« èŠ‚ä»»åŠ¡åˆ—è¡¨
    
    Args:
        state: ç ”ç©¶çŠ¶æ€ï¼ŒåŒ…å« document_outline
        
    Returns:
        dict: åŒ…å« chapters_to_process å’Œ current_chapter_index çš„å­—å…¸
    """
    document_outline = state.get("document_outline", {})

    if not document_outline or "chapters" not in document_outline:
        raise ValueError("æ–‡æ¡£å¤§çº²ä¸å­˜åœ¨æˆ–æ ¼å¼æ— æ•ˆ")

    logger.info(f"ğŸ“‚ å¼€å§‹æ‹†åˆ†ç« èŠ‚ä»»åŠ¡")

    # ä»å¤§çº²ä¸­æå–ç« èŠ‚ä¿¡æ¯
    chapters = document_outline.get("chapters", [])

    # åˆ›å»ºç« èŠ‚ä»»åŠ¡åˆ—è¡¨
    chapters_to_process = []
    for chapter in chapters:
        chapter_task = {
            "chapter_number":
            chapter.get("chapter_number",
                        len(chapters_to_process) + 1),
            "chapter_title":
            chapter.get("chapter_title", f"ç¬¬{len(chapters_to_process) + 1}ç« "),
            "description":
            chapter.get("description", ""),
            "key_points":
            chapter.get("key_points", []),
            "estimated_sections":
            chapter.get("estimated_sections", 3),
            "research_data":
            ""  # å°†åœ¨ç« èŠ‚å·¥ä½œæµä¸­å¡«å……
        }
        chapters_to_process.append(chapter_task)

    logger.info(f"âœ… æˆåŠŸåˆ›å»º {len(chapters_to_process)} ä¸ªç« èŠ‚ä»»åŠ¡")

    # æ‰“å°ç« èŠ‚åˆ—è¡¨
    for i, chapter in enumerate(chapters_to_process):
        logger.info(
            f"  ğŸ“„ ç¬¬{chapter['chapter_number']}ç« : {chapter['chapter_title']}")

    return {
        "chapters_to_process": chapters_to_process,
        "current_chapter_index": 0,
        "completed_chapters_content": []  # åˆå§‹åŒ–å·²å®Œæˆç« èŠ‚åˆ—è¡¨
    }


def _parse_web_search_results(web_results: str, query: str,
                              start_id: int) -> list[Source]:
    """
    è§£æç½‘ç»œæœç´¢ç»“æœï¼Œåˆ›å»º Source å¯¹è±¡åˆ—è¡¨
    
    Args:
        web_results: ç½‘ç»œæœç´¢çš„åŸå§‹ç»“æœå­—ç¬¦ä¸²
        query: æœç´¢æŸ¥è¯¢
        start_id: èµ·å§‹ID
        
    Returns:
        list[Source]: Source å¯¹è±¡åˆ—è¡¨
    """
    sources = []
    current_id = start_id

    try:
        # ç®€å•çš„è§£æé€»è¾‘ï¼šæŒ‰è¡Œåˆ†å‰²ï¼Œæå–æ ‡é¢˜å’ŒURL
        lines = web_results.split('\n')
        current_title = ""
        current_url = ""
        current_content = ""

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # å°è¯•æå–æ ‡é¢˜å’ŒURL
            if line.startswith('http') or line.startswith('https'):
                # è¿™æ˜¯ä¸€ä¸ªURL
                if current_title and current_content:
                    # åˆ›å»ºå‰ä¸€ä¸ªæº
                    source = Source(
                        id=current_id,
                        source_type="webpage",
                        title=current_title,
                        url=current_url,
                        content=current_content[:500] + "..."
                        if len(current_content) > 500 else current_content)
                    sources.append(source)
                    current_id += 1

                current_url = line
                current_title = ""
                current_content = ""
            elif line.startswith('æ ‡é¢˜:') or line.startswith('Title:'):
                current_title = line.split(':', 1)[1].strip()
            elif line.startswith('å†…å®¹:') or line.startswith('Content:'):
                current_content = line.split(':', 1)[1].strip()
            elif not current_title and len(
                    line) > 10 and not line.startswith('==='):
                # å¯èƒ½æ˜¯æ ‡é¢˜
                current_title = line
            elif current_title and len(line) > 20:
                # å¯èƒ½æ˜¯å†…å®¹
                current_content += " " + line

        # å¤„ç†æœ€åä¸€ä¸ªæº
        if current_title and current_content:
            source = Source(
                id=current_id,
                source_type="webpage",
                title=current_title,
                url=current_url,
                content=current_content[:500] +
                "..." if len(current_content) > 500 else current_content)
            sources.append(source)

        # å¦‚æœæ²¡æœ‰è§£æåˆ°ä»»ä½•æºï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤æº
        if not sources:
            source = Source(id=current_id,
                            source_type="webpage",
                            title=f"ç½‘ç»œæœç´¢ç»“æœ - {query}",
                            url="",
                            content=web_results[:500] +
                            "..." if len(web_results) > 500 else web_results)
            sources.append(source)

    except Exception as e:
        logger.error(f"è§£æç½‘ç»œæœç´¢ç»“æœå¤±è´¥: {str(e)}")
        # åˆ›å»ºé»˜è®¤æº
        source = Source(id=start_id,
                        source_type="webpage",
                        title=f"ç½‘ç»œæœç´¢ç»“æœ - {query}",
                        url="",
                        content=web_results[:500] +
                        "..." if len(web_results) > 500 else web_results)
        sources.append(source)

    return sources


def bibliography_node(state: ResearchState) -> dict:
    """
    å‚è€ƒæ–‡çŒ®ç”ŸæˆèŠ‚ç‚¹
    
    åœ¨æ–‡æ¡£ç”Ÿæˆå®Œæˆåï¼ŒåŸºäºå…¨å±€å¼•ç”¨çš„ä¿¡æ¯æºç”Ÿæˆå‚è€ƒæ–‡çŒ®éƒ¨åˆ†
    
    Args:
        state: ç ”ç©¶çŠ¶æ€ï¼ŒåŒ…å« final_document å’Œ cited_sources
        
    Returns:
        dict: åŒ…å«æ›´æ–°åçš„ final_document çš„å­—å…¸
    """
    final_document = state.get("final_document", "")
    cited_sources = state.get("cited_sources", {})

    if not final_document:
        logger.warning("âŒ æ²¡æœ‰æ‰¾åˆ°æœ€ç»ˆæ–‡æ¡£å†…å®¹")
        return {"final_document": "æ–‡æ¡£ç”Ÿæˆå¤±è´¥ï¼Œæ— æ³•æ·»åŠ å‚è€ƒæ–‡çŒ®ã€‚"}

    if not cited_sources:
        logger.info("ğŸ“š æ²¡æœ‰å¼•ç”¨çš„ä¿¡æ¯æºï¼Œè·³è¿‡å‚è€ƒæ–‡çŒ®ç”Ÿæˆ")
        return {"final_document": final_document}

    logger.info(f"ğŸ“š å¼€å§‹ç”Ÿæˆå‚è€ƒæ–‡çŒ®ï¼Œå…±æœ‰ {len(cited_sources)} ä¸ªå¼•ç”¨æº")

    def format_reference_entry(source: Source, number: int) -> str:
        """æ ¹æ®æºç±»å‹æ ¼å¼åŒ–å‚è€ƒæ–‡çŒ®æ¡ç›®ä¸ºBibTeXé£æ ¼çš„å­¦æœ¯æ ¼å¼"""

        if source.source_type == "webpage":
            # ç½‘é¡µæ ¼å¼: [ç¼–å·] ä½œè€…/ç½‘ç«™. "æ ‡é¢˜". ç½‘ç«™å (å¹´ä»½). URL
            content_lines = source.content.strip().split('\n')
            actual_title = "ç½‘é¡µèµ„æº"
            url = source.url or ""
            website_name = ""
            year = "2024"  # é»˜è®¤å¹´ä»½ï¼Œå¯ä»¥ä»URLæˆ–å†…å®¹ä¸­æå–

            # å°è¯•ä»contentä¸­æå–æ›´å¥½çš„ä¿¡æ¯
            for line in content_lines:
                if line.strip().startswith('URL:'):
                    url = line.replace('URL:', '').strip()
                    # ä»URLä¸­æå–ç½‘ç«™åç§°
                    import re
                    domain_match = re.search(r'https?://(?:www\.)?([^/]+)',
                                             url)
                    if domain_match:
                        domain = domain_match.group(1)
                        if 'baidu.com' in domain:
                            website_name = "ç™¾åº¦ç™¾ç§‘"
                        elif 'csdn.net' in domain:
                            website_name = "CSDNåšå®¢"
                        elif 'juejin.cn' in domain:
                            website_name = "æ˜é‡‘"
                        elif 'aliyun.com' in domain:
                            website_name = "é˜¿é‡Œäº‘å¼€å‘è€…ç¤¾åŒº"
                        elif 'github.io' in domain:
                            website_name = "GitHub Pages"
                        else:
                            website_name = domain

                elif 'å†…å®¹é¢„è§ˆ:' in line:
                    # æå–å®é™…æ ‡é¢˜
                    preview_part = line.split('å†…å®¹é¢„è§ˆ:')[-1].strip()
                    if preview_part:
                        # æå–ç½‘é¡µæ ‡é¢˜ï¼ˆå»æ‰ç½‘ç«™åç¼€ï¼‰
                        import re
                        title_match = re.search(
                            r'([^_]+?)(?:\s*[_\-]\s*(?:ç™¾åº¦ç™¾ç§‘|CSDNåšå®¢|æ˜é‡‘|é˜¿é‡Œäº‘|GitHub)|$)',
                            preview_part)
                        if title_match:
                            actual_title = title_match.group(1).strip()
                        elif len(preview_part) > 10:
                            actual_title = preview_part[:60] + "..." if len(
                                preview_part) > 60 else preview_part

            # å¦‚æœæ²¡æœ‰æå–åˆ°åˆé€‚çš„æ ‡é¢˜ï¼Œä½¿ç”¨åŸå§‹title
            if actual_title == "ç½‘é¡µèµ„æº" and source.title:
                if source.title.startswith('Search results for:'):
                    query_part = source.title.replace('Search results for:',
                                                      '').strip()
                    actual_title = f"{query_part}"
                else:
                    actual_title = source.title

            # æ ¼å¼åŒ–ä¸ºå­¦æœ¯å¼•ç”¨æ ¼å¼
            if website_name and url:
                return f'[{number}] {website_name}. "{actual_title}". {website_name} ({year}). {url}'
            elif url:
                return f'[{number}] ç½‘ç»œèµ„æº. "{actual_title}". åœ¨çº¿èµ„æº ({year}). {url}'
            else:
                return f'[{number}] ç½‘ç»œèµ„æº. "{actual_title}". åœ¨çº¿èµ„æº ({year}).'

        elif source.source_type == "es_result":
            # å†…éƒ¨çŸ¥è¯†åº“æ ¼å¼: [ç¼–å·] ä½œè€…/æœºæ„. "æ–‡æ¡£æ ‡é¢˜". å†…éƒ¨çŸ¥è¯†åº“ (å¹´ä»½).
            content_lines = source.content.strip().split('\n')
            doc_title = "å†…éƒ¨çŸ¥è¯†åº“æ–‡æ¡£"
            author = "å†…éƒ¨èµ„æ–™"
            year = "2024"

            for line in content_lines:
                if '.pdf' in line or '.doc' in line:
                    # æå–æ–‡æ¡£åç§°
                    import re
                    doc_match = re.search(r'([^/\]]+\.(?:pdf|doc|docx))', line)
                    if doc_match:
                        full_name = doc_match.group(1)
                        # å»æ‰æ–‡ä»¶æ‰©å±•å
                        doc_title = re.sub(r'\.(pdf|doc|docx)$', '', full_name)
                        break
                elif '[personal_knowledge_base]' in line:
                    # æå–ç´§è·Ÿçš„æ–‡æ¡£æ ‡é¢˜
                    remaining = line.split(
                        '[personal_knowledge_base]')[-1].strip()
                    if remaining:
                        doc_title = remaining.split(
                            '.'
                        )[0] if '.' in remaining else remaining[:80] + "..."

                        # å°è¯•ä»æ ‡é¢˜ä¸­æå–æœºæ„ä¿¡æ¯
                        if 'ã€' in doc_title and 'ã€‘' in doc_title:
                            import re
                            org_match = re.search(r'ã€([^ã€‘]+)ã€‘', doc_title)
                            if org_match:
                                author = org_match.group(1)
                        break

            return f'[{number}] {author}. "{doc_title}". å†…éƒ¨çŸ¥è¯†åº“ ({year}).'

        elif source.source_type == "document":
            # æ–‡æ¡£æ ¼å¼: [ç¼–å·] ä½œè€…. "æ–‡æ¡£æ ‡é¢˜". æ–‡æ¡£ç±»å‹ (å¹´ä»½).
            return f'[{number}] æ–‡æ¡£èµ„æ–™. "{source.title}". å†…éƒ¨æ–‡æ¡£ (2024).'

        else:
            # é»˜è®¤æ ¼å¼
            return f'[{number}] æœªçŸ¥æ¥æº. "{source.title}". {source.source_type} (2024).'

    # ç”Ÿæˆå‚è€ƒæ–‡çŒ®éƒ¨åˆ†
    bibliography_section = "\n\n## å‚è€ƒæ–‡çŒ®\n\n"

    # æŒ‰æºIDæ’åºï¼Œç¡®ä¿å‚è€ƒæ–‡çŒ®é¡ºåºä¸€è‡´
    sorted_sources = sorted(cited_sources.items(), key=lambda x: x[0])

    # ä½¿ç”¨å…¨å±€è¿ç»­çš„ç¼–å·
    for global_number, (source_id, source) in enumerate(sorted_sources, 1):
        # ä½¿ç”¨æ–°çš„æ ¼å¼åŒ–å‡½æ•°
        reference_entry = format_reference_entry(source, global_number)
        bibliography_section += reference_entry + "\n"

    # å°†å‚è€ƒæ–‡çŒ®éƒ¨åˆ†æ·»åŠ åˆ°æœ€ç»ˆæ–‡æ¡£
    updated_document = final_document + bibliography_section

    logger.info(f"âœ… å‚è€ƒæ–‡çŒ®ç”Ÿæˆå®Œæˆï¼Œæ–‡æ¡£æ€»é•¿åº¦: {len(updated_document)} å­—ç¬¦")

    # è®°å½•å¼•ç”¨çš„æºä¿¡æ¯
    for global_number, (source_id, source) in enumerate(sorted_sources, 1):
        logger.debug(
            f"  ğŸ“– [{global_number}] {source.title} ({source.source_type})")

    return {"final_document": updated_document}


def _format_sources_to_text(sources: list[Source]) -> str:
    """
    å°† Source å¯¹è±¡åˆ—è¡¨æ ¼å¼åŒ–ä¸ºæ–‡æœ¬æ ¼å¼ï¼Œç”¨äºå‘åå…¼å®¹
    
    Args:
        sources: Source å¯¹è±¡åˆ—è¡¨
        
    Returns:
        str: æ ¼å¼åŒ–çš„æ–‡æœ¬
    """
    if not sources:
        return "æ²¡æœ‰æ”¶é›†åˆ°ç›¸å…³æ•°æ®"

    formatted_text = "æ”¶é›†åˆ°çš„ä¿¡æ¯æº:\n\n"

    for i, source in enumerate(sources, 1):
        formatted_text += f"=== ä¿¡æ¯æº {i} ===\n"
        formatted_text += f"æ ‡é¢˜: {source.title}\n"
        if source.url:
            formatted_text += f"URL: {source.url}\n"
        formatted_text += f"ç±»å‹: {source.source_type}\n"
        formatted_text += f"å†…å®¹: {source.content}\n\n"

    return formatted_text


def _parse_es_search_results(es_results: str, query: str,
                             start_id: int) -> list[Source]:
    """
    è§£æESæœç´¢ç»“æœï¼Œåˆ›å»º Source å¯¹è±¡åˆ—è¡¨
    
    Args:
        es_results: ESæœç´¢çš„åŸå§‹ç»“æœå­—ç¬¦ä¸²
        query: æœç´¢æŸ¥è¯¢
        start_id: èµ·å§‹ID
        
    Returns:
        list[Source]: Source å¯¹è±¡åˆ—è¡¨
    """
    sources = []
    current_id = start_id

    try:
        # è§£æESæœç´¢ç»“æœ
        lines = es_results.split('\n')
        current_title = ""
        current_content = ""
        current_url = ""

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # å°è¯•æå–æ–‡æ¡£ä¿¡æ¯
            if line.startswith('æ–‡æ¡£æ ‡é¢˜:') or line.startswith('Title:'):
                current_title = line.split(':', 1)[1].strip()
            elif line.startswith('æ–‡æ¡£å†…å®¹:') or line.startswith('Content:'):
                current_content = line.split(':', 1)[1].strip()
            elif line.startswith('æ–‡æ¡£URL:') or line.startswith('URL:'):
                current_url = line.split(':', 1)[1].strip()
            elif line.startswith('---') or line.startswith('==='):
                # åˆ†éš”ç¬¦ï¼Œå¤„ç†å‰ä¸€ä¸ªæ–‡æ¡£
                if current_title and current_content:
                    source = Source(
                        id=current_id,
                        source_type="es_result",
                        title=current_title,
                        url=current_url,
                        content=current_content[:500] + "..."
                        if len(current_content) > 500 else current_content)
                    sources.append(source)
                    current_id += 1
                    current_title = ""
                    current_content = ""
                    current_url = ""
            elif not current_title and len(line) > 10:
                # å¯èƒ½æ˜¯æ ‡é¢˜
                current_title = line
            elif current_title and len(line) > 20:
                # å¯èƒ½æ˜¯å†…å®¹
                current_content += " " + line

        # å¤„ç†æœ€åä¸€ä¸ªæ–‡æ¡£
        if current_title and current_content:
            source = Source(
                id=current_id,
                source_type="es_result",
                title=current_title,
                url=current_url,
                content=current_content[:500] +
                "..." if len(current_content) > 500 else current_content)
            sources.append(source)

        # å¦‚æœæ²¡æœ‰è§£æåˆ°ä»»ä½•æºï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤æº
        if not sources:
            source = Source(id=start_id,
                            source_type="es_result",
                            title=f"çŸ¥è¯†åº“æœç´¢ç»“æœ - {query}",
                            url="",
                            content=es_results[:500] +
                            "..." if len(es_results) > 500 else es_results)
            sources.append(source)

    except Exception as e:
        logger.error(f"è§£æESæœç´¢ç»“æœå¤±è´¥: {str(e)}")
        # åˆ›å»ºé»˜è®¤æº
        source = Source(id=start_id,
                        source_type="es_result",
                        title=f"çŸ¥è¯†åº“æœç´¢ç»“æœ - {query}",
                        url="",
                        content=es_results[:500] +
                        "..." if len(es_results) > 500 else es_results)
        sources.append(source)

    return sources


def fusion_editor_node(state: ResearchState, llm_client: LLMClient) -> dict:
    """
    èåˆç¼–è¾‘å™¨èŠ‚ç‚¹
    
    å¯¹æœ€ç»ˆæ–‡æ¡£è¿›è¡Œæ¶¦è‰²ï¼Œç‰¹åˆ«æ˜¯é‡å†™å¼•è¨€å’Œç»“è®ºéƒ¨åˆ†ï¼Œä½¿å…¶æ›´å¥½åœ°ä¸æ–‡æ¡£ä¸»ä½“å†…å®¹åè°ƒ
    
    Args:
        state: ç ”ç©¶çŠ¶æ€ï¼ŒåŒ…å« completed_chapters å’Œ final_document
        llm_client: LLMå®¢æˆ·ç«¯å®ä¾‹
        
    Returns:
        dict: åŒ…å«æ›´æ–°åçš„ final_document çš„å­—å…¸
    """
    completed_chapters = state.get("completed_chapters", [])
    topic = state.get("topic", "")

    logger.info("ğŸ¨ å¼€å§‹èåˆç¼–è¾‘å™¨å¤„ç†")
    logger.info(f"ğŸ“š å·²å®Œæˆç« èŠ‚æ•°é‡: {len(completed_chapters)}")

    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„ç« èŠ‚è¿›è¡Œå¤„ç†
    if len(completed_chapters) <= 1:
        logger.info("ğŸ“ ç« èŠ‚æ•°é‡ä¸è¶³ï¼Œè·³è¿‡èåˆç¼–è¾‘")
        return {"final_document": state.get("final_document", "")}

    # æå–å¼•è¨€å’Œç»“è®º
    intro_chapter = completed_chapters[0]
    conclusion_chapter = completed_chapters[-1]
    middle_chapters = completed_chapters[1:-1]

    logger.info(f"ğŸ“– æå–å¼•è¨€ç« èŠ‚: {intro_chapter.get('title', 'Unknown')}")
    logger.info(f"ğŸ“– æå–ç»“è®ºç« èŠ‚: {conclusion_chapter.get('title', 'Unknown')}")
    logger.info(f"ğŸ“š ä¸­é—´ç« èŠ‚æ•°é‡: {len(middle_chapters)}")

    # è·å–å¼•è¨€å’Œç»“è®ºçš„åŸå§‹å†…å®¹
    intro_content = intro_chapter.get("content", "")
    conclusion_content = conclusion_chapter.get("content", "")

    # åˆ›å»ºå…¨å±€æ‘˜è¦ï¼šåˆå¹¶ä¸­é—´ç« èŠ‚çš„æ‘˜è¦
    global_summary_parts = []
    for chapter in middle_chapters:
        if isinstance(chapter, dict):
            summary = chapter.get("summary", "")
            if summary:
                global_summary_parts.append(summary)
            else:
                # å¦‚æœæ²¡æœ‰æ‘˜è¦ï¼Œä½¿ç”¨å†…å®¹çš„å‰200å­—ç¬¦
                content = chapter.get("content", "")
                if content:
                    summary = content[:200] + "..." if len(
                        content) > 200 else content
                    global_summary_parts.append(summary)

    global_summary = "\n\n".join(global_summary_parts)
    logger.info(f"ğŸ“‹ å…¨å±€æ‘˜è¦é•¿åº¦: {len(global_summary)} å­—ç¬¦")

    # è·å–ä¸­é—´ç« èŠ‚çš„å®Œæ•´å†…å®¹
    middle_chapters_content = []
    for chapter in middle_chapters:
        if isinstance(chapter, dict):
            content = chapter.get("content", "")
            if content:
                middle_chapters_content.append(content)

    middle_content = "\n\n".join(middle_chapters_content)

    try:
        # é‡å†™å¼•è¨€
        logger.info("âœï¸ å¼€å§‹é‡å†™å¼•è¨€")
        intro_prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„é¦–å¸­ç¼–è¾‘ï¼Œè´Ÿè´£é‡å†™æ–‡æ¡£çš„å¼•è¨€éƒ¨åˆ†ã€‚

**æ–‡æ¡£ä¸»é¢˜:** {topic}

**åŸå§‹å¼•è¨€å†…å®¹:**
{intro_content}

**æ–‡æ¡£ä¸»ä½“ç« èŠ‚æ‘˜è¦:**
{global_summary}

**ä»»åŠ¡è¦æ±‚:**
1. ä»”ç»†é˜…è¯»åŸå§‹å¼•è¨€å’Œä¸»ä½“ç« èŠ‚æ‘˜è¦
2. é‡å†™å¼•è¨€ï¼Œä½¿å…¶æ›´å¥½åœ°ï¼š
   - ä¸ºè¯»è€…æä¾›æ¸…æ™°çš„æ–‡æ¡£æ¦‚è§ˆ
   - å‡†ç¡®é¢„è§ˆä¸»ä½“ç« èŠ‚å°†è¦è®¨è®ºçš„ä¸»è¦è§‚ç‚¹
   - å»ºç«‹é€»è¾‘è¿è´¯æ€§ï¼Œç¡®ä¿å¼•è¨€ä¸ä¸»ä½“å†…å®¹è‡ªç„¶è¡”æ¥
   - ä¿æŒä¸“ä¸šæ€§å’Œå­¦æœ¯æ€§
3. ä¿æŒåŸæœ‰çš„ç« èŠ‚æ ‡é¢˜å’ŒåŸºæœ¬ç»“æ„
4. ç¡®ä¿é‡å†™åçš„å¼•è¨€ä¸ä¸»ä½“ç« èŠ‚çš„å†…å®¹å’Œé£æ ¼ä¿æŒä¸€è‡´

è¯·é‡å†™å¼•è¨€ï¼Œä½¿å…¶æ›´å¥½åœ°ä¸ºæ•´ä¸ªæ–‡æ¡£æœåŠ¡ã€‚"""

        polished_intro = llm_client.invoke(intro_prompt,
                                           temperature=0.3,
                                           max_tokens=2000)

        logger.info(f"âœ… å¼•è¨€é‡å†™å®Œæˆï¼Œé•¿åº¦: {len(polished_intro)} å­—ç¬¦")

        # é‡å†™ç»“è®º
        logger.info("âœï¸ å¼€å§‹é‡å†™ç»“è®º")
        conclusion_prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„é¦–å¸­ç¼–è¾‘ï¼Œè´Ÿè´£é‡å†™æ–‡æ¡£çš„ç»“è®ºéƒ¨åˆ†ã€‚

**æ–‡æ¡£ä¸»é¢˜:** {topic}

**åŸå§‹ç»“è®ºå†…å®¹:**
{conclusion_content}

**æ–‡æ¡£ä¸»ä½“ç« èŠ‚æ‘˜è¦:**
{global_summary}

**ä»»åŠ¡è¦æ±‚:**
1. ä»”ç»†é˜…è¯»åŸå§‹ç»“è®ºå’Œä¸»ä½“ç« èŠ‚æ‘˜è¦
2. é‡å†™ç»“è®ºï¼Œä½¿å…¶æ›´å¥½åœ°ï¼š
   - æ€»ç»“æ–‡æ¡£çš„æ ¸å¿ƒè§‚ç‚¹å’Œä¸»è¦å‘ç°
   - åæ˜ ä¸»ä½“ç« èŠ‚è®¨è®ºçš„å…³é”®å†…å®¹
   - æä¾›å¯¹ä¸»é¢˜çš„æ·±å…¥æ€è€ƒå’Œæ´å¯Ÿ
   - ä¸ºè¯»è€…æä¾›æœ‰ä»·å€¼çš„æ”¶å°¾
3. ä¿æŒåŸæœ‰çš„ç« èŠ‚æ ‡é¢˜å’ŒåŸºæœ¬ç»“æ„
4. ç¡®ä¿é‡å†™åçš„ç»“è®ºä¸ä¸»ä½“ç« èŠ‚çš„å†…å®¹å’Œé£æ ¼ä¿æŒä¸€è‡´

è¯·é‡å†™ç»“è®ºï¼Œä½¿å…¶æ›´å¥½åœ°æ€»ç»“å’Œåæ€æ•´ä¸ªæ–‡æ¡£çš„æ ¸å¿ƒè®ºç‚¹ã€‚"""

        polished_conclusion = llm_client.invoke(conclusion_prompt,
                                                temperature=0.3,
                                                max_tokens=2000)

        logger.info(f"âœ… ç»“è®ºé‡å†™å®Œæˆï¼Œé•¿åº¦: {len(polished_conclusion)} å­—ç¬¦")

        # é‡æ–°ç»„è£…æ–‡æ¡£
        final_document_parts = [polished_intro]

        if middle_content:
            final_document_parts.append(middle_content)

        final_document_parts.append(polished_conclusion)

        final_document = "\n\n".join(final_document_parts)

        logger.info(f"ğŸ“„ èåˆç¼–è¾‘å®Œæˆï¼Œæœ€ç»ˆæ–‡æ¡£é•¿åº¦: {len(final_document)} å­—ç¬¦")

        return {"final_document": final_document}

    except Exception as e:
        logger.error(f"âŒ èåˆç¼–è¾‘å™¨å¤„ç†å¤±è´¥: {str(e)}")
        # è¿”å›åŸå§‹æ–‡æ¡£
        return {"final_document": state.get("final_document", "")}
