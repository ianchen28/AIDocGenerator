# service/src/doc_agent/graph/main_orchestrator/nodes.py
from loguru import logger
import pprint
from typing import Dict, List
import json
from ..state import ResearchState
from ...llm_clients.base import LLMClient
from ...tools.web_search import WebSearchTool
from ...tools.es_search import ESSearchTool
from ...tools.reranker import RerankerTool
from ...llm_clients.providers import EmbeddingClient, RerankerClient

# æ·»åŠ é…ç½®å¯¼å…¥
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_file = Path(__file__)
service_dir = None
for parent in current_file.parents:
    if parent.name == 'service':
        service_dir = parent
        break

if service_dir and str(service_dir) not in sys.path:
    sys.path.insert(0, str(service_dir))

from core.config import settings
from src.doc_agent.utils.search_utils import search_and_rerank


async def initial_research_node(state: ResearchState,
                                web_search_tool: WebSearchTool,
                                es_search_tool: ESSearchTool,
                                reranker_tool: RerankerTool = None,
                                llm_client: LLMClient = None) -> dict:
    """
    åˆå§‹ç ”ç©¶èŠ‚ç‚¹
    
    æ‰§è¡Œé«˜å±‚æ¬¡çš„ç ”ç©¶ï¼Œæ”¶é›†å…³äºä¸»é¢˜çš„æ¦‚è§ˆä¿¡æ¯ï¼Œå¹¶è¿›è¡Œæ•°æ®å‹ç¼©
    
    Args:
        state: ç ”ç©¶çŠ¶æ€ï¼ŒåŒ…å« topic
        web_search_tool: ç½‘ç»œæœç´¢å·¥å…·
        es_search_tool: Elasticsearchæœç´¢å·¥å…·
        reranker_tool: é‡æ’åºå·¥å…·ï¼ˆå¯é€‰ï¼‰
        llm_client: LLMå®¢æˆ·ç«¯ï¼ˆç”¨äºæ•°æ®å¤„ç†ï¼‰
        
    Returns:
        dict: åŒ…å« initial_gathered_data çš„å­—å…¸
    """
    topic = state.get("topic", "")
    if not topic:
        raise ValueError("ä¸»é¢˜ä¸èƒ½ä¸ºç©º")

    logger.info(f"ğŸ” å¼€å§‹åˆå§‹ç ”ç©¶: {topic}")

    # ç”Ÿæˆåˆå§‹æœç´¢æŸ¥è¯¢ - æ›´é€šç”¨å’Œå¹¿æ³›çš„æŸ¥è¯¢
    initial_queries = [
        f"{topic} æ¦‚è¿°", f"{topic} ä¸»è¦å†…å®¹", f"{topic} å…³é”®è¦ç‚¹", f"{topic} æœ€æ–°å‘å±•",
        f"{topic} é‡è¦æ€§"
    ]

    all_results = []

    # è·å–embeddingé…ç½®
    embedding_config = settings.supported_models.get("gte_qwen")
    embedding_client = None
    if embedding_config:
        try:
            embedding_client = EmbeddingClient(
                base_url=embedding_config.url,
                api_key=embedding_config.api_key)
            logger.info("âœ… Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸  Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            embedding_client = None

    # æ‰§è¡Œæœç´¢
    for i, query in enumerate(initial_queries, 1):
        logger.info(f"æ‰§è¡Œåˆå§‹æœç´¢ {i}/{len(initial_queries)}: {query}")

        # ç½‘ç»œæœç´¢
        web_results = ""
        try:
            web_results = web_search_tool.search(query)
            if "æ¨¡æ‹Ÿ" in web_results or "mock" in web_results.lower():
                web_results = ""
        except Exception as e:
            logger.error(f"ç½‘ç»œæœç´¢å¤±è´¥: {str(e)}")
            web_results = ""

        # ESæœç´¢ - ä½¿ç”¨æ–°çš„æœç´¢å’Œé‡æ’åºåŠŸèƒ½
        es_results = ""
        try:
            if embedding_client:
                # å°è¯•å‘é‡æ£€ç´¢
                try:
                    embedding_response = embedding_client.invoke(query)
                    embedding_data = json.loads(embedding_response)

                    # è§£æå‘é‡
                    if isinstance(embedding_data, list):
                        query_vector = embedding_data[0] if len(
                            embedding_data) > 0 and isinstance(
                                embedding_data[0], list) else embedding_data
                    elif isinstance(embedding_data,
                                    dict) and 'data' in embedding_data:
                        query_vector = embedding_data['data']
                    else:
                        query_vector = None

                    if query_vector and len(query_vector) == 1536:
                        # ä½¿ç”¨æ–°çš„æœç´¢å’Œé‡æ’åºåŠŸèƒ½
                        _, reranked_results, formatted_es_results = await search_and_rerank(
                            es_search_tool=es_search_tool,
                            query=query,
                            query_vector=query_vector,
                            reranker_tool=reranker_tool,
                            initial_top_k=8,  # åˆå§‹ç ”ç©¶è·å–æ›´å¤šç»“æœ
                            final_top_k=5  # é‡æ’åºåè¿”å›top5
                        )
                        es_results = formatted_es_results
                        logger.info(
                            f"âœ… å‘é‡æ£€ç´¢+é‡æ’åºæ‰§è¡ŒæˆåŠŸï¼Œç»“æœé•¿åº¦: {len(formatted_es_results)}"
                        )
                    else:
                        # å›é€€åˆ°æ–‡æœ¬æœç´¢
                        _, reranked_results, formatted_es_results = await search_and_rerank(
                            es_search_tool=es_search_tool,
                            query=query,
                            query_vector=None,
                            reranker_tool=reranker_tool,
                            initial_top_k=8,
                            final_top_k=5)
                        es_results = formatted_es_results
                        logger.info(
                            f"âœ… æ–‡æœ¬æœç´¢+é‡æ’åºæ‰§è¡ŒæˆåŠŸï¼Œç»“æœé•¿åº¦: {len(formatted_es_results)}"
                        )
                except Exception as e:
                    logger.error(f"å‘é‡æ£€ç´¢å¤±è´¥: {str(e)}")
                    # å›é€€åˆ°æ–‡æœ¬æœç´¢
                    _, reranked_results, formatted_es_results = await search_and_rerank(
                        es_search_tool=es_search_tool,
                        query=query,
                        query_vector=None,
                        reranker_tool=reranker_tool,
                        initial_top_k=8,
                        final_top_k=5)
                    es_results = formatted_es_results
                    logger.info(
                        f"âœ… æ–‡æœ¬æœç´¢+é‡æ’åºæ‰§è¡ŒæˆåŠŸï¼Œç»“æœé•¿åº¦: {len(formatted_es_results)}")
            else:
                # æ²¡æœ‰embeddingå®¢æˆ·ç«¯ï¼Œç›´æ¥ä½¿ç”¨æ–‡æœ¬æœç´¢
                _, reranked_results, formatted_es_results = await search_and_rerank(
                    es_search_tool=es_search_tool,
                    query=query,
                    query_vector=None,
                    reranker_tool=reranker_tool,
                    initial_top_k=8,
                    final_top_k=5)
                es_results = formatted_es_results
                logger.info(
                    f"âœ… æ–‡æœ¬æœç´¢+é‡æ’åºæ‰§è¡ŒæˆåŠŸï¼Œç»“æœé•¿åº¦: {len(formatted_es_results)}")

        except Exception as e:
            logger.error(f"ESæœç´¢å¤±è´¥: {str(e)}")
            es_results = f"ESæœç´¢å¤±è´¥: {str(e)}"

        # èšåˆç»“æœ
        query_results = f"=== åˆå§‹æœç´¢ {i}: {query} ===\n\n"
        if web_results:
            query_results += f"ç½‘ç»œæœç´¢ç»“æœ:\n{web_results}\n\n"
        if es_results:
            query_results += f"çŸ¥è¯†åº“æœç´¢ç»“æœ:\n{es_results}\n\n"
        if not web_results and not es_results:
            query_results += "æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœ\n\n"

        all_results.append(query_results)

    # åˆå¹¶æ‰€æœ‰ç»“æœ
    raw_initial_gathered_data = "\n\n".join(all_results)
    logger.info(f"âœ… åˆå§‹ç ”ç©¶å®Œæˆï¼Œæ”¶é›†åˆ° {len(raw_initial_gathered_data)} å­—ç¬¦çš„åŸå§‹æ•°æ®")

    # å¦‚æœæ•°æ®é‡è¿‡å¤§ï¼Œè¿›è¡Œå‹ç¼©å¤„ç†
    if len(raw_initial_gathered_data) > 10000:  # è¶…è¿‡10Kå­—ç¬¦æ—¶å‹ç¼©
        logger.info("ğŸ“Š æ•°æ®é‡è¾ƒå¤§ï¼Œè¿›è¡Œå‹ç¼©å¤„ç†...")

        if llm_client:
            try:
                # å¯¼å…¥å†…å®¹å¤„ç†å·¥å…·
                from ...utils.content_processor import process_research_data

                # å¤„ç†ç ”ç©¶æ•°æ®
                processed_data = process_research_data(
                    raw_initial_gathered_data,
                    llm_client,
                    summary_length=4000,  # æ‘˜è¦é•¿åº¦
                    key_points_count=10  # å…³é”®è¦ç‚¹æ•°é‡
                )

                # æ„å»ºå‹ç¼©åçš„æ•°æ®
                compressed_data = f"""
# ç ”ç©¶æ•°æ®æ‘˜è¦

## å…³é”®è¦ç‚¹
{chr(10).join([f"{i+1}. {point}" for i, point in enumerate(processed_data['key_points'])])}

## è¯¦ç»†æ‘˜è¦
{processed_data['summary']}

---
*åŸå§‹æ•°æ®é•¿åº¦: {processed_data['original_length']} å­—ç¬¦*
*å‹ç¼©åé•¿åº¦: {processed_data['processed_length']} å­—ç¬¦*
*å‹ç¼©ç‡: {((processed_data['original_length'] - processed_data['processed_length']) / processed_data['original_length'] * 100):.1f}%*
"""

                logger.info(f"âœ… æ•°æ®å‹ç¼©å®Œæˆ: {len(compressed_data)} å­—ç¬¦")
                return {"initial_gathered_data": compressed_data}

            except Exception as e:
                logger.warning(f"âš ï¸  æ•°æ®å‹ç¼©å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨ç®€å•æˆªæ–­")
                # åå¤‡æ–¹æ¡ˆï¼šç®€å•æˆªæ–­
                truncated_data = raw_initial_gathered_data[:8000] + "\n\n... (å†…å®¹å·²æˆªæ–­)"
                return {"initial_gathered_data": truncated_data}
        else:
            logger.warning("âš ï¸  æœªæä¾›LLMå®¢æˆ·ç«¯ï¼Œä½¿ç”¨ç®€å•æˆªæ–­")
            truncated_data = raw_initial_gathered_data[:8000] + "\n\n... (å†…å®¹å·²æˆªæ–­)"
            return {"initial_gathered_data": truncated_data}
    else:
        # æ•°æ®é‡ä¸å¤§ï¼Œç›´æ¥è¿”å›
        return {"initial_gathered_data": raw_initial_gathered_data}


def outline_generation_node(state: ResearchState,
                            llm_client: LLMClient) -> dict:
    """
    å¤§çº²ç”ŸæˆèŠ‚ç‚¹
    
    åŸºäºåˆå§‹ç ”ç©¶æ•°æ®ç”Ÿæˆæ–‡æ¡£çš„ç»“æ„åŒ–å¤§çº²
    
    Args:
        state: ç ”ç©¶çŠ¶æ€ï¼ŒåŒ…å« topic å’Œ initial_gathered_data
        llm_client: LLMå®¢æˆ·ç«¯å®ä¾‹
        
    Returns:
        dict: åŒ…å« document_outline çš„å­—å…¸
    """
    topic = state.get("topic", "")
    initial_gathered_data = state.get("initial_gathered_data", "")

    if not topic:
        raise ValueError("ä¸»é¢˜ä¸èƒ½ä¸ºç©º")

    if not initial_gathered_data:
        raise ValueError("åˆå§‹ç ”ç©¶æ•°æ®ä¸èƒ½ä¸ºç©º")

    logger.info(f"ğŸ“‹ å¼€å§‹ç”Ÿæˆæ–‡æ¡£å¤§çº²: {topic}")

    # è·å–é…ç½®
    outline_config = settings.get_agent_component_config("task_planner")
    if not outline_config:
        temperature = 0.3  # å¤§çº²ç”Ÿæˆéœ€è¦æ›´ä½çš„æ¸©åº¦ä»¥ç¡®ä¿ç»“æ„æ€§
        max_tokens = 2000
        extra_params = {}
    else:
        temperature = outline_config.temperature * 0.7  # é™ä½æ¸©åº¦
        max_tokens = outline_config.max_tokens
        extra_params = outline_config.extra_params

    # å¯¼å…¥æç¤ºè¯æ¨¡æ¿
    from ...prompts import OUTLINE_GENERATION_PROMPT

    # æ„å»ºæç¤ºè¯
    prompt = OUTLINE_GENERATION_PROMPT.format(
        topic=topic,
        initial_gathered_data=initial_gathered_data[:8000]  # é™åˆ¶è¾“å…¥é•¿åº¦
    )

    logger.debug(
        f"Invoking LLM with outline generation prompt:\n{pprint.pformat(prompt)}"
    )

    try:
        # è°ƒç”¨LLMç”Ÿæˆå¤§çº²
        response = llm_client.invoke(prompt,
                                     temperature=temperature,
                                     max_tokens=max_tokens,
                                     **extra_params)

        # è§£æJSONå“åº”
        try:
            # æ¸…ç†å“åº”ï¼Œç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
            cleaned_response = response.strip()
            if cleaned_response.startswith("```json"):
                cleaned_response = cleaned_response[7:]
            if cleaned_response.startswith("```"):
                cleaned_response = cleaned_response[3:]
            if cleaned_response.endswith("```"):
                cleaned_response = cleaned_response[:-3]

            document_outline = json.loads(cleaned_response.strip())

            # éªŒè¯å¤§çº²ç»“æ„
            if "chapters" not in document_outline:
                raise ValueError("å¤§çº²ç¼ºå°‘chapterså­—æ®µ")

            logger.info(f"âœ… æˆåŠŸç”Ÿæˆå¤§çº²ï¼ŒåŒ…å« {len(document_outline['chapters'])} ä¸ªç« èŠ‚")

        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSONè§£æå¤±è´¥: {str(e)}")
            # è¿”å›é»˜è®¤å¤§çº²
            document_outline = {
                "title":
                topic,
                "summary":
                f"å…³äº{topic}çš„ç»¼åˆæ€§æ–‡æ¡£",
                "chapters": [{
                    "chapter_number": 1,
                    "chapter_title": "æ¦‚è¿°ä¸èƒŒæ™¯",
                    "description": f"ä»‹ç»{topic}çš„åŸºæœ¬æ¦‚å¿µã€å†å²èƒŒæ™¯å’Œé‡è¦æ€§",
                    "key_points": ["åŸºæœ¬å®šä¹‰", "å†å²å‘å±•", "ç°å®æ„ä¹‰"],
                    "estimated_sections": 3
                }, {
                    "chapter_number": 2,
                    "chapter_title": "æ ¸å¿ƒå†…å®¹åˆ†æ",
                    "description": f"æ·±å…¥åˆ†æ{topic}çš„æ ¸å¿ƒè¦ç´ å’Œå…³é”®ç‰¹å¾",
                    "key_points": ["ä¸»è¦ç‰¹å¾", "å…³é”®è¦ç´ ", "æŠ€æœ¯ç»†èŠ‚"],
                    "estimated_sections": 4
                }, {
                    "chapter_number": 3,
                    "chapter_title": "åº”ç”¨ä¸å®è·µ",
                    "description": f"æ¢è®¨{topic}çš„å®é™…åº”ç”¨åœºæ™¯å’Œæœ€ä½³å®è·µ",
                    "key_points": ["åº”ç”¨åœºæ™¯", "æ¡ˆä¾‹åˆ†æ", "å®æ–½æ–¹æ³•"],
                    "estimated_sections": 3
                }, {
                    "chapter_number": 4,
                    "chapter_title": "æœªæ¥å±•æœ›",
                    "description": f"åˆ†æ{topic}çš„å‘å±•è¶‹åŠ¿å’Œæœªæ¥å¯èƒ½æ€§",
                    "key_points": ["å‘å±•è¶‹åŠ¿", "æŒ‘æˆ˜æœºé‡", "å‰æ™¯é¢„æµ‹"],
                    "estimated_sections": 2
                }],
                "total_chapters":
                4,
                "estimated_total_words":
                12000
            }

        return {"document_outline": document_outline}

    except Exception as e:
        logger.error(f"âŒ å¤§çº²ç”Ÿæˆå¤±è´¥: {str(e)}")
        # è¿”å›åŸºç¡€å¤§çº²
        return {
            "document_outline": {
                "title":
                topic,
                "summary":
                f"å…³äº{topic}çš„æ–‡æ¡£",
                "chapters": [{
                    "chapter_number": 1,
                    "chapter_title": "å¼•è¨€",
                    "description": f"ä»‹ç»{topic}çš„åŸºæœ¬ä¿¡æ¯",
                    "key_points": ["æ¦‚è¿°"],
                    "estimated_sections": 2
                }],
                "total_chapters":
                1,
                "estimated_total_words":
                3000
            }
        }


def split_chapters_node(state: ResearchState) -> dict:
    """
    ç« èŠ‚æ‹†åˆ†èŠ‚ç‚¹
    
    å°†æ–‡æ¡£å¤§çº²æ‹†åˆ†ä¸ºç‹¬ç«‹çš„ç« èŠ‚ä»»åŠ¡åˆ—è¡¨
    
    Args:
        state: ç ”ç©¶çŠ¶æ€ï¼ŒåŒ…å« document_outline
        
    Returns:
        dict: åŒ…å« chapters_to_process å’Œ current_chapter_index çš„å­—å…¸
    """
    document_outline = state.get("document_outline", {})

    if not document_outline or "chapters" not in document_outline:
        raise ValueError("æ–‡æ¡£å¤§çº²ä¸å­˜åœ¨æˆ–æ ¼å¼æ— æ•ˆ")

    logger.info(f"ğŸ“‚ å¼€å§‹æ‹†åˆ†ç« èŠ‚ä»»åŠ¡")

    # ä»å¤§çº²ä¸­æå–ç« èŠ‚ä¿¡æ¯
    chapters = document_outline.get("chapters", [])

    # åˆ›å»ºç« èŠ‚ä»»åŠ¡åˆ—è¡¨
    chapters_to_process = []
    for chapter in chapters:
        chapter_task = {
            "chapter_number":
            chapter.get("chapter_number",
                        len(chapters_to_process) + 1),
            "chapter_title":
            chapter.get("chapter_title", f"ç¬¬{len(chapters_to_process) + 1}ç« "),
            "description":
            chapter.get("description", ""),
            "key_points":
            chapter.get("key_points", []),
            "estimated_sections":
            chapter.get("estimated_sections", 3),
            "research_data":
            ""  # å°†åœ¨ç« èŠ‚å·¥ä½œæµä¸­å¡«å……
        }
        chapters_to_process.append(chapter_task)

    logger.info(f"âœ… æˆåŠŸåˆ›å»º {len(chapters_to_process)} ä¸ªç« èŠ‚ä»»åŠ¡")

    # æ‰“å°ç« èŠ‚åˆ—è¡¨
    for i, chapter in enumerate(chapters_to_process):
        logger.info(
            f"  ğŸ“„ ç¬¬{chapter['chapter_number']}ç« : {chapter['chapter_title']}")

    return {
        "chapters_to_process": chapters_to_process,
        "current_chapter_index": 0,
        "completed_chapters_content": []  # åˆå§‹åŒ–å·²å®Œæˆç« èŠ‚åˆ—è¡¨
    }
