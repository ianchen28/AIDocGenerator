"""
ç ”ç©¶èŠ‚ç‚¹æ¨¡å—

è´Ÿè´£åˆå§‹ç ”ç©¶ï¼Œæ”¶é›†ä¸»é¢˜ç›¸å…³çš„ä¿¡æ¯æº
"""

import asyncio
import json
from typing import Any, Dict, List, Optional

from doc_agent.core.logger import logger
from doc_agent.core.config import settings
from doc_agent.graph.common import (
    parse_es_search_results,
    parse_web_search_results,
)
from doc_agent.graph.state import ResearchState
from doc_agent.llm_clients.base import LLMClient
from doc_agent.llm_clients.providers import EmbeddingClient
from doc_agent.tools.es_search import ESSearchTool
from doc_agent.tools.reranker import RerankerTool
from doc_agent.tools.web_search import WebSearchTool
from doc_agent.utils.search_utils import search_and_rerank


async def initial_research_node(state: ResearchState,
                                web_search_tool: WebSearchTool,
                                es_search_tool: ESSearchTool,
                                reranker_tool: RerankerTool = None,
                                llm_client: LLMClient = None) -> dict:
    """
    åˆå§‹ç ”ç©¶èŠ‚ç‚¹ - ç»Ÿä¸€ç‰ˆæœ¬
    åŸºäºä¸»é¢˜è¿›è¡Œåˆå§‹ç ”ç©¶ï¼Œæ”¶é›†ç›¸å…³ä¿¡æ¯æº
    æ ¹æ®é…ç½®è‡ªåŠ¨è°ƒæ•´æœç´¢æ·±åº¦å’ŒæŸ¥è¯¢æ•°é‡
    
    Args:
        state: ç ”ç©¶çŠ¶æ€ï¼ŒåŒ…å« topic
        web_search_tool: ç½‘ç»œæœç´¢å·¥å…·
        es_search_tool: ESæœç´¢å·¥å…·
        reranker_tool: é‡æ’åºå·¥å…·
        llm_client: LLMå®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        dict: åŒ…å« initial_sources çš„å­—å…¸ï¼ŒåŒ…å« Source å¯¹è±¡åˆ—è¡¨
    """
    topic = state.get("topic", "")
    if not topic:
        raise ValueError("ä¸»é¢˜ä¸èƒ½ä¸ºç©º")

    # è·å–å¤æ‚åº¦é…ç½®
    complexity_config = settings.get_complexity_config()

    logger.info(f"ğŸ” å¼€å§‹åˆå§‹ç ”ç©¶ (æ¨¡å¼: {complexity_config['level']}): {topic}")

    # æ ¹æ®é…ç½®ç”ŸæˆæŸ¥è¯¢æ•°é‡
    num_queries = complexity_config['initial_search_queries']

    # ç”Ÿæˆæœç´¢æŸ¥è¯¢
    if num_queries == 2:  # å¿«é€Ÿæ¨¡å¼
        initial_queries = [f"{topic} æ¦‚è¿°", f"{topic} ä¸»è¦å†…å®¹"]
    elif num_queries <= 5:  # æ ‡å‡†æ¨¡å¼
        initial_queries = [
            f"{topic} æ¦‚è¿°", f"{topic} ä¸»è¦å†…å®¹", f"{topic} å…³é”®è¦ç‚¹", f"{topic} æœ€æ–°å‘å±•",
            f"{topic} é‡è¦æ€§"
        ][:num_queries]
    else:  # å…¨é¢æ¨¡å¼
        initial_queries = [
            f"{topic} æ¦‚è¿°", f"{topic} ä¸»è¦å†…å®¹", f"{topic} å…³é”®è¦ç‚¹", f"{topic} æœ€æ–°å‘å±•",
            f"{topic} é‡è¦æ€§", f"{topic} å®è·µæ¡ˆä¾‹", f"{topic} æœªæ¥è¶‹åŠ¿", f"{topic} ç›¸å…³æŠ€æœ¯"
        ][:num_queries]

    logger.info(f"ğŸ“Š é…ç½®æœç´¢è½®æ•°: {num_queries}ï¼Œå®é™…æ‰§è¡Œ: {len(initial_queries)} è½®")

    all_sources = []  # å­˜å‚¨æ‰€æœ‰ Source å¯¹è±¡
    source_id_counter = 1  # æºIDè®¡æ•°å™¨

    # è·å–embeddingé…ç½®
    embedding_config = settings.supported_models.get("gte_qwen")
    embedding_client = None
    if embedding_config:
        try:
            embedding_client = EmbeddingClient(
                base_url=embedding_config.url,
                api_key=embedding_config.api_key)
            logger.info("âœ… Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸  Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            embedding_client = None

    # æ‰§è¡Œæœç´¢
    for i, query in enumerate(initial_queries, 1):
        logger.info(f"æ‰§è¡Œåˆå§‹æœç´¢ {i}/{len(initial_queries)}: {query}")

        # ç½‘ç»œæœç´¢
        web_raw_results = []
        web_str_results = ""
        try:
            # ä½¿ç”¨å¼‚æ­¥æœç´¢æ–¹æ³•
            web_raw_results, web_str_results = await web_search_tool.search_async(
                query)
            if "æ¨¡æ‹Ÿ" in web_str_results or "mock" in web_str_results.lower():
                web_str_results = ""
                web_raw_results = []
        except Exception as e:
            logger.error(f"ç½‘ç»œæœç´¢å¤±è´¥: {str(e)}")
            web_str_results = ""
            web_raw_results = []

        # ESæœç´¢
        es_raw_results = []
        es_str_results = ""
        try:
            if embedding_client:
                # å°è¯•å‘é‡æ£€ç´¢
                try:
                    embedding_response = embedding_client.invoke(query)
                    embedding_data = json.loads(embedding_response)

                    # è§£æå‘é‡
                    if isinstance(embedding_data, list):
                        query_vector = embedding_data[0] if len(
                            embedding_data) > 0 and isinstance(
                                embedding_data[0], list) else embedding_data
                    elif isinstance(embedding_data,
                                    dict) and 'data' in embedding_data:
                        query_vector = embedding_data['data']
                    else:
                        query_vector = None

                    if query_vector:
                        # ä½¿ç”¨å‘é‡æ£€ç´¢
                        _, es_raw_results, es_str_results = await search_and_rerank(
                            es_search_tool, query, query_vector, reranker_tool)
                        logger.info(
                            f"âœ… å‘é‡æ£€ç´¢+é‡æ’åºæ‰§è¡ŒæˆåŠŸï¼Œç»“æœé•¿åº¦: {len(es_raw_results)}")
                    else:
                        # å›é€€åˆ°æ–‡æœ¬æœç´¢
                        es_raw_results = await es_search_tool.search(query)
                        logger.info(f"âœ… æ–‡æœ¬æœç´¢æ‰§è¡ŒæˆåŠŸï¼Œç»“æœé•¿åº¦: {len(es_raw_results)}")

                except Exception as e:
                    logger.warning(f"âš ï¸  å‘é‡æ£€ç´¢å¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬æœç´¢: {str(e)}")
                    es_raw_results = await es_search_tool.search(query)
            else:
                # ç›´æ¥ä½¿ç”¨æ–‡æœ¬æœç´¢
                es_raw_results = await es_search_tool.search(query)

        except Exception as e:
            logger.error(f"ESæœç´¢å¤±è´¥: {str(e)}")
            es_raw_results = []

        # å¤„ç†æœç´¢ç»“æœå¹¶åˆ›å»ºSourceå¯¹è±¡
        if web_str_results and web_str_results.strip():
            try:
                web_sources = parse_web_search_results(web_raw_results, query,
                                                       source_id_counter)
                all_sources.extend(web_sources)
                source_id_counter += len(web_sources)
                logger.info(f"âœ… ä»ç½‘ç»œæœç´¢ä¸­æå–åˆ° {len(web_sources)} ä¸ªæº")
            except Exception as e:
                logger.error(f"âŒ è§£æç½‘ç»œæœç´¢ç»“æœå¤±è´¥: {str(e)}")

        if es_raw_results and len(es_raw_results) > 0:
            try:
                es_sources = parse_es_search_results(es_raw_results, query,
                                                     source_id_counter)
                all_sources.extend(es_sources)
                source_id_counter += len(es_sources)
                logger.info(f"âœ… ä»ESæœç´¢ä¸­æå–åˆ° {len(es_sources)} ä¸ªæº")
            except Exception as e:
                logger.error(f"âŒ è§£æESæœç´¢ç»“æœå¤±è´¥: {str(e)}")

    # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦æˆªæ–­æ•°æ®
    truncate_length = complexity_config.get('data_truncate_length', -1)
    if truncate_length > 0:
        # é™åˆ¶æ¯ä¸ªæºçš„å†…å®¹é•¿åº¦
        for source in all_sources:
            if len(source.content) > truncate_length // len(all_sources):
                source.content = source.content[:truncate_length //
                                                len(all_sources
                                                    )] + "... (å†…å®¹å·²æˆªæ–­)"

    logger.info(f"âœ… åˆå§‹ç ”ç©¶å®Œæˆï¼Œæ”¶é›†åˆ° {len(all_sources)} ä¸ªä¿¡æ¯æº")

    return {"initial_sources": all_sources}
