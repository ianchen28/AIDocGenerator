"""
ç ”ç©¶èŠ‚ç‚¹æ¨¡å—

è´Ÿè´£åˆå§‹ç ”ç©¶ï¼Œæ”¶é›†ä¸»é¢˜ç›¸å…³çš„ä¿¡æ¯æº
"""

import json

from doc_agent.core.config import settings
from doc_agent.core.logger import logger
from doc_agent.graph.callbacks import publish_event, safe_serialize
from doc_agent.graph.common import (
    parse_es_search_results,
    parse_web_search_results,
)
from doc_agent.graph.state import ResearchState
from doc_agent.llm_clients.base import LLMClient
from doc_agent.llm_clients.providers import EmbeddingClient
from doc_agent.tools.es_search import ESSearchTool
from doc_agent.tools.reranker import RerankerTool
from doc_agent.tools.web_search import WebSearchTool
from doc_agent.utils.search_utils import search_and_rerank


async def initial_research_node(state: ResearchState,
                                web_search_tool: WebSearchTool,
                                es_search_tool: ESSearchTool,
                                reranker_tool: RerankerTool = None,
                                llm_client: LLMClient = None) -> dict:
    """
    åˆå§‹ç ”ç©¶èŠ‚ç‚¹ - ç»Ÿä¸€ç‰ˆæœ¬
    åŸºäºä¸»é¢˜è¿›è¡Œåˆå§‹ç ”ç©¶ï¼Œæ”¶é›†ç›¸å…³ä¿¡æ¯æº
    æ ¹æ®é…ç½®è‡ªåŠ¨è°ƒæ•´æœç´¢æ·±åº¦å’ŒæŸ¥è¯¢æ•°é‡
    
    Args:
        state: ç ”ç©¶çŠ¶æ€ï¼ŒåŒ…å« topic
        web_search_tool: ç½‘ç»œæœç´¢å·¥å…·
        es_search_tool: ESæœç´¢å·¥å…·
        reranker_tool: é‡æ’åºå·¥å…·
        llm_client: LLMå®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        dict: åŒ…å« initial_sources çš„å­—å…¸ï¼ŒåŒ…å« Source å¯¹è±¡åˆ—è¡¨
    """
    task_prompt = state.get("task_prompt", "")
    if not task_prompt:
        raise ValueError("ä¸» task_prompt ä¸èƒ½ä¸ºç©º")

    # LLM æå– topicï¼Œè¦æ±‚å­—æ•°ï¼ˆå¦‚æœ‰ï¼‰ï¼Œå…¶ä»–æ ¼å¼å†…å®¹è¦æ±‚
    # è¿”å›
    # ```json
    # {
    #     "topic": "ä»»åŠ¡çš„ä¸»é¢˜",
    #     "word_count": "ä»»åŠ¡çš„å­—æ•°è¦æ±‚",
    #     "other_requirements": "ä»»åŠ¡çš„å…¶ä»–æ ¼å¼å†…å®¹è¦æ±‚"
    # }
    # ```
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»»åŠ¡åˆ†æå¼•æ“ã€‚ä½ çš„å”¯ä¸€ç›®æ ‡æ˜¯è§£æç”¨æˆ·æä¾›çš„æ–‡æœ¬ï¼Œå¹¶ä»ä¸­æå–å…³é”®çš„ä»»åŠ¡è¦æ±‚ã€‚

ä½ å¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹æŒ‡ä»¤ï¼š
1.  åˆ†ææ–‡æœ¬ï¼Œè¯†åˆ«å‡ºä»»åŠ¡çš„ã€ä¸»é¢˜ã€‘ã€ã€å­—æ•°è¦æ±‚ã€‘å’Œã€å…¶ä»–è¦æ±‚ã€‘ã€‚
2.  ä½ çš„è¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªå•ä¸€ã€æœ‰æ•ˆçš„ JSON å¯¹è±¡ï¼Œä¸èƒ½åŒ…å«ä»»ä½• JSON æ ¼å¼ä¹‹å¤–çš„é¢å¤–æ–‡æœ¬ã€è§£é‡Šæˆ–æ³¨é‡Šã€‚
3.  ä¸¥æ ¼æŒ‰ç…§ä¸‹é¢çš„ schema å’Œå­—æ®µè§„åˆ™ç”Ÿæˆ JSONï¼š

```json
{{
    "topic": "ä»»åŠ¡çš„æ ¸å¿ƒä¸»é¢˜æˆ–æ ‡é¢˜ã€‚",
    "word_count": "ä»æ–‡æœ¬ä¸­æå–æ˜ç¡®çš„å­—æ•°è¦æ±‚ï¼Œåªä¿ç•™æ•°å­—éƒ¨åˆ†ä½œä¸ºå­—ç¬¦ä¸²ã€‚å¦‚æœæ²¡æœ‰æåˆ°ä»»ä½•å­—æ•°è¦æ±‚ï¼Œåˆ™è¯¥å­—æ®µçš„å€¼å¿…é¡»æ˜¯ '-1'ã€‚",
    "other_requirements": "é™¤äº†ä¸»é¢˜å’Œå­—æ•°å¤–çš„æ‰€æœ‰å…¶ä»–å…·ä½“è¦æ±‚ï¼Œä¾‹å¦‚æ ¼å¼ã€é£æ ¼ã€éœ€è¦åŒ…å«çš„è¦ç‚¹ã€å—ä¼—ç­‰ã€‚å¦‚æœæ²¡æœ‰ï¼Œåˆ™è¯¥å­—æ®µä¸ºç©ºå­—ç¬¦ä¸² ''ã€‚"
}}
```

ç¤ºä¾‹å­¦ä¹ :

ç¤ºä¾‹ 1:
ç”¨æˆ·è¾“å…¥: "å¸®æˆ‘å†™ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½å¯¹æœªæ¥ç¤¾ä¼šå½±å“çš„æ–‡ç« ï¼Œè¦æ±‚ 800 å­—å·¦å³ï¼Œéœ€è¦åŒ…å«æ­£åä¸¤æ–¹é¢çš„è§‚ç‚¹ã€‚"
JSON è¾“å‡º:

```JSON
{{
    "topic": "äººå·¥æ™ºèƒ½å¯¹æœªæ¥ç¤¾ä¼šå½±å“",
    "word_count": "800",
    "other_requirements": "éœ€è¦åŒ…å«æ­£åä¸¤æ–¹é¢çš„è§‚ç‚¹"
}}
```

ç¤ºä¾‹ 2:
ç”¨æˆ·è¾“å…¥: "å†™ä¸€ä»½å…³äºä¸‹å­£åº¦å¸‚åœºè¥é”€æ´»åŠ¨çš„ç­–åˆ’æ–¹æ¡ˆï¼Œè¦åŒ…æ‹¬ç›®æ ‡å—ä¼—åˆ†æå’Œé¢„ç®—åˆ†é…ã€‚"
JSON è¾“å‡º:

```JSON
{{
    "topic": "ä¸‹å­£åº¦å¸‚åœºè¥é”€æ´»åŠ¨çš„ç­–åˆ’æ–¹æ¡ˆ",
    "word_count": "-1",
    "other_requirements": "è¦åŒ…æ‹¬ç›®æ ‡å—ä¼—åˆ†æå’Œé¢„ç®—åˆ†é…"
}}
```
ç¤ºä¾‹ 3:
ç”¨æˆ·è¾“å…¥: "ç»™æˆ‘è®²è®²å…¨çƒå˜æš–çš„åŸå› "
JSON è¾“å‡º:

```JSON
{{
    "topic": "å…¨çƒå˜æš–çš„åŸå› ",
    "word_count": "-1",
    "other_requirements": ""
}}
```
ç¤ºä¾‹ 4:
ç”¨æˆ·è¾“å…¥: "è¯·ä¸ºæˆ‘ä»¬å…¬å¸çš„å­£åº¦æ€»ç»“æŠ¥å‘Šå†™ä¸€ä¸ªå¼€åœºç™½ï¼Œå¤§çº¦ 200 å­—ï¼Œé£æ ¼è¦æ­£å¼ã€é¼“èˆäººå¿ƒï¼Œå¹¶ä¸”è¦æåˆ°æˆ‘ä»¬å›¢é˜Ÿç¬¬äºŒå­£åº¦çš„ä¸»è¦æˆå°±ï¼š'é¡¹ç›®AæˆåŠŸä¸Šçº¿'å’Œ'å®¢æˆ·æ»¡æ„åº¦æå‡15%'"
JSON è¾“å‡º:

```JSON
{{
    "topic": "å…¬å¸å­£åº¦æ€»ç»“æŠ¥å‘Šçš„å¼€åœºç™½",
    "word_count": "200",
    "other_requirements": "é£æ ¼è¦æ±‚æ­£å¼ã€é¼“èˆäººå¿ƒï¼›éœ€è¦æåˆ°ç¬¬äºŒå­£åº¦çš„ä¸»è¦æˆå°±ï¼š'é¡¹ç›®AæˆåŠŸä¸Šçº¿'å’Œ'å®¢æˆ·æ»¡æ„åº¦æå‡15%'"
}}
```

ä»»åŠ¡å¼€å§‹

ç”¨æˆ·è¾“å…¥:
{task_prompt}
    """
    response = llm_client.invoke(prompt)
    logger.info(f"ğŸ” åˆå§‹ç ”ç©¶: {response}")
    # å»é™¤ ```json å’Œ ```
    response = response.replace("```json", "").replace("```", "")
    response = json.loads(response)
    topic = response.get("topic", "")
    word_count = response.get("word_count", "-1")
    other_requirements = response.get("other_requirements", "")
    if not topic:
        raise ValueError("ä¸»é¢˜ä¸èƒ½ä¸ºç©º")
    try:
        word_count = int(word_count)
    except ValueError:
        word_count = 5000  # é»˜è®¤å€¼
    if word_count < 0:
        word_count = 5000  # é»˜è®¤å€¼
    if other_requirements:
        other_requirements = other_requirements.split("\n")

    # è·å–å¤æ‚åº¦é…ç½®
    complexity_config = settings.get_complexity_config()
    job_id = state.get("job_id", "")
    task_prompt = state.get("task_prompt", "")

    logger.info(f"ğŸ” å¼€å§‹åˆå§‹ç ”ç©¶ (æ¨¡å¼: {complexity_config['level']}): {task_prompt}")

    # Outline-1a & 1b: å¼€å§‹åˆæ­¥è°ƒç ”ï¼Œå¹¶åŒ…å« query
    publish_event(job_id, "åˆæ­¥è°ƒç ”", "outline_generation", "START", {
        "task_prompt": task_prompt,
        "description": "å¼€å§‹æ ¹æ®æ‚¨çš„è¦æ±‚è¿›è¡Œåˆæ­¥è°ƒç ”å’Œä¿¡æ¯æœç´¢..."
    })

    # æ ¹æ®é…ç½®ç”ŸæˆæŸ¥è¯¢æ•°é‡
    num_queries = complexity_config['initial_search_queries']

    # ç”¨ LLM ç”Ÿæˆåˆå§‹æœç´¢æŸ¥è¯¢
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶ç­–ç•¥å¸ˆå’Œä¿¡æ¯æ£€ç´¢ä¸“å®¶ã€‚

ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·æä¾›çš„ã€ä¸»é¢˜ã€‘å’Œã€å…·ä½“è¦æ±‚ã€‘ï¼Œç”Ÿæˆä¸€ç»„ç”¨äºåœ¨å¤šä¸ªä¿¡æ¯æºï¼ˆå¦‚ Google æœç´¢ã€Elasticsearch æ•°æ®åº“ï¼‰è¿›è¡Œæ£€ç´¢çš„é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„æœç´¢æŸ¥è¯¢(Search Queries)ã€‚

**æ ¸å¿ƒæŒ‡ä»¤:**

1.  **æ·±å…¥ç†è§£**: åˆ†æã€ä¸»é¢˜ã€‘å’Œã€å…·ä½“è¦æ±‚ã€‘ï¼Œç†è§£ç”¨æˆ·çš„æ ¸å¿ƒæ„å›¾ã€‚
2.  **æ‹†è§£ä¸æ‰©å±•**: å°†å¤æ‚çš„ä»»åŠ¡æ‹†è§£æˆå¤šä¸ªæ›´å°ã€æ›´å…·ä½“çš„å­é—®é¢˜ã€‚ä»ä¸åŒè§’åº¦è¿›è¡Œæ€è€ƒï¼Œä¾‹å¦‚ï¼š
    * **æ ¸å¿ƒå®šä¹‰**: å…³äºä¸»é¢˜æœ¬èº«æ˜¯ä»€ä¹ˆã€‚
    * **å…³é”®æ–¹é¢**: ä»»åŠ¡è¦æ±‚ä¸­æåˆ°çš„æ¯ä¸ªè¦ç‚¹ã€‚
    * **æ¡ˆä¾‹/æ•°æ®**: å¯»æ‰¾ç›¸å…³çš„å®ä¾‹ã€ç»Ÿè®¡æ•°æ®æˆ–è¯æ®ã€‚
    * **æ–¹æ³•/è¿‡ç¨‹**: å¦‚æœæ˜¯â€œå¦‚ä½•åšâ€çš„é—®é¢˜ï¼Œå¯»æ‰¾å…·ä½“æ­¥éª¤å’Œæ–¹æ³•ã€‚
    * **å¯¹æ¯”/è¯„ä»·**: å¯»æ‰¾æ­£åè§‚ç‚¹ã€ä¼˜ç¼ºç‚¹å¯¹æ¯”ã€‚
3.  **ç”ŸæˆæŸ¥è¯¢**: åŸºäºä¸Šè¿°åˆ†æï¼Œç”Ÿæˆä¸€ç»„ç®€æ´ã€æœ‰æ•ˆçš„æœç´¢æŸ¥è¯¢ã€‚æŸ¥è¯¢åº”è¯¥åƒçœŸäººä¸“å®¶ä¼šè¾“å…¥åˆ°æœç´¢å¼•æ“ä¸­çš„é‚£æ ·ã€‚æ•°é‡ä¸å®œè¿‡å¤šï¼Œä¸€èˆ¬ä»¥ 2-5 ä¸ªä¸ºå®œã€‚
4.  **æ ¼å¼åŒ–è¾“å‡º**: ä½ çš„è¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªå•ä¸€ã€æœ‰æ•ˆçš„ JSON å¯¹è±¡ï¼Œæ ¼å¼ä¸º `{{{{"search_queries": ["æŸ¥è¯¢1", "æŸ¥è¯¢2", ...]}}}}`ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡å­—ã€‚

---
**ç¤ºä¾‹å­¦ä¹ :**

**ç¤ºä¾‹ 1:**
è¾“å…¥:
{{
    "topic": "äººå·¥æ™ºèƒ½å¯¹æœªæ¥ç¤¾ä¼šå½±å“",
    "other_requirements": "éœ€è¦åŒ…å«æ­£åä¸¤æ–¹é¢çš„è§‚ç‚¹"
}}
JSON è¾“å‡º:
```json
{{
    "search_queries": [
        "äººå·¥æ™ºèƒ½å¯¹ç¤¾ä¼šçš„å½±å“",
        "AI æŠ€æœ¯çš„ç§¯æåº”ç”¨æ¡ˆä¾‹",
        "äººå·¥æ™ºèƒ½å¸¦æ¥çš„å¥½å¤„å’Œæœºé‡",
    ]
}}
```

ä»»åŠ¡å¼€å§‹

è¯·æ ¹æ®ä¸‹é¢çš„è¾“å…¥ç”Ÿæˆæœç´¢æŸ¥è¯¢ï¼š

è¾“å…¥:

JSON

{{
    "topic": "{topic}",
    "other_requirements": "{other_requirements or ''}"
}}
```
    """

    response = llm_client.invoke(prompt)
    logger.info(f"ğŸ” åˆå§‹æœç´¢æŸ¥è¯¢: {response}")
    # å»é™¤ ```json å’Œ ```
    response = response.replace("```json", "").replace("```", "")
    try:
        response = json.loads(response)
        initial_queries = response.get("search_queries", [])
        assert isinstance(initial_queries, list)
        assert len(initial_queries) > 0
        assert all(isinstance(query, str) for query in initial_queries)
        num_queries = len(initial_queries)
    except Exception as e:
        logger.error(f"âŒ è§£æåˆå§‹æœç´¢æŸ¥è¯¢å¤±è´¥: {str(e)}ï¼Œå›é€€åˆ°é»˜è®¤æœç´¢")

        # ç”Ÿæˆæœç´¢æŸ¥è¯¢
        if num_queries == 2:  # å¿«é€Ÿæ¨¡å¼
            initial_queries = [f"{topic} æ¦‚è¿°", f"{topic} ä¸»è¦å†…å®¹"]
        elif num_queries <= 5:  # æ ‡å‡†æ¨¡å¼
            initial_queries = [
                f"{topic} æ¦‚è¿°", f"{topic} ä¸»è¦å†…å®¹", f"{topic} å…³é”®è¦ç‚¹",
                f"{topic} æœ€æ–°å‘å±•", f"{topic} é‡è¦æ€§"
            ][:num_queries]
        else:  # å…¨é¢æ¨¡å¼
            initial_queries = [
                f"{topic} æ¦‚è¿°", f"{topic} ä¸»è¦å†…å®¹", f"{topic} å…³é”®è¦ç‚¹",
                f"{topic} æœ€æ–°å‘å±•", f"{topic} é‡è¦æ€§", f"{topic} å®è·µæ¡ˆä¾‹",
                f"{topic} æœªæ¥è¶‹åŠ¿", f"{topic} ç›¸å…³æŠ€æœ¯"
            ][:num_queries]

    logger.info(f"ğŸ“Š é…ç½®æœç´¢è½®æ•°: {num_queries}ï¼Œå®é™…æ‰§è¡Œ: {len(initial_queries)} è½®")

    publish_event(job_id, "åˆæ­¥è°ƒç ”", "outline_generation", "RUNNING", {
        "queries": initial_queries,
        "description": "å¼€å§‹è¿›è¡Œä¿¡æ¯æœç´¢..."
    })

    all_sources = []  # å­˜å‚¨æ‰€æœ‰ Source å¯¹è±¡
    source_id_counter = 1  # æºIDè®¡æ•°å™¨
    web_sources = []  # å­˜å‚¨ç½‘ç»œæœç´¢æº
    es_sources = []  # å­˜å‚¨ESæœç´¢æº

    # è·å–embeddingé…ç½®
    embedding_config = settings.supported_models.get("gte_qwen")
    embedding_client = None
    if embedding_config:
        try:
            embedding_client = EmbeddingClient(
                base_url=embedding_config.url,
                api_key=embedding_config.api_key)
            logger.info("âœ… Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸  Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            embedding_client = None

    # æ‰§è¡Œæœç´¢
    for i, query in enumerate(initial_queries, 1):
        logger.info(f"æ‰§è¡Œåˆå§‹æœç´¢ {i}/{len(initial_queries)}: {query}")

        # ç½‘ç»œæœç´¢
        web_raw_results = []
        web_str_results = ""
        try:
            # ä½¿ç”¨å¼‚æ­¥æœç´¢æ–¹æ³•
            web_raw_results, web_str_results = await web_search_tool.search_async(
                query)
            if "æ¨¡æ‹Ÿ" in web_str_results or "mock" in web_str_results.lower():
                web_str_results = ""
                web_raw_results = []
        except Exception as e:
            logger.error(f"ç½‘ç»œæœç´¢å¤±è´¥: {str(e)}")
            web_str_results = ""
            web_raw_results = []

        # ESæœç´¢
        es_raw_results = []
        es_str_results = ""
        try:
            if embedding_client:
                # å°è¯•å‘é‡æ£€ç´¢
                try:
                    embedding_response = embedding_client.invoke(query)
                    embedding_data = json.loads(embedding_response)

                    # è§£æå‘é‡
                    if isinstance(embedding_data, list):
                        query_vector = embedding_data[0] if len(
                            embedding_data) > 0 and isinstance(
                                embedding_data[0], list) else embedding_data
                    elif isinstance(embedding_data,
                                    dict) and 'data' in embedding_data:
                        query_vector = embedding_data['data']
                    else:
                        query_vector = None

                    if query_vector:
                        # ä½¿ç”¨å‘é‡æ£€ç´¢
                        _, es_raw_results, es_str_results = await search_and_rerank(
                            es_search_tool, query, query_vector, reranker_tool)
                        logger.info(
                            f"âœ… å‘é‡æ£€ç´¢+é‡æ’åºæ‰§è¡ŒæˆåŠŸï¼Œç»“æœé•¿åº¦: {len(es_raw_results)}")
                    else:
                        # å›é€€åˆ°æ–‡æœ¬æœç´¢
                        es_raw_results = await es_search_tool.search(query)
                        logger.info(f"âœ… æ–‡æœ¬æœç´¢æ‰§è¡ŒæˆåŠŸï¼Œç»“æœé•¿åº¦: {len(es_raw_results)}")

                except Exception as e:
                    logger.warning(f"âš ï¸  å‘é‡æ£€ç´¢å¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬æœç´¢: {str(e)}")
                    es_raw_results = await es_search_tool.search(query)
            else:
                # ç›´æ¥ä½¿ç”¨æ–‡æœ¬æœç´¢
                es_raw_results = await es_search_tool.search(query)

        except Exception as e:
            logger.error(f"ESæœç´¢å¤±è´¥: {str(e)}")
            es_raw_results = []

        # å¤„ç†æœç´¢ç»“æœå¹¶åˆ›å»ºSourceå¯¹è±¡
        if web_str_results and web_str_results.strip():
            try:
                current_web_sources = parse_web_search_results(
                    web_raw_results, query, source_id_counter)
                web_sources.extend(current_web_sources)
                all_sources.extend(current_web_sources)
                source_id_counter += len(current_web_sources)
                logger.info(f"âœ… ä»ç½‘ç»œæœç´¢ä¸­æå–åˆ° {len(current_web_sources)} ä¸ªæº")
            except Exception as e:
                logger.error(f"âŒ è§£æç½‘ç»œæœç´¢ç»“æœå¤±è´¥: {str(e)}")

        if es_raw_results and len(es_raw_results) > 0:
            try:
                current_es_sources = parse_es_search_results(
                    es_raw_results, query, source_id_counter)
                es_sources.extend(current_es_sources)
                all_sources.extend(current_es_sources)
                source_id_counter += len(current_es_sources)
                logger.info(f"âœ… ä»ESæœç´¢ä¸­æå–åˆ° {len(current_es_sources)} ä¸ªæº")
            except Exception as e:
                logger.error(f"âŒ è§£æESæœç´¢ç»“æœå¤±è´¥: {str(e)}")

    # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦æˆªæ–­æ•°æ®
    truncate_length = complexity_config.get('data_truncate_length', -1)
    if truncate_length > 0:
        # é™åˆ¶æ¯ä¸ªæºçš„å†…å®¹é•¿åº¦
        for source in all_sources:
            if len(source.content) > truncate_length // len(all_sources):
                source.content = source.content[:truncate_length //
                                                len(all_sources
                                                    )] + "... (å†…å®¹å·²æˆªæ–­)"

    logger.info(f"âœ… åˆå§‹ç ”ç©¶å®Œæˆï¼Œæ”¶é›†åˆ° {len(all_sources)} ä¸ªä¿¡æ¯æº")

    publish_event(
        job_id, "åˆæ­¥è°ƒç ”", "outline_generation", "SUCCESS", {
            "web_sources": [safe_serialize(source) for source in web_sources],
            "es_sources": [safe_serialize(source) for source in es_sources],
            "description":
            f"åˆæ­¥è°ƒç ”å®Œæˆï¼Œæ”¶é›†åˆ°ä¿¡æ¯æºï¼šå†…éƒ¨æœç´¢ç»“æœ {len(es_sources)} ä¸ªï¼Œç½‘ç»œæœç´¢ç»“æœ {len(web_sources)} ä¸ª..."
        })

    return {
        "initial_sources": all_sources,
        "topic": topic,
        "word_count": word_count,
        "prompt_requirements": other_requirements
    }
