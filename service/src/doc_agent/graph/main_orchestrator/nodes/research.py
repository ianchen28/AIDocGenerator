"""
ç ”ç©¶èŠ‚ç‚¹æ¨¡å—

è´Ÿè´£åˆå§‹ç ”ç©¶ï¼Œæ”¶é›†ä¸»é¢˜ç›¸å…³çš„ä¿¡æ¯æº
"""

import json

from loguru import logger

from doc_agent.core.config import settings
from doc_agent.graph.common import (
    parse_es_search_results,
    parse_web_search_results,
)
from doc_agent.graph.state import ResearchState
from doc_agent.llm_clients.base import LLMClient
from doc_agent.llm_clients.providers import EmbeddingClient
from doc_agent.tools.es_search import ESSearchTool
from doc_agent.tools.reranker import RerankerTool
from doc_agent.tools.web_search import WebSearchTool
from doc_agent.utils.search_utils import search_and_rerank


async def initial_research_node(state: ResearchState,
                                web_search_tool: WebSearchTool,
                                es_search_tool: ESSearchTool,
                                reranker_tool: RerankerTool = None,
                                llm_client: LLMClient = None) -> dict:
    """
    åˆå§‹ç ”ç©¶èŠ‚ç‚¹ - ç»Ÿä¸€ç‰ˆæœ¬
    åŸºäºä¸»é¢˜è¿›è¡Œåˆå§‹ç ”ç©¶ï¼Œæ”¶é›†ç›¸å…³ä¿¡æ¯æº
    æ ¹æ®é…ç½®è‡ªåŠ¨è°ƒæ•´æœç´¢æ·±åº¦å’ŒæŸ¥è¯¢æ•°é‡
    
    Args:
        state: ç ”ç©¶çŠ¶æ€ï¼ŒåŒ…å« topic
        web_search_tool: ç½‘ç»œæœç´¢å·¥å…·
        es_search_tool: ESæœç´¢å·¥å…·
        reranker_tool: é‡æ’åºå·¥å…·
        llm_client: LLMå®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        dict: åŒ…å« initial_sources çš„å­—å…¸ï¼ŒåŒ…å« Source å¯¹è±¡åˆ—è¡¨
    """
    topic = state.get("topic", "")
    if not topic:
        raise ValueError("ä¸»é¢˜ä¸èƒ½ä¸ºç©º")

    # è·å–å¤æ‚åº¦é…ç½®
    complexity_config = settings.get_complexity_config()

    logger.info(f"ğŸ” å¼€å§‹åˆå§‹ç ”ç©¶ (æ¨¡å¼: {complexity_config['level']}): {topic}")

    # æ ¹æ®é…ç½®ç”ŸæˆæŸ¥è¯¢æ•°é‡
    num_queries = complexity_config['initial_search_queries']

    # ç”Ÿæˆæœç´¢æŸ¥è¯¢
    if num_queries == 2:  # å¿«é€Ÿæ¨¡å¼
        initial_queries = [f"{topic} æ¦‚è¿°", f"{topic} ä¸»è¦å†…å®¹"]
    elif num_queries <= 5:  # æ ‡å‡†æ¨¡å¼
        initial_queries = [
            f"{topic} æ¦‚è¿°", f"{topic} ä¸»è¦å†…å®¹", f"{topic} å…³é”®è¦ç‚¹", f"{topic} æœ€æ–°å‘å±•",
            f"{topic} é‡è¦æ€§"
        ][:num_queries]
    else:  # å…¨é¢æ¨¡å¼
        initial_queries = [
            f"{topic} æ¦‚è¿°", f"{topic} ä¸»è¦å†…å®¹", f"{topic} å…³é”®è¦ç‚¹", f"{topic} æœ€æ–°å‘å±•",
            f"{topic} é‡è¦æ€§", f"{topic} å®è·µæ¡ˆä¾‹", f"{topic} æœªæ¥è¶‹åŠ¿", f"{topic} ç›¸å…³æŠ€æœ¯"
        ][:num_queries]

    logger.info(f"ğŸ“Š é…ç½®æœç´¢è½®æ•°: {num_queries}ï¼Œå®é™…æ‰§è¡Œ: {len(initial_queries)} è½®")

    all_sources = []  # å­˜å‚¨æ‰€æœ‰ Source å¯¹è±¡
    source_id_counter = 1  # æºIDè®¡æ•°å™¨

    # è·å–embeddingé…ç½®
    embedding_config = settings.supported_models.get("gte_qwen")
    embedding_client = None
    if embedding_config:
        try:
            embedding_client = EmbeddingClient(
                base_url=embedding_config.url,
                api_key=embedding_config.api_key)
            logger.info("âœ… Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸  Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            embedding_client = None

    # æ‰§è¡Œæœç´¢
    for i, query in enumerate(initial_queries, 1):
        logger.info(f"æ‰§è¡Œåˆå§‹æœç´¢ {i}/{len(initial_queries)}: {query}")

        # ç½‘ç»œæœç´¢
        web_results = ""
        try:
            # ä½¿ç”¨å¼‚æ­¥æœç´¢æ–¹æ³•
            web_results = await web_search_tool.search_async(query)
            if "æ¨¡æ‹Ÿ" in web_results or "mock" in web_results.lower():
                web_results = ""
        except Exception as e:
            logger.error(f"ç½‘ç»œæœç´¢å¤±è´¥: {str(e)}")
            web_results = ""

        # ESæœç´¢
        es_results = ""
        try:
            if embedding_client:
                # å°è¯•å‘é‡æ£€ç´¢
                try:
                    embedding_response = embedding_client.invoke(query)
                    embedding_data = json.loads(embedding_response)

                    # è§£æå‘é‡
                    if isinstance(embedding_data, list):
                        query_vector = embedding_data[0] if len(
                            embedding_data) > 0 and isinstance(
                                embedding_data[0], list) else embedding_data
                    elif isinstance(embedding_data,
                                    dict) and 'data' in embedding_data:
                        query_vector = embedding_data['data']
                    else:
                        query_vector = None

                    if query_vector:
                        # ä½¿ç”¨å‘é‡æ£€ç´¢
                        es_results = await search_and_rerank(
                            query, es_search_tool, reranker_tool, query_vector)
                        logger.info(f"âœ… å‘é‡æ£€ç´¢+é‡æ’åºæ‰§è¡ŒæˆåŠŸï¼Œç»“æœé•¿åº¦: {len(es_results)}")
                    else:
                        # å›é€€åˆ°æ–‡æœ¬æœç´¢
                        es_results = await es_search_tool.search(query)
                        logger.info(f"âœ… æ–‡æœ¬æœç´¢æ‰§è¡ŒæˆåŠŸï¼Œç»“æœé•¿åº¦: {len(es_results)}")

                except Exception as e:
                    logger.warning(f"âš ï¸  å‘é‡æ£€ç´¢å¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬æœç´¢: {str(e)}")
                    es_results = await es_search_tool.search(query)
            else:
                # ç›´æ¥ä½¿ç”¨æ–‡æœ¬æœç´¢
                es_results = await es_search_tool.search(query)

        except Exception as e:
            logger.error(f"ESæœç´¢å¤±è´¥: {str(e)}")
            es_results = ""

        # å¤„ç†æœç´¢ç»“æœå¹¶åˆ›å»ºSourceå¯¹è±¡
        if web_results and web_results.strip():
            try:
                web_sources = parse_web_search_results(web_results, query,
                                                       source_id_counter)
                all_sources.extend(web_sources)
                source_id_counter += len(web_sources)
                logger.info(f"âœ… ä»ç½‘ç»œæœç´¢ä¸­æå–åˆ° {len(web_sources)} ä¸ªæº")
            except Exception as e:
                logger.error(f"âŒ è§£æç½‘ç»œæœç´¢ç»“æœå¤±è´¥: {str(e)}")

        if es_results and len(es_results) > 0:
            try:
                # å°†ESSearchResultåˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼
                es_results_str = _convert_es_results_to_string(es_results)
                es_sources = parse_es_search_results(es_results_str, query,
                                                     source_id_counter)
                all_sources.extend(es_sources)
                source_id_counter += len(es_sources)
                logger.info(f"âœ… ä»ESæœç´¢ä¸­æå–åˆ° {len(es_sources)} ä¸ªæº")
            except Exception as e:
                logger.error(f"âŒ è§£æESæœç´¢ç»“æœå¤±è´¥: {str(e)}")

    # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦æˆªæ–­æ•°æ®
    truncate_length = complexity_config.get('data_truncate_length', -1)
    if truncate_length > 0:
        # é™åˆ¶æ¯ä¸ªæºçš„å†…å®¹é•¿åº¦
        for source in all_sources:
            if len(source.content) > truncate_length // len(all_sources):
                source.content = source.content[:truncate_length //
                                                len(all_sources
                                                    )] + "... (å†…å®¹å·²æˆªæ–­)"

    logger.info(f"âœ… åˆå§‹ç ”ç©¶å®Œæˆï¼Œæ”¶é›†åˆ° {len(all_sources)} ä¸ªä¿¡æ¯æº")

    return {"initial_sources": all_sources}


def _convert_es_results_to_string(es_results: list) -> str:
    """
    å°†ESSearchResultåˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼
    
    Args:
        es_results: ESSearchResultåˆ—è¡¨
        
    Returns:
        str: æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²
    """
    if not es_results:
        return ""

    result_lines = []
    for i, result in enumerate(es_results, 1):
        result_lines.append(f"--- æ–‡æ¡£ {i} ---")
        # ä½¿ç”¨ original_content ä½œä¸ºæ ‡é¢˜ï¼Œdiv_content ä½œä¸ºå†…å®¹
        title = result.original_content[:100] + "..." if len(
            result.original_content) > 100 else result.original_content
        content = result.div_content or result.original_content
        result_lines.append(f"æ–‡æ¡£æ ‡é¢˜: {title}")
        result_lines.append(f"æ–‡æ¡£å†…å®¹: {content}")
        if result.source:
            result_lines.append(f"æ–‡æ¡£æ¥æº: {result.source}")
        result_lines.append(f"ç›¸ä¼¼åº¦åˆ†æ•°: {result.score}")
        result_lines.append("")

    return "\n".join(result_lines)
