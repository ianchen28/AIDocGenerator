# service/src/doc_agent/graph/router.py
from typing import Literal
from .state import ResearchState
from ..llm_clients.base import LLMClient


def supervisor_router(
    state: ResearchState, llm_client: LLMClient
) -> Literal["continue_to_writer", "rerun_researcher"]:
    """
    æ¡ä»¶è·¯ç”±: å†³ç­–ä¸‹ä¸€æ­¥èµ°å‘
    
    è¯„ä¼°æ”¶é›†çš„ç ”ç©¶æ•°æ®æ˜¯å¦è¶³å¤Ÿæ’°å†™é«˜è´¨é‡æ–‡æ¡£
    
    Args:
        state: ç ”ç©¶çŠ¶æ€ï¼ŒåŒ…å« topic å’Œ gathered_data
        llm_client: LLMå®¢æˆ·ç«¯å®ä¾‹
        
    Returns:
        str: "continue_to_writer" å¦‚æœæ•°æ®å……è¶³ï¼Œ"rerun_researcher" å¦‚æœéœ€è¦æ›´å¤šç ”ç©¶
    """
    print("ğŸš€ ====== è¿›å…¥ supervisor_router è·¯ç”±èŠ‚ç‚¹ ======")

    # 1. ä»çŠ¶æ€ä¸­æå– topic å’Œ gathered_data
    topic = state.get("topic", "")
    gathered_data = state.get("gathered_data", "")

    print(f"ğŸ“‹ Topic: {topic}")
    print(f"ğŸ“Š Gathered data é•¿åº¦: {len(gathered_data)} å­—ç¬¦")
    print(f"ğŸ“ Gathered data é¢„è§ˆ: {gathered_data[:500]}...")

    if not topic:
        # å¦‚æœæ²¡æœ‰ä¸»é¢˜ï¼Œé»˜è®¤éœ€è¦é‡æ–°ç ”ç©¶
        print("âŒ æ²¡æœ‰ä¸»é¢˜ï¼Œè¿”å› rerun_researcher")
        return "rerun_researcher"

    if not gathered_data:
        # å¦‚æœæ²¡æœ‰æ”¶é›†åˆ°æ•°æ®ï¼Œéœ€è¦é‡æ–°ç ”ç©¶
        print("âŒ æ²¡æœ‰æ”¶é›†åˆ°æ•°æ®ï¼Œè¿”å› rerun_researcher")
        return "rerun_researcher"

    # 2. æ„å»ºé«˜åº¦ç‰¹å®šå’Œçº¦æŸçš„æç¤ºï¼ˆä¸­æ–‡ï¼‰
    prompt = f"""
**è§’è‰²ï¼š** ä½ æ˜¯ä¸€ä½ç ”ç©¶ä¸»ç®¡ï¼Œéœ€è¦åˆ¤æ–­ä¸‹æ–¹èµ„æ–™æ˜¯å¦è¶³å¤Ÿæ’°å†™å®Œæ•´æ–‡æ¡£ã€‚

**ä¸»é¢˜ï¼š**ã€Œ{topic}ã€

**å·²æ”¶é›†çš„ç ”ç©¶èµ„æ–™ï¼š**
{gathered_data}

**è¯„åˆ¤æ ‡å‡†ï¼ˆå®½æ¾ç‰ˆï¼‰ï¼š**
- å¦‚æœèµ„æ–™åŒ…å«3æ¡æˆ–ä»¥ä¸Šè¯¦ç»†æ£€ç´¢ç»“æœï¼Œæ¯æ¡éƒ½æœ‰å…·ä½“å†…å®¹ï¼Œä¸”æ€»å­—æ•°è¶…è¿‡500å­—ï¼Œåˆ™è§†ä¸º"å……è¶³"
- å¦‚æœåªæœ‰1-2æ¡æ£€ç´¢ï¼Œæˆ–å†…å®¹è¿‡äºç®€å•ï¼ˆåªæœ‰å®šä¹‰ï¼‰ï¼Œåˆ™è§†ä¸º"ä¸å……è¶³"

**é‡è¦æç¤ºï¼š**
- åªè¦èµ„æ–™å†…å®¹ä¸°å¯Œã€æœ‰å¤šä¸ªæ–¹é¢ã€æœ‰å…·ä½“ä¿¡æ¯ï¼Œå°±åº”è¯¥è¿”å›FINISH
- ä¸è¦è¿‡äºè‹›åˆ»ï¼Œèµ„æ–™ä¸éœ€è¦å®Œç¾æ— ç¼º
- é‡ç‚¹çœ‹æ˜¯å¦æœ‰è¶³å¤Ÿçš„å†…å®¹æ¥å†™æ–‡æ¡£

**ä½ çš„å†³ç­–ï¼š**
è¯·æ ¹æ®ä¸Šè¿°æ ‡å‡†åˆ¤æ–­ï¼Œå½“å‰èµ„æ–™èƒ½å¦ç”¨äºæ’°å†™æ–‡æ¡£ï¼Ÿåªèƒ½å›ç­”ä¸€ä¸ªå•è¯ï¼š
- "FINISH" = èµ„æ–™å……è¶³ï¼Œå¯ä»¥å†™æ–‡æ¡£
- "CONTINUE" = èµ„æ–™ä¸è¶³ï¼Œéœ€è¦æ›´å¤šç ”ç©¶

è¯·ç›´æ¥è¾“å‡ºï¼šFINISH æˆ– CONTINUE
"""

    try:
        # 3. è°ƒç”¨ LLM å®¢æˆ·ç«¯
        # æ ¹æ®æ¨¡å‹ç±»å‹è®¾ç½®åˆé€‚çš„max_tokens
        max_tokens = 10
        if hasattr(llm_client, 'reasoning') and llm_client.reasoning:
            # å¦‚æœæ”¯æŒæ¨ç†æ¨¡å¼ï¼Œä½¿ç”¨æ›´å¤§çš„tokené™åˆ¶
            max_tokens = 1000
        if hasattr(llm_client,
                   'model_name') and 'gemini' in llm_client.model_name.lower():
            max_tokens = 5000

        # é™åˆ¶ prompt é•¿åº¦ï¼Œé¿å…è¶…è¿‡æ¨¡å‹è¾“å…¥é™åˆ¶
        max_prompt_length = 30000  # 30K å­—ç¬¦é™åˆ¶
        if len(prompt) > max_prompt_length:
            print(f"âš ï¸  Prompt é•¿åº¦ {len(prompt)} è¶…è¿‡é™åˆ¶ {max_prompt_length}ï¼Œè¿›è¡Œæˆªæ–­")
            # ä¿ç•™å¼€å¤´å’Œç»“å°¾çš„é‡è¦ä¿¡æ¯ï¼Œæˆªæ–­ä¸­é—´çš„ç ”ç©¶èµ„æ–™
            header = prompt[:prompt.find("**å·²æ”¶é›†çš„ç ”ç©¶èµ„æ–™ï¼š**") +
                            len("**å·²æ”¶é›†çš„ç ”ç©¶èµ„æ–™ï¼š**")]
            footer = prompt[prompt.find("**è¯„åˆ¤æ ‡å‡†ï¼ˆå®½æ¾ç‰ˆï¼‰ï¼š**"):]
            # ä» gathered_data ä¸­å–å‰ 10000 å­—ç¬¦
            gathered_data_preview = gathered_data[:10000] + "\n\n... (å†…å®¹å·²æˆªæ–­ï¼Œä¿ç•™å‰10000å­—ç¬¦)"
            prompt = header + "\n\n" + gathered_data_preview + "\n\n" + footer
            print(f"ğŸ“ æˆªæ–­å prompt é•¿åº¦: {len(prompt)} å­—ç¬¦")

        print(f"ğŸ¤– è°ƒç”¨ LLM è¿›è¡Œå†³ç­–åˆ¤æ–­...")
        print(f"ğŸ“ Prompt é•¿åº¦: {len(prompt)} å­—ç¬¦")
        print(f"ğŸ”§ å‚æ•°: max_tokens={max_tokens}, temperature=0")

        # æ·»åŠ é‡è¯•æœºåˆ¶
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = llm_client.invoke(prompt,
                                             temperature=0,
                                             max_tokens=max_tokens)
                break  # æˆåŠŸåˆ™è·³å‡ºé‡è¯•å¾ªç¯
            except Exception as e:
                if "400" in str(e) and attempt < max_retries - 1:
                    print(f"âš ï¸  ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥ (400é”™è¯¯)ï¼Œæ­£åœ¨é‡è¯•...")
                    import time
                    time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
                    continue
                else:
                    raise e  # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥æˆ–å…¶ä»–é”™è¯¯ï¼ŒæŠ›å‡ºå¼‚å¸¸

        # 4. è§£æå“åº” - ä½¿ç”¨ strip() å’Œ upper() å¤„ç†ç©ºç™½å’Œå¤§å°å†™
        decision = response.strip().upper()

        # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
        print(f"ğŸ” LLMåŸå§‹å“åº”: '{response}'")
        print(f"ğŸ” å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
        print(f"ğŸ” å¤„ç†åå†³ç­–: '{decision}'")
        print(f"ğŸ” æ˜¯å¦åŒ…å«FINISH: {'FINISH' in decision}")
        print(f"ğŸ” æ˜¯å¦åŒ…å«CONTINUE: {'CONTINUE' in decision}")

        # 5. æ ¹æ®å“åº”å†³å®šè·¯ç”±
        if "FINISH" in decision:
            print("âœ… å†³ç­–: FINISH -> continue_to_writer")
            return "continue_to_writer"
        else:
            # å¦‚æœåŒ…å« "CONTINUE" æˆ–å…¶ä»–ä»»ä½•å†…å®¹ï¼Œéƒ½éœ€è¦é‡æ–°ç ”ç©¶
            print("âœ… å†³ç­–: CONTINUE/å…¶ä»– -> rerun_researcher")
            return "rerun_researcher"

    except Exception as e:
        # å¦‚æœ LLM è°ƒç”¨å¤±è´¥ï¼Œé»˜è®¤ç»§ç»­ç ”ç©¶ä»¥ç¡®ä¿å®‰å…¨
        print(f"âŒ Supervisor router error: {str(e)}")
        print(f"âŒ é”™è¯¯ç±»å‹: {type(e).__name__}")
        return "rerun_researcher"
