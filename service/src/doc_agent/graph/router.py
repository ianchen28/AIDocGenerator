# service/src/doc_agent/graph/router.py
from typing import Literal
from .state import ResearchState
from ..llm_clients.base import LLMClient


def supervisor_router(
    state: ResearchState, llm_client: LLMClient
) -> Literal["continue_to_writer", "rerun_researcher"]:
    """
    æ¡ä»¶è·¯ç”±: å†³ç­–ä¸‹ä¸€æ­¥èµ°å‘
    
    è¯„ä¼°æ”¶é›†çš„ç ”ç©¶æ•°æ®æ˜¯å¦è¶³å¤Ÿæ’°å†™é«˜è´¨é‡æ–‡æ¡£
    
    Args:
        state: ç ”ç©¶çŠ¶æ€ï¼ŒåŒ…å« topic å’Œ gathered_data
        llm_client: LLMå®¢æˆ·ç«¯å®ä¾‹
        
    Returns:
        str: "continue_to_writer" å¦‚æœæ•°æ®å……è¶³ï¼Œ"rerun_researcher" å¦‚æœéœ€è¦æ›´å¤šç ”ç©¶
    """
    # 1. ä»çŠ¶æ€ä¸­æå– topic å’Œ gathered_data
    topic = state.get("topic", "")
    gathered_data = state.get("gathered_data", "")

    if not topic:
        # å¦‚æœæ²¡æœ‰ä¸»é¢˜ï¼Œé»˜è®¤éœ€è¦é‡æ–°ç ”ç©¶
        return "rerun_researcher"

    if not gathered_data:
        # å¦‚æœæ²¡æœ‰æ”¶é›†åˆ°æ•°æ®ï¼Œéœ€è¦é‡æ–°ç ”ç©¶
        return "rerun_researcher"

    # 2. æ„å»ºé«˜åº¦ç‰¹å®šå’Œçº¦æŸçš„æç¤ºï¼ˆä¸­æ–‡ï¼‰
    prompt = f"""
**è§’è‰²ï¼š** ä½ æ˜¯ä¸€ä½ç ”ç©¶ä¸»ç®¡ï¼Œéœ€è¦åˆ¤æ–­ä¸‹æ–¹èµ„æ–™æ˜¯å¦è¶³å¤Ÿæ’°å†™å®Œæ•´æ–‡æ¡£ã€‚

**ä¸»é¢˜ï¼š**ã€Œ{topic}ã€

**å·²æ”¶é›†çš„ç ”ç©¶èµ„æ–™ï¼š**
{gathered_data}

**è¯„åˆ¤æ ‡å‡†ï¼ˆå®½æ¾ç‰ˆï¼‰ï¼š**
- å¦‚æœèµ„æ–™åŒ…å«3æ¡æˆ–ä»¥ä¸Šè¯¦ç»†æ£€ç´¢ç»“æœï¼Œæ¯æ¡éƒ½æœ‰å…·ä½“å†…å®¹ï¼Œä¸”æ€»å­—æ•°è¶…è¿‡500å­—ï¼Œåˆ™è§†ä¸º"å……è¶³"
- å¦‚æœåªæœ‰1-2æ¡æ£€ç´¢ï¼Œæˆ–å†…å®¹è¿‡äºç®€å•ï¼ˆåªæœ‰å®šä¹‰ï¼‰ï¼Œåˆ™è§†ä¸º"ä¸å……è¶³"

**é‡è¦æç¤ºï¼š**
- åªè¦èµ„æ–™å†…å®¹ä¸°å¯Œã€æœ‰å¤šä¸ªæ–¹é¢ã€æœ‰å…·ä½“ä¿¡æ¯ï¼Œå°±åº”è¯¥è¿”å›FINISH
- ä¸è¦è¿‡äºè‹›åˆ»ï¼Œèµ„æ–™ä¸éœ€è¦å®Œç¾æ— ç¼º
- é‡ç‚¹çœ‹æ˜¯å¦æœ‰è¶³å¤Ÿçš„å†…å®¹æ¥å†™æ–‡æ¡£

**ä½ çš„å†³ç­–ï¼š**
è¯·æ ¹æ®ä¸Šè¿°æ ‡å‡†åˆ¤æ–­ï¼Œå½“å‰èµ„æ–™èƒ½å¦ç”¨äºæ’°å†™æ–‡æ¡£ï¼Ÿåªèƒ½å›ç­”ä¸€ä¸ªå•è¯ï¼š
- "FINISH" = èµ„æ–™å……è¶³ï¼Œå¯ä»¥å†™æ–‡æ¡£
- "CONTINUE" = èµ„æ–™ä¸è¶³ï¼Œéœ€è¦æ›´å¤šç ”ç©¶

è¯·ç›´æ¥è¾“å‡ºï¼šFINISH æˆ– CONTINUE
"""

    try:
        # 3. è°ƒç”¨ LLM å®¢æˆ·ç«¯
        # æ ¹æ®æ¨¡å‹ç±»å‹è®¾ç½®åˆé€‚çš„max_tokens
        max_tokens = 10
        if hasattr(llm_client, 'reasoning') and llm_client.reasoning:
            # å¦‚æœæ”¯æŒæ¨ç†æ¨¡å¼ï¼Œä½¿ç”¨æ›´å¤§çš„tokené™åˆ¶
            max_tokens = 1000
        if hasattr(llm_client,
                   'model_name') and 'gemini' in llm_client.model_name.lower():
            max_tokens = 5000
        response = llm_client.invoke(prompt,
                                     temperature=0,
                                     max_tokens=max_tokens)

        # 4. è§£æå“åº” - ä½¿ç”¨ strip() å’Œ upper() å¤„ç†ç©ºç™½å’Œå¤§å°å†™
        decision = response.strip().upper()

        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        print(f"ğŸ” LLMåŸå§‹å“åº”: '{response}'")
        print(f"ğŸ” å¤„ç†åå†³ç­–: '{decision}'")

        # 5. æ ¹æ®å“åº”å†³å®šè·¯ç”±
        if "FINISH" in decision:
            print("âœ… å†³ç­–: FINISH -> continue_to_writer")
            return "continue_to_writer"
        else:
            # å¦‚æœåŒ…å« "CONTINUE" æˆ–å…¶ä»–ä»»ä½•å†…å®¹ï¼Œéƒ½éœ€è¦é‡æ–°ç ”ç©¶
            print("âœ… å†³ç­–: CONTINUE/å…¶ä»– -> rerun_researcher")
            return "rerun_researcher"

    except Exception as e:
        # å¦‚æœ LLM è°ƒç”¨å¤±è´¥ï¼Œé»˜è®¤ç»§ç»­ç ”ç©¶ä»¥ç¡®ä¿å®‰å…¨
        print(f"Supervisor router error: {str(e)}")
        return "rerun_researcher"
