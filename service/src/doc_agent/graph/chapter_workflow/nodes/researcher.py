"""
ç ”ç©¶èŠ‚ç‚¹æ¨¡å—

è´Ÿè´£æ‰§è¡Œæœç´¢å’Œæ”¶é›†ä¿¡æ¯
"""

import json
from typing import Any

from loguru import logger

from doc_agent.core.config import settings
from doc_agent.graph.common import merge_sources_with_deduplication
from doc_agent.graph.common import parse_es_search_results as _parse_es_search_results
from doc_agent.graph.common import parse_web_search_results as _parse_web_search_results
from doc_agent.graph.state import ResearchState
from doc_agent.llm_clients.providers import EmbeddingClient
from doc_agent.tools.es_search import ESSearchTool
from doc_agent.tools.reranker import RerankerTool
from doc_agent.tools.web_search import WebSearchTool
from doc_agent.utils.search_utils import search_and_rerank


def researcher_node(state: ResearchState,
                    web_search_tool: WebSearchTool) -> dict[str, Any]:
    """å·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨ async_researcher_node"""
    raise NotImplementedError("è¯·ä½¿ç”¨ async_researcher_node")


async def async_researcher_node(
        state: ResearchState,
        web_search_tool: WebSearchTool,
        es_search_tool: ESSearchTool,
        reranker_tool: RerankerTool = None) -> dict[str, Any]:
    """
    å¼‚æ­¥èŠ‚ç‚¹2: æ‰§è¡Œæœç´¢ç ”ç©¶
    ä»çŠ¶æ€ä¸­è·å– search_queriesï¼Œä½¿ç”¨æœç´¢å·¥å…·æ”¶é›†ç›¸å…³ä¿¡æ¯
    ä¼˜å…ˆä½¿ç”¨å‘é‡æ£€ç´¢ï¼Œå¦‚æœå¤±è´¥åˆ™å›é€€åˆ°æ–‡æœ¬æœç´¢
    ä½¿ç”¨é‡æ’åºå·¥å…·å¯¹æœç´¢ç»“æœè¿›è¡Œä¼˜åŒ–

    Args:
        state: ç ”ç©¶çŠ¶æ€ï¼ŒåŒ…å« search_queries
        web_search_tool: ç½‘ç»œæœç´¢å·¥å…·
        es_search_tool: Elasticsearchæœç´¢å·¥å…·
        reranker_tool: é‡æ’åºå·¥å…·ï¼ˆå¯é€‰ï¼‰

    Returns:
        dict: åŒ…å« gathered_sources çš„å­—å…¸ï¼ŒåŒ…å« Source å¯¹è±¡åˆ—è¡¨
    """
    logger.info("ğŸ” ResearcherèŠ‚ç‚¹æ¥æ”¶åˆ°çš„å®Œæ•´çŠ¶æ€:")
    logger.debug(f"  - topic: {state.get('topic', 'N/A')}")
    logger.debug(
        f"  - current_chapter_index: {state.get('current_chapter_index', 'N/A')}"
    )
    logger.debug(
        f"  - research_plan: {state.get('research_plan', 'N/A')[:100]}...")
    logger.debug(f"  - search_queries: {state.get('search_queries', [])}")
    logger.debug(
        f"  - search_queriesç±»å‹: {type(state.get('search_queries', []))}")
    logger.debug(
        f"  - search_queriesé•¿åº¦: {len(state.get('search_queries', []))}")
    logger.debug(
        f"  - gathered_data: {state.get('gathered_data', 'N/A')[:50]}...")

    search_queries = state.get("search_queries", [])

    if not search_queries:
        logger.warning("âŒ æ²¡æœ‰æœç´¢æŸ¥è¯¢ï¼Œè¿”å›é»˜è®¤æ¶ˆæ¯")
        return {"gathered_sources": []}

    # è·å–å¤æ‚åº¦é…ç½®
    complexity_config = settings.get_complexity_config()
    logger.info(f"ğŸ”§ ä½¿ç”¨å¤æ‚åº¦çº§åˆ«: {complexity_config['level']}")

    all_sources = []  # å­˜å‚¨æ‰€æœ‰ Source å¯¹è±¡
    source_id_counter = 1  # æºIDè®¡æ•°å™¨

    # è·å–ç°æœ‰çš„ä¿¡æºåˆ—è¡¨ï¼ˆä»çŠ¶æ€ä¸­è·å–ï¼‰
    existing_sources = state.get("gathered_sources", [])
    if existing_sources:
        logger.info(f"ğŸ“š å‘ç°ç°æœ‰ä¿¡æº {len(existing_sources)} ä¸ªï¼Œå°†è¿›è¡Œå»é‡å¤„ç†")
        # æ›´æ–°æºIDè®¡æ•°å™¨
        source_id_counter = max([source.id for source in existing_sources],
                                default=0) + 1

    # è·å–embeddingé…ç½®
    embedding_config = settings.supported_models.get("gte_qwen")
    embedding_client = None
    if embedding_config:
        try:
            embedding_client = EmbeddingClient(
                base_url=embedding_config.url,
                api_key=embedding_config.api_key)
            logger.info("âœ… Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸  Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            embedding_client = None
    else:
        logger.warning("âŒ æœªæ‰¾åˆ° embedding é…ç½®ï¼Œå°†ä½¿ç”¨æ–‡æœ¬æœç´¢")

    # æ ¹æ®å¤æ‚åº¦é…ç½®è·å–æ–‡æ¡£é…ç½®å‚æ•°
    initial_top_k = complexity_config.get('vector_recall_size', 10)
    final_top_k = complexity_config.get('rerank_size', 5)

    # åº”ç”¨åŸºäºå¤æ‚åº¦çš„æŸ¥è¯¢æ•°é‡é™åˆ¶
    max_queries = complexity_config.get(
        'chapter_search_queries', complexity_config.get('max_queries', 5))
    if len(search_queries) > max_queries:
        logger.info(f"ğŸ”§ é™åˆ¶æœç´¢æŸ¥è¯¢æ•°é‡ä» {len(search_queries)} åˆ° {max_queries}")
        search_queries = search_queries[:max_queries]

    # æ‰§è¡Œæœç´¢
    for i, query in enumerate(search_queries, 1):
        logger.info(f"æ‰§è¡Œæœç´¢æŸ¥è¯¢ {i}/{len(search_queries)}: {query}")

        # ============================
        # ESæœç´¢
        # ============================
        es_raw_results = None
        es_str_results = ""
        try:
            if embedding_client:
                # å°è¯•å‘é‡æ£€ç´¢
                try:
                    embedding_response = embedding_client.invoke(query)
                    try:
                        embedding_data = json.loads(embedding_response)
                        if isinstance(embedding_data, list):
                            if len(embedding_data) > 0 and isinstance(
                                    embedding_data[0], list):
                                query_vector = embedding_data[0]
                            else:
                                query_vector = embedding_data
                        elif isinstance(embedding_data,
                                        dict) and 'data' in embedding_data:
                            query_vector = embedding_data['data']
                        else:
                            logger.warning(
                                f"âš ï¸  æ— æ³•è§£æembeddingå“åº”æ ¼å¼: {type(embedding_data)}"
                            )
                            query_vector = None
                    except json.JSONDecodeError:
                        logger.warning("âš ï¸  JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬æœç´¢")
                        query_vector = None

                    if query_vector and len(query_vector) == 1536:
                        logger.debug(
                            f"âœ… å‘é‡ç»´åº¦: {len(query_vector)}ï¼Œå‰5: {query_vector[:5]}"
                        )
                        # ä½¿ç”¨æ–°çš„æœç´¢å’Œé‡æ’åºåŠŸèƒ½
                        search_query = query if query.strip() else "ç›¸å…³æ–‡æ¡£"

                        _, reranked_results, formatted_es_results = await search_and_rerank(
                            es_search_tool=es_search_tool,
                            query=search_query,
                            query_vector=query_vector,
                            reranker_tool=reranker_tool,
                            initial_top_k=initial_top_k,
                            final_top_k=final_top_k,
                            config={
                                'min_score':
                                complexity_config.get('min_score', 0.3)
                            })
                        es_str_results = formatted_es_results
                        logger.info(
                            f"âœ… å‘é‡æ£€ç´¢+é‡æ’åºæ‰§è¡ŒæˆåŠŸï¼Œç»“æœé•¿åº¦: {len(formatted_es_results)}"
                        )
                    else:
                        logger.warning("âŒ å‘é‡ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬æœç´¢")
                        # å›é€€åˆ°æ–‡æœ¬æœç´¢
                        _, reranked_results, formatted_es_results = await search_and_rerank(
                            es_search_tool=es_search_tool,
                            query=query,
                            query_vector=None,
                            reranker_tool=reranker_tool,
                            initial_top_k=initial_top_k,
                            final_top_k=final_top_k,
                            config={
                                'min_score':
                                complexity_config.get('min_score', 0.3)
                            })
                        es_str_results = formatted_es_results
                        logger.info(
                            f"âœ… æ–‡æœ¬æœç´¢+é‡æ’åºæ‰§è¡ŒæˆåŠŸï¼Œç»“æœé•¿åº¦: {len(formatted_es_results)}"
                        )
                except Exception as e:
                    logger.error(f"âŒ å‘é‡æ£€ç´¢å¼‚å¸¸: {str(e)}ï¼Œä½¿ç”¨æ–‡æœ¬æœç´¢")
                    # å›é€€åˆ°æ–‡æœ¬æœç´¢
                    _, reranked_results, formatted_es_results = await search_and_rerank(
                        es_search_tool=es_search_tool,
                        query=query,
                        query_vector=None,
                        reranker_tool=reranker_tool,
                        initial_top_k=initial_top_k,
                        final_top_k=final_top_k,
                        config={
                            'min_score':
                            complexity_config.get('min_score', 0.3)
                        })
                    es_str_results = formatted_es_results
                    logger.info(
                        f"âœ… æ–‡æœ¬æœç´¢+é‡æ’åºæ‰§è¡ŒæˆåŠŸï¼Œç»“æœé•¿åº¦: {len(formatted_es_results)}")
            else:
                # æ²¡æœ‰embeddingå®¢æˆ·ç«¯ï¼Œç›´æ¥ä½¿ç”¨æ–‡æœ¬æœç´¢
                logger.info("ğŸ“ ä½¿ç”¨æ–‡æœ¬æœç´¢")

                _, reranked_results, formatted_es_results = await search_and_rerank(
                    es_search_tool=es_search_tool,
                    query=query,
                    query_vector=None,
                    reranker_tool=reranker_tool,
                    initial_top_k=initial_top_k,
                    final_top_k=final_top_k,
                    config={
                        'min_score': complexity_config.get('min_score', 0.3)
                    })
                es_str_results = formatted_es_results
                logger.info(
                    f"âœ… æ–‡æœ¬æœç´¢+é‡æ’åºæ‰§è¡ŒæˆåŠŸï¼Œç»“æœé•¿åº¦: {len(formatted_es_results)}")

        except Exception as e:
            logger.error(f"âŒ ESæœç´¢å¤±è´¥: {str(e)}")
            es_str_results = f"ESæœç´¢å¤±è´¥: {str(e)}"

        # ============================
        # ç½‘ç»œæœç´¢
        # ============================
        web_raw_results = None
        web_str_results = ""
        try:
            # ä½¿ç”¨å¼‚æ­¥æœç´¢æ–¹æ³•
            web_raw_results = await web_search_tool.search_async(query)
            if "æ¨¡æ‹Ÿ" in web_raw_results or "mock" in web_raw_results.lower():
                logger.info(f"ç½‘ç»œæœç´¢è¿”å›æ¨¡æ‹Ÿç»“æœï¼Œè·³è¿‡: {query}")
                web_str_results = ""
            if "æœç´¢å¤±è´¥" in web_str_results:
                logger.error(f"ç½‘ç»œæœç´¢å¤±è´¥: {web_str_results}")
                web_str_results = ""
        except Exception as e:
            logger.error(f"ç½‘ç»œæœç´¢å¤±è´¥: {str(e)}")
            web_str_results = ""

        # å¤„ç†ç½‘ç»œæœç´¢ç»“æœ
        if web_str_results and web_str_results.strip():
            try:
                # è§£æç½‘ç»œæœç´¢ç»“æœï¼Œåˆ›å»º Source å¯¹è±¡
                web_sources = _parse_web_search_results(
                    web_str_results, query, source_id_counter)

                # ä½¿ç”¨æ–°çš„å»é‡é€»è¾‘
                deduplicated_web_sources = merge_sources_with_deduplication(
                    web_sources, existing_sources)
                new_web_sources = [
                    s for s in deduplicated_web_sources
                    if s not in existing_sources
                ]

                all_sources.extend(new_web_sources)
                source_id_counter += len(new_web_sources)
                logger.info(
                    f"âœ… ä»ç½‘ç»œæœç´¢ä¸­æå–åˆ° {len(web_sources)} ä¸ªæºï¼Œå»é‡åæ–°å¢ {len(new_web_sources)} ä¸ª"
                )
            except Exception as e:
                logger.error(f"âŒ è§£æç½‘ç»œæœç´¢ç»“æœå¤±è´¥: {str(e)}")

        # å¤„ç†ESæœç´¢ç»“æœ
        if es_str_results and es_str_results.strip():
            try:
                # è§£æESæœç´¢ç»“æœï¼Œåˆ›å»º Source å¯¹è±¡
                es_sources = _parse_es_search_results(es_str_results, query,
                                                      source_id_counter)

                # ä½¿ç”¨æ–°çš„å»é‡é€»è¾‘
                deduplicated_es_sources = merge_sources_with_deduplication(
                    es_sources, existing_sources)
                new_es_sources = [
                    s for s in deduplicated_es_sources
                    if s not in existing_sources
                ]

                all_sources.extend(new_es_sources)
                source_id_counter += len(new_es_sources)
                logger.info(
                    f"âœ… ä»ESæœç´¢ä¸­æå–åˆ° {len(es_sources)} ä¸ªæºï¼Œå»é‡åæ–°å¢ {len(new_es_sources)} ä¸ª"
                )
            except Exception as e:
                logger.error(f"âŒ è§£æESæœç´¢ç»“æœå¤±è´¥: {str(e)}")

        # æ ¹æ®å¤æ‚åº¦é…ç½®å†³å®šæ˜¯å¦æˆªæ–­ç»“æœ
        truncate_length = complexity_config.get('data_truncate_length', -1)
        if truncate_length > 0:
            if es_str_results and len(es_str_results) > truncate_length:
                es_str_results = es_str_results[:truncate_length] + "\n... (ç»“æœå·²æˆªæ–­)"
            if web_str_results and len(web_str_results) > truncate_length:
                web_str_results = web_str_results[:truncate_length] + "\n... (ç»“æœå·²æˆªæ–­)"

    # åˆå¹¶æ‰€æœ‰ä¿¡æºï¼ˆåŒ…æ‹¬ç°æœ‰çš„å’Œæ–°çš„ï¼‰
    final_sources = merge_sources_with_deduplication(all_sources,
                                                     existing_sources)

    # è¿”å›ç»“æ„åŒ–çš„æºåˆ—è¡¨
    logger.info(
        f"âœ… æ€»å…±æ”¶é›†åˆ° {len(final_sources)} ä¸ªä¿¡æ¯æºï¼ˆåŸæœ‰ {len(existing_sources)} ä¸ªï¼Œæ–°å¢ {len(all_sources)} ä¸ªï¼‰"
    )
    for i, source in enumerate(final_sources[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ªæºä½œä¸ºé¢„è§ˆ
        logger.debug(
            f"  {i}. [{source.id}] {source.title} ({source.source_type})")

    return {"gathered_sources": final_sources}
