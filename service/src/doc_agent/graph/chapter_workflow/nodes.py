# service/src/doc_agent/graph/chapter_workflow/nodes.py
import pprint
import sys
import re
from pathlib import Path
from typing import Any

from loguru import logger

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_file = Path(__file__)
service_dir = None
for parent in current_file.parents:
    if parent.name == 'service':
        service_dir = parent
        break

if service_dir and str(service_dir) not in sys.path:
    sys.path.insert(0, str(service_dir))

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
if service_dir:
    src_dir = service_dir / "src"
    if str(src_dir) not in sys.path:
        sys.path.insert(0, str(src_dir))

# å¯¼å…¥é¡¹ç›®å†…éƒ¨æ¨¡å—
from core.config import settings

from ...common import parse_planner_response
from ...common.prompt_selector import PromptSelector
from ...llm_clients.base import LLMClient
from ...llm_clients.providers import EmbeddingClient
from ...tools.es_search import ESSearchTool
from ...tools.reranker import RerankerTool
from ...tools.web_search import WebSearchTool
from ...utils.search_utils import search_and_rerank
from ..state import ResearchState
from ...schemas import Source


def planner_node(state: ResearchState,
                 llm_client: LLMClient,
                 prompt_selector: PromptSelector,
                 genre: str = "default") -> dict[str, Any]:
    """
    èŠ‚ç‚¹1: è§„åˆ’ç ”ç©¶æ­¥éª¤
    ä»çŠ¶æ€ä¸­è·å– topic å’Œå½“å‰ç« èŠ‚ä¿¡æ¯ï¼Œåˆ›å»º prompt è°ƒç”¨ LLM ç”Ÿæˆç ”ç©¶è®¡åˆ’å’Œæœç´¢æŸ¥è¯¢
    Args:
        state: ç ”ç©¶çŠ¶æ€ï¼ŒåŒ…å« topic å’Œå½“å‰ç« èŠ‚ä¿¡æ¯
        llm_client: LLMå®¢æˆ·ç«¯å®ä¾‹
        prompt_selector: PromptSelectorå®ä¾‹ï¼Œç”¨äºè·å–promptæ¨¡æ¿
        genre: genreç±»å‹ï¼Œé»˜è®¤ä¸º"default"
    Returns:
        dict: åŒ…å« research_plan å’Œ search_queries çš„å­—å…¸
    """
    topic = state.get("topic", "")
    current_chapter_index = state.get("current_chapter_index", 0)
    chapters_to_process = state.get("chapters_to_process", [])

    if not topic:
        raise ValueError("Topic is required in state")

    # è·å–å½“å‰ç« èŠ‚ä¿¡æ¯
    current_chapter = None
    if current_chapter_index < len(chapters_to_process):
        current_chapter = chapters_to_process[current_chapter_index]

    chapter_title = current_chapter.get("chapter_title",
                                        "") if current_chapter else ""
    chapter_description = current_chapter.get("description",
                                              "") if current_chapter else ""

    logger.info(f"ğŸ“‹ è§„åˆ’ç« èŠ‚ç ”ç©¶: {chapter_title}")
    logger.info(f"ğŸ“ ç« èŠ‚æè¿°: {chapter_description}")

    # è·å–ä»»åŠ¡è§„åˆ’å™¨é…ç½®
    task_planner_config = settings.get_agent_component_config("task_planner")
    if not task_planner_config:
        raise ValueError("Task planner configuration not found")

    # ä½¿ç”¨ PromptSelector è·å– prompt æ¨¡æ¿
    try:
        prompt_template = prompt_selector.get_prompt("chapter_workflow",
                                                     "planner", genre)
        logger.debug(f"âœ… æˆåŠŸè·å– planner prompt æ¨¡æ¿ï¼Œgenre: {genre}")
    except Exception as e:
        logger.error(f"âŒ è·å– planner prompt æ¨¡æ¿å¤±è´¥: {e}")
        # ä½¿ç”¨ prompts/planner.py ä¸­çš„å¤‡ç”¨æ¨¡æ¿
        try:
            from ...prompts.planner import PROMPTS
            prompt_template = PROMPTS.get("v1_fallback", PROMPTS["v1_default"])
            logger.debug("âœ… æˆåŠŸè·å– planner å¤‡ç”¨æ¨¡æ¿")
        except Exception as e2:
            logger.error(f"âŒ è·å– planner å¤‡ç”¨æ¨¡æ¿ä¹Ÿå¤±è´¥: {e2}")
            # æœ€åçš„å¤‡ç”¨æ–¹æ¡ˆ
            prompt_template = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶è§„åˆ’ä¸“å®¶ã€‚è¯·ä¸ºä»¥ä¸‹ç« èŠ‚åˆ¶å®šè¯¦ç»†çš„ç ”ç©¶è®¡åˆ’å’Œæœç´¢ç­–ç•¥ã€‚

**æ–‡æ¡£ä¸»é¢˜:** {topic}

**å½“å‰ç« èŠ‚ä¿¡æ¯:**
- ç« èŠ‚æ ‡é¢˜: {chapter_title}
- ç« èŠ‚æè¿°: {chapter_description}

**ä»»åŠ¡è¦æ±‚:**
1. åˆ†æç« èŠ‚ä¸»é¢˜ï¼Œç¡®å®šç ”ç©¶é‡ç‚¹å’Œæ–¹å‘
2. åˆ¶å®šè¯¦ç»†çš„ç ”ç©¶è®¡åˆ’ï¼ŒåŒ…æ‹¬ç ”ç©¶æ­¥éª¤å’Œæ–¹æ³•
3. ç”Ÿæˆ5-8ä¸ªé«˜è´¨é‡çš„æœç´¢æŸ¥è¯¢ï¼Œç”¨äºæ”¶é›†ç›¸å…³ä¿¡æ¯
4. ç¡®ä¿æœç´¢æŸ¥è¯¢å…·æœ‰é’ˆå¯¹æ€§å’Œå…¨é¢æ€§

**è¾“å‡ºæ ¼å¼:**
è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- research_plan: è¯¦ç»†çš„ç ”ç©¶è®¡åˆ’
- search_queries: æœç´¢æŸ¥è¯¢åˆ—è¡¨ï¼ˆæ•°ç»„ï¼‰

è¯·ç«‹å³å¼€å§‹åˆ¶å®šç ”ç©¶è®¡åˆ’ã€‚
"""

    # åˆ›å»ºç ”ç©¶è®¡åˆ’ç”Ÿæˆçš„ promptï¼Œè¦æ±‚ JSON æ ¼å¼å“åº”
    prompt = prompt_template.format(topic=topic,
                                    chapter_title=chapter_title,
                                    chapter_description=chapter_description)

    logger.debug(f"Invoking LLM with prompt:\n{pprint.pformat(prompt)}")

    try:
        # è°ƒç”¨ LLM ç”Ÿæˆç ”ç©¶è®¡åˆ’
        response = llm_client.invoke(
            prompt,
            temperature=task_planner_config.temperature,
            max_tokens=task_planner_config.max_tokens,
            **task_planner_config.extra_params)

        logger.debug(f"ğŸ” LLMåŸå§‹å“åº”: {repr(response)}")
        logger.debug(f"ğŸ“ å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
        logger.info(f"ğŸ” LLMå“åº”å†…å®¹:\n{response}")

        # è§£æ JSON å“åº”
        research_plan, search_queries = parse_planner_response(response)

        # åº”ç”¨æœç´¢è½®æ•°é™åˆ¶
        max_search_rounds = getattr(settings.search_config,
                                    'max_search_rounds', 5)
        logger.info(f"ğŸ“Š planner_node å½“å‰æœç´¢è½®æ•°é…ç½®: {max_search_rounds}")

        if len(search_queries) > max_search_rounds:
            logger.info(
                f"ğŸ“Š é™åˆ¶æœç´¢æŸ¥è¯¢æ•°é‡: {len(search_queries)} -> {max_search_rounds}")
            search_queries = search_queries[:max_search_rounds]

        logger.info(f"âœ… ç”Ÿæˆç ”ç©¶è®¡åˆ’: {len(search_queries)} ä¸ªæœç´¢æŸ¥è¯¢")
        for i, query in enumerate(search_queries, 1):
            logger.debug(f"  {i}. {query}")

        # è¿”å›å®Œæ•´çš„çŠ¶æ€æ›´æ–°
        result = {
            "research_plan": research_plan,
            "search_queries": search_queries
        }
        logger.debug(f"ğŸ“¤ PlannerèŠ‚ç‚¹è¿”å›ç»“æœ: {pprint.pformat(result)}")
        return result

    except Exception as e:
        # å¦‚æœ LLM è°ƒç”¨å¤±è´¥ï¼Œè¿”å›é»˜è®¤è®¡åˆ’
        logger.error(f"Planner node error: {str(e)}")
        default_queries = [
            f"{topic} {chapter_title} æ¦‚è¿°", f"{topic} {chapter_title} ä¸»è¦å†…å®¹",
            f"{topic} {chapter_title} å…³é”®è¦ç‚¹", f"{topic} {chapter_title} æœ€æ–°å‘å±•",
            f"{topic} {chapter_title} é‡è¦æ€§"
        ]
        logger.warning(f"âš ï¸  ä½¿ç”¨é»˜è®¤æœç´¢æŸ¥è¯¢: {len(default_queries)} ä¸ª")
        for i, query in enumerate(default_queries, 1):
            logger.debug(f"  {i}. {query}")

        result = {
            "research_plan": f"ç ”ç©¶è®¡åˆ’ï¼šå¯¹ç« èŠ‚ {chapter_title} è¿›è¡Œæ·±å…¥ç ”ç©¶ï¼Œæ”¶é›†ç›¸å…³ä¿¡æ¯å¹¶æ•´ç†æˆæ–‡æ¡£ã€‚",
            "search_queries": default_queries
        }
        logger.debug(f"ğŸ“¤ PlannerèŠ‚ç‚¹è¿”å›é»˜è®¤ç»“æœ: {pprint.pformat(result)}")
        return result


def researcher_node(state: ResearchState,
                    web_search_tool: WebSearchTool) -> dict[str, Any]:
    """å·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨ async_researcher_node"""
    raise NotImplementedError("è¯·ä½¿ç”¨ async_researcher_node")


async def async_researcher_node(
        state: ResearchState,
        web_search_tool: WebSearchTool,
        es_search_tool: ESSearchTool,
        reranker_tool: RerankerTool = None) -> dict[str, Any]:
    """
    å¼‚æ­¥èŠ‚ç‚¹2: æ‰§è¡Œæœç´¢ç ”ç©¶
    ä»çŠ¶æ€ä¸­è·å– search_queriesï¼Œä½¿ç”¨æœç´¢å·¥å…·æ”¶é›†ç›¸å…³ä¿¡æ¯
    ä¼˜å…ˆä½¿ç”¨å‘é‡æ£€ç´¢ï¼Œå¦‚æœå¤±è´¥åˆ™å›é€€åˆ°æ–‡æœ¬æœç´¢
    ä½¿ç”¨é‡æ’åºå·¥å…·å¯¹æœç´¢ç»“æœè¿›è¡Œä¼˜åŒ–
    Args:
        state: ç ”ç©¶çŠ¶æ€ï¼ŒåŒ…å« search_queries
        web_search_tool: ç½‘ç»œæœç´¢å·¥å…·
        es_search_tool: Elasticsearchæœç´¢å·¥å…·
        reranker_tool: é‡æ’åºå·¥å…·ï¼ˆå¯é€‰ï¼‰
    Returns:
        dict: åŒ…å« gathered_sources çš„å­—å…¸ï¼ŒåŒ…å« Source å¯¹è±¡åˆ—è¡¨
    """
    logger.info("ğŸ” ResearcherèŠ‚ç‚¹æ¥æ”¶åˆ°çš„å®Œæ•´çŠ¶æ€:")
    logger.debug(f"  - topic: {state.get('topic', 'N/A')}")
    logger.debug(
        f"  - current_chapter_index: {state.get('current_chapter_index', 'N/A')}"
    )
    logger.debug(
        f"  - research_plan: {state.get('research_plan', 'N/A')[:100]}...")
    logger.debug(f"  - search_queries: {state.get('search_queries', [])}")
    logger.debug(
        f"  - search_queriesç±»å‹: {type(state.get('search_queries', []))}")
    logger.debug(
        f"  - search_queriesé•¿åº¦: {len(state.get('search_queries', []))}")
    logger.debug(
        f"  - gathered_data: {state.get('gathered_data', 'N/A')[:50]}...")

    search_queries = state.get("search_queries", [])

    if not search_queries:
        logger.warning("âŒ æ²¡æœ‰æœç´¢æŸ¥è¯¢ï¼Œè¿”å›é»˜è®¤æ¶ˆæ¯")
        return {"gathered_sources": []}

    all_sources = []  # å­˜å‚¨æ‰€æœ‰ Source å¯¹è±¡
    source_id_counter = 1  # æºIDè®¡æ•°å™¨

    # è·å–embeddingé…ç½®
    embedding_config = settings.supported_models.get("gte_qwen")
    embedding_client = None
    if embedding_config:
        try:
            embedding_client = EmbeddingClient(
                base_url=embedding_config.url,
                api_key=embedding_config.api_key)
            logger.info("âœ… Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸  Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            embedding_client = None
    else:
        logger.warning("âŒ æœªæ‰¾åˆ° embedding é…ç½®ï¼Œå°†ä½¿ç”¨æ–‡æœ¬æœç´¢")

    # è·å–é…ç½®å‚æ•° - ä½¿ç”¨å¿«é€Ÿæ¨¡å¼ä»¥åŠ å¿«æµ‹è¯•
    doc_config = settings.get_document_config(fast_mode=True)
    initial_top_k = doc_config.get('vector_recall_size', 10)
    final_top_k = doc_config.get('rerank_size', 5)

    # é™åˆ¶æœç´¢æŸ¥è¯¢æ•°é‡ä»¥åŠ å¿«æµ‹è¯•
    max_queries = settings.search_config.max_queries
    if len(search_queries) > max_queries:
        logger.info(f"ğŸ”§ é™åˆ¶æœç´¢æŸ¥è¯¢æ•°é‡ä» {len(search_queries)} åˆ° {max_queries}")
        search_queries = search_queries[:max_queries]

    # ä½¿ç”¨ä¼ å…¥çš„ESå·¥å…·ï¼Œä¸å†å†…éƒ¨åˆ›å»º
    for i, query in enumerate(search_queries, 1):
        logger.info(f"æ‰§è¡Œæœç´¢æŸ¥è¯¢ {i}/{len(search_queries)}: {query}")

        # ç½‘ç»œæœç´¢
        web_results = ""
        try:
            # ä½¿ç”¨å¼‚æ­¥æœç´¢æ–¹æ³•
            web_results = await web_search_tool.search_async(query)
            if "æ¨¡æ‹Ÿ" in web_results or "mock" in web_results.lower():
                logger.info(f"ç½‘ç»œæœç´¢è¿”å›æ¨¡æ‹Ÿç»“æœï¼Œè·³è¿‡: {query}")
                web_results = ""
        except Exception as e:
            logger.error(f"ç½‘ç»œæœç´¢å¤±è´¥: {str(e)}")
            web_results = ""

        # ESæœç´¢ - ä½¿ç”¨æ–°çš„æœç´¢å’Œé‡æ’åºåŠŸèƒ½
        es_results = ""
        try:
            if embedding_client:
                # å°è¯•å‘é‡æ£€ç´¢
                try:
                    embedding_response = embedding_client.invoke(query)
                    import json
                    try:
                        embedding_data = json.loads(embedding_response)
                        if isinstance(embedding_data, list):
                            if len(embedding_data) > 0 and isinstance(
                                    embedding_data[0], list):
                                query_vector = embedding_data[0]
                            else:
                                query_vector = embedding_data
                        elif isinstance(embedding_data,
                                        dict) and 'data' in embedding_data:
                            query_vector = embedding_data['data']
                        else:
                            logger.warning(
                                f"âš ï¸  æ— æ³•è§£æembeddingå“åº”æ ¼å¼: {type(embedding_data)}"
                            )
                            query_vector = None
                    except json.JSONDecodeError:
                        logger.warning("âš ï¸  JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬æœç´¢")
                        query_vector = None

                    if query_vector and len(query_vector) == 1536:
                        logger.debug(
                            f"âœ… å‘é‡ç»´åº¦: {len(query_vector)}ï¼Œå‰5: {query_vector[:5]}"
                        )
                        # ä½¿ç”¨æ–°çš„æœç´¢å’Œé‡æ’åºåŠŸèƒ½
                        search_query = query if query.strip() else "ç›¸å…³æ–‡æ¡£"

                        _, reranked_results, formatted_es_results = await search_and_rerank(
                            es_search_tool=es_search_tool,
                            query=search_query,
                            query_vector=query_vector,
                            reranker_tool=reranker_tool,
                            initial_top_k=initial_top_k,
                            final_top_k=final_top_k,
                            config=doc_config)
                        es_results = formatted_es_results
                        logger.info(
                            f"âœ… å‘é‡æ£€ç´¢+é‡æ’åºæ‰§è¡ŒæˆåŠŸï¼Œç»“æœé•¿åº¦: {len(formatted_es_results)}"
                        )
                    else:
                        logger.warning("âŒ å‘é‡ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬æœç´¢")
                        # å›é€€åˆ°æ–‡æœ¬æœç´¢
                        _, reranked_results, formatted_es_results = await search_and_rerank(
                            es_search_tool=es_search_tool,
                            query=query,
                            query_vector=None,
                            reranker_tool=reranker_tool,
                            initial_top_k=initial_top_k,
                            final_top_k=final_top_k,
                            config=doc_config)
                        es_results = formatted_es_results
                        logger.info(
                            f"âœ… æ–‡æœ¬æœç´¢+é‡æ’åºæ‰§è¡ŒæˆåŠŸï¼Œç»“æœé•¿åº¦: {len(formatted_es_results)}"
                        )
                except Exception as e:
                    logger.error(f"âŒ å‘é‡æ£€ç´¢å¼‚å¸¸: {str(e)}ï¼Œä½¿ç”¨æ–‡æœ¬æœç´¢")
                    # å›é€€åˆ°æ–‡æœ¬æœç´¢
                    _, reranked_results, formatted_es_results = await search_and_rerank(
                        es_search_tool=es_search_tool,
                        query=query,
                        query_vector=None,
                        reranker_tool=reranker_tool,
                        initial_top_k=initial_top_k,
                        final_top_k=final_top_k,
                        config=doc_config)
                    es_results = formatted_es_results
                    logger.info(
                        f"âœ… æ–‡æœ¬æœç´¢+é‡æ’åºæ‰§è¡ŒæˆåŠŸï¼Œç»“æœé•¿åº¦: {len(formatted_es_results)}")
            else:
                # æ²¡æœ‰embeddingå®¢æˆ·ç«¯ï¼Œç›´æ¥ä½¿ç”¨æ–‡æœ¬æœç´¢
                logger.info("ğŸ“ ä½¿ç”¨æ–‡æœ¬æœç´¢")

                _, reranked_results, formatted_es_results = await search_and_rerank(
                    es_search_tool=es_search_tool,
                    query=query,
                    query_vector=None,
                    reranker_tool=reranker_tool,
                    initial_top_k=initial_top_k,
                    final_top_k=final_top_k,
                    config=doc_config)
                es_results = formatted_es_results
                logger.info(
                    f"âœ… æ–‡æœ¬æœç´¢+é‡æ’åºæ‰§è¡ŒæˆåŠŸï¼Œç»“æœé•¿åº¦: {len(formatted_es_results)}")

        except Exception as e:
            logger.error(f"âŒ ESæœç´¢å¤±è´¥: {str(e)}")
            es_results = f"ESæœç´¢å¤±è´¥: {str(e)}"

        # å¤„ç†ç½‘ç»œæœç´¢ç»“æœ
        if web_results and web_results.strip():
            try:
                # è§£æç½‘ç»œæœç´¢ç»“æœï¼Œåˆ›å»º Source å¯¹è±¡
                web_sources = _parse_web_search_results(
                    web_results, query, source_id_counter)
                all_sources.extend(web_sources)
                source_id_counter += len(web_sources)
                logger.info(f"âœ… ä»ç½‘ç»œæœç´¢ä¸­æå–åˆ° {len(web_sources)} ä¸ªæº")
            except Exception as e:
                logger.error(f"âŒ è§£æç½‘ç»œæœç´¢ç»“æœå¤±è´¥: {str(e)}")

        # å¤„ç†ESæœç´¢ç»“æœ
        if es_results and es_results.strip():
            try:
                # è§£æESæœç´¢ç»“æœï¼Œåˆ›å»º Source å¯¹è±¡
                es_sources = _parse_es_search_results(es_results, query,
                                                      source_id_counter)
                all_sources.extend(es_sources)
                source_id_counter += len(es_sources)
                logger.info(f"âœ… ä»ESæœç´¢ä¸­æå–åˆ° {len(es_sources)} ä¸ªæº")
            except Exception as e:
                logger.error(f"âŒ è§£æESæœç´¢ç»“æœå¤±è´¥: {str(e)}")

    # è¿”å›ç»“æ„åŒ–çš„æºåˆ—è¡¨
    logger.info(f"âœ… æ€»å…±æ”¶é›†åˆ° {len(all_sources)} ä¸ªä¿¡æ¯æº")
    for i, source in enumerate(all_sources[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ªæºä½œä¸ºé¢„è§ˆ
        logger.debug(
            f"  {i}. [{source.id}] {source.title} ({source.source_type})")

    return {"gathered_sources": all_sources}


def writer_node(state: ResearchState,
                llm_client: LLMClient,
                prompt_selector: PromptSelector,
                genre: str = "default",
                prompt_version: str = "v3_context_aware") -> dict[str, Any]:
    """
    ç« èŠ‚å†™ä½œèŠ‚ç‚¹
    åŸºäºå½“å‰ç« èŠ‚çš„ç ”ç©¶æ•°æ®å’Œå·²å®Œæˆç« èŠ‚çš„ä¸Šä¸‹æ–‡ï¼Œç”Ÿæˆå½“å‰ç« èŠ‚çš„å†…å®¹
    æ”¯æŒå¼•ç”¨å·¥ä½œæµï¼Œè‡ªåŠ¨å¤„ç†å¼•ç”¨æ ‡è®°å’Œæºè¿½è¸ª
    Args:
        state: ç ”ç©¶çŠ¶æ€ï¼ŒåŒ…å«ç« èŠ‚ä¿¡æ¯ã€ç ”ç©¶æ•°æ®å’Œå·²å®Œæˆç« èŠ‚
        llm_client: LLMå®¢æˆ·ç«¯å®ä¾‹
        prompt_selector: PromptSelectorå®ä¾‹ï¼Œç”¨äºè·å–promptæ¨¡æ¿
        genre: genreç±»å‹ï¼Œé»˜è®¤ä¸º"default"
    Returns:
        dict: åŒ…å«å½“å‰ç« èŠ‚å†…å®¹å’Œå¼•ç”¨æºçš„å­—å…¸
    """
    # è·å–åŸºæœ¬ä¿¡æ¯
    topic = state.get("topic", "")
    current_chapter_index = state.get("current_chapter_index", 0)
    chapters_to_process = state.get("chapters_to_process", [])
    completed_chapters_content = state.get("completed_chapters_content", [])
    completed_chapters = state.get("completed_chapters", [])

    # éªŒè¯å½“å‰ç« èŠ‚ç´¢å¼•
    if current_chapter_index >= len(chapters_to_process):
        raise ValueError(f"ç« èŠ‚ç´¢å¼• {current_chapter_index} è¶…å‡ºèŒƒå›´")

    # è·å–å½“å‰ç« èŠ‚ä¿¡æ¯
    current_chapter = chapters_to_process[current_chapter_index]
    chapter_title = current_chapter.get("chapter_title", "")
    chapter_description = current_chapter.get("description", "")

    # ä»çŠ¶æ€ä¸­è·å–ç ”ç©¶æ•°æ®
    gathered_sources = state.get("gathered_sources", [])
    gathered_data = state.get("gathered_data", "")  # ä¿æŒå‘åå…¼å®¹

    if not chapter_title:
        raise ValueError("ç« èŠ‚æ ‡é¢˜ä¸èƒ½ä¸ºç©º")

    # æ„å»ºæ»‘åŠ¨çª—å£ + å…¨å±€æ‘˜è¦ä¸Šä¸‹æ–‡
    context_for_writing = ""
    if completed_chapters:
        # è·å–æœ€åä¸€ç« çš„å®Œæ•´å†…å®¹ï¼ˆæ»‘åŠ¨çª—å£ï¼‰
        last_chapter = completed_chapters[-1]
        if isinstance(last_chapter, dict) and "content" in last_chapter:
            context_for_writing += f"**Context from the previous chapter (Full Text):**\n{last_chapter['content']}\n\n"
            logger.info(
                f"ğŸ“– æ·»åŠ å‰ä¸€ç« å®Œæ•´å†…å®¹åˆ°ä¸Šä¸‹æ–‡ï¼Œé•¿åº¦: {len(last_chapter['content'])} å­—ç¬¦")

        # å¦‚æœæœ‰æ›´å¤šç« èŠ‚ï¼Œè·å–æ—©æœŸç« èŠ‚çš„æ‘˜è¦ï¼ˆå…¨å±€æ‘˜è¦ï¼‰
        if len(completed_chapters) > 1:
            earlier_summaries = []
            for chapter in completed_chapters[:-1]:  # é™¤äº†æœ€åä¸€ç« çš„æ‰€æœ‰ç« èŠ‚
                if isinstance(chapter, dict) and "summary" in chapter:
                    earlier_summaries.append(chapter["summary"])
                elif isinstance(chapter, dict) and "content" in chapter:
                    # å¦‚æœæ²¡æœ‰æ‘˜è¦ï¼Œä½¿ç”¨å†…å®¹çš„å‰200å­—ç¬¦ä½œä¸ºæ‘˜è¦
                    content = chapter["content"]
                    summary = content[:200] + "..." if len(
                        content) > 200 else content
                    earlier_summaries.append(summary)

            if earlier_summaries:
                context_for_writing += f"**Context from earlier chapters (Summaries):**\n" + "\n\n".join(
                    earlier_summaries)
                logger.info(f"ğŸ“š æ·»åŠ  {len(earlier_summaries)} ä¸ªæ—©æœŸç« èŠ‚æ‘˜è¦åˆ°ä¸Šä¸‹æ–‡")

    if not context_for_writing:
        context_for_writing = "è¿™æ˜¯ç¬¬ä¸€ç« ï¼Œæ²¡æœ‰å‰ç½®å†…å®¹ã€‚"
        logger.info("ğŸ“ è¿™æ˜¯ç¬¬ä¸€ç« ï¼Œä½¿ç”¨é»˜è®¤ä¸Šä¸‹æ–‡")

    # å¦‚æœæ²¡æœ‰æ”¶é›†åˆ°æºæ•°æ®ï¼Œå°è¯•ä½¿ç”¨æ—§çš„ gathered_data
    if not gathered_sources and not gathered_data:
        return {
            "final_document": f"## {chapter_title}\n\nç”±äºæ²¡æœ‰æ”¶é›†åˆ°ç›¸å…³æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆç« èŠ‚å†…å®¹ã€‚",
            "cited_sources_in_chapter": []
        }

    # æ ¼å¼åŒ–å¯ç”¨ä¿¡æ¯æºåˆ—è¡¨
    available_sources_text = ""
    if gathered_sources:
        available_sources_text = "å¯ç”¨ä¿¡æ¯æºåˆ—è¡¨:\n\n"
        for source in gathered_sources:
            available_sources_text += f"[Source {source.id}] {source.title}\n"
            available_sources_text += f"  ç±»å‹: {source.source_type}\n"
            if source.url:
                available_sources_text += f"  URL: {source.url}\n"
            available_sources_text += f"  å†…å®¹: {source.content[:200]}...\n\n"

        # åŒæ—¶ä¿æŒå‘åå…¼å®¹çš„ gathered_data
        gathered_data = _format_sources_to_text(gathered_sources)
    elif not gathered_data:
        gathered_data = "æ²¡æœ‰æ”¶é›†åˆ°ç›¸å…³æ•°æ®"

    # è·å–æ–‡æ¡£ç”Ÿæˆå™¨é…ç½®
    document_writer_config = settings.get_agent_component_config(
        "document_writer")
    if not document_writer_config:
        temperature = 0.7
        max_tokens = 4000
        extra_params = {}
    else:
        temperature = document_writer_config.temperature
        max_tokens = document_writer_config.max_tokens
        extra_params = document_writer_config.extra_params

    # æ„å»ºå·²å®Œæˆç« èŠ‚çš„ä¸Šä¸‹æ–‡æ‘˜è¦
    previous_chapters_context = ""
    if completed_chapters_content:
        previous_chapters_context = "\n\n".join([
            f"ç¬¬{i+1}ç« æ‘˜è¦:\n{content[:500]}..."
            if len(content) > 500 else f"ç¬¬{i+1}ç« :\n{content}"
            for i, content in enumerate(completed_chapters_content)
        ])

    # è·å–æ ·å¼æŒ‡å—å†…å®¹
    style_guide_content = state.get("style_guide_content")

    # ä½¿ç”¨ PromptSelector è·å– prompt æ¨¡æ¿
    try:
        # æ ¹æ®æŒ‡å®šçš„ prompt_version è·å–æ¨¡æ¿
        from ...prompts.writer import PROMPTS

        # å¦‚æœæœ‰æ ·å¼æŒ‡å—ï¼Œä¼˜å…ˆä½¿ç”¨ v4_with_style_guide ç‰ˆæœ¬
        if style_guide_content and style_guide_content.strip():
            if "v4_with_style_guide" in PROMPTS:
                prompt_template = PROMPTS["v4_with_style_guide"]
                logger.info(f"âœ… ä½¿ç”¨ v4_with_style_guide ç‰ˆæœ¬ï¼Œæ£€æµ‹åˆ°æ ·å¼æŒ‡å—")
            else:
                # å¦‚æœæ²¡æœ‰ v4 ç‰ˆæœ¬ï¼Œå›é€€åˆ°æŒ‡å®šç‰ˆæœ¬
                prompt_template = PROMPTS.get(prompt_version,
                                              PROMPTS.get("v3_context_aware"))
                logger.warning(
                    f"âš ï¸  v4_with_style_guide ç‰ˆæœ¬ä¸å­˜åœ¨ï¼Œå›é€€åˆ° {prompt_version}")
        else:
            # æ²¡æœ‰æ ·å¼æŒ‡å—ï¼Œä½¿ç”¨æŒ‡å®šç‰ˆæœ¬
            if prompt_version in PROMPTS:
                prompt_template = PROMPTS[prompt_version]
                logger.debug(f"âœ… æˆåŠŸè·å– writer {prompt_version} prompt æ¨¡æ¿")
            elif "v3_context_aware" in PROMPTS:
                prompt_template = PROMPTS["v3_context_aware"]
                logger.debug(f"âœ… å›é€€åˆ° writer v3_context_aware prompt æ¨¡æ¿")
            elif "v2_with_citations" in PROMPTS:
                prompt_template = PROMPTS["v2_with_citations"]
                logger.debug(f"âœ… å›é€€åˆ° writer v2_with_citations prompt æ¨¡æ¿")
            else:
                raise KeyError(
                    f"æŒ‡å®šçš„ prompt_version '{prompt_version}' å’Œå¤‡ç”¨ç‰ˆæœ¬éƒ½ä¸å­˜åœ¨")
    except Exception as e:
        logger.warning(f"âš ï¸  è·å– {prompt_version} prompt å¤±è´¥: {e}")
        try:
            # å›é€€åˆ°é»˜è®¤ç‰ˆæœ¬
            prompt_template = prompt_selector.get_prompt(
                "prompts", "writer", genre)
            logger.debug(f"âœ… æˆåŠŸè·å– writer prompt æ¨¡æ¿ï¼Œgenre: {genre}")
        except Exception as e2:
            logger.error(f"âŒ è·å– writer prompt æ¨¡æ¿å¤±è´¥: {e2}")
            # ä½¿ç”¨ prompts/writer.py ä¸­çš„ç®€åŒ–å¤‡ç”¨æ¨¡æ¿
            try:
                from ...prompts.writer import PROMPTS
                simple_prompt_template = PROMPTS.get("v2_fallback_simple",
                                                     PROMPTS["v1_simple"])
                logger.debug("âœ… æˆåŠŸè·å– writer ç®€åŒ–å¤‡ç”¨æ¨¡æ¿")
            except Exception as e2:
                logger.error(f"âŒ è·å– writer ç®€åŒ–å¤‡ç”¨æ¨¡æ¿ä¹Ÿå¤±è´¥: {e2}")
                # æœ€åçš„å¤‡ç”¨æ–¹æ¡ˆ
                simple_prompt_template = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£å†™ä½œä¸“å®¶ã€‚è¯·åŸºäºæä¾›çš„ç ”ç©¶æ•°æ®ï¼Œä¸ºæŒ‡å®šç« èŠ‚æ’°å†™å†…å®¹ã€‚

**æ–‡æ¡£ä¸»é¢˜:** {topic}
**ç« èŠ‚æ ‡é¢˜:** {chapter_title}
**ç« èŠ‚æè¿°:** {chapter_description}
**ç« èŠ‚ç¼–å·:** {chapter_number}/{total_chapters}

**å¯ç”¨ä¿¡æ¯æº:**
{available_sources}

**ç ”ç©¶æ•°æ®:**
{gathered_data}

**å†™ä½œè¦æ±‚:**
1. åŸºäºç ”ç©¶æ•°æ®æ’°å†™å†…å®¹ï¼Œç¡®ä¿ä¿¡æ¯å‡†ç¡®æ€§å’Œå®Œæ•´æ€§
2. ä¿æŒç« èŠ‚ç»“æ„æ¸…æ™°ï¼Œé€»è¾‘è¿è´¯
3. ä½¿ç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€
4. åœ¨å†™ä½œæ—¶ï¼Œå¦‚æœä½¿ç”¨äº†æŸä¸ªä¿¡æ¯æºçš„å†…å®¹ï¼Œè¯·ä½¿ç”¨ç‰¹æ®Šæ ‡è®°ï¼š<sources>[æºID]</sources>
5. ä¾‹å¦‚ï¼š<sources>[1]</sources> è¿™é‡Œä½¿ç”¨äº†æº1çš„ä¿¡æ¯
6. å¦‚æœæ˜¯è‡ªå·±çš„ç»¼åˆæ€»ç»“ï¼Œä½¿ç”¨ï¼š<sources>[]</sources>

è¯·ç«‹å³å¼€å§‹æ’°å†™ç« èŠ‚å†…å®¹ã€‚
"""

    # æ„å»ºé«˜è´¨é‡çš„æç¤ºè¯
    if style_guide_content and style_guide_content.strip():
        # æ ¼å¼åŒ–æ ·å¼æŒ‡å—å†…å®¹
        formatted_style_guide = f"""
{style_guide_content}

"""
        prompt = prompt_template.format(
            topic=topic,
            chapter_title=chapter_title,
            chapter_description=chapter_description,
            chapter_number=current_chapter_index + 1,
            total_chapters=len(chapters_to_process),
            previous_chapters_context=previous_chapters_context
            if previous_chapters_context else "è¿™æ˜¯ç¬¬ä¸€ç« ï¼Œæ²¡æœ‰å‰ç½®å†…å®¹ã€‚",
            gathered_data=gathered_data,
            available_sources=available_sources_text,
            context_for_writing=context_for_writing,
            style_guide_content=formatted_style_guide)
        logger.info(f"ğŸ“ åŒ…å«æ ·å¼æŒ‡å—çš„å†™ä½œï¼Œæ ·å¼æŒ‡å—é•¿åº¦: {len(style_guide_content)} å­—ç¬¦")
    else:
        # ä¸åŒ…å«æ ·å¼æŒ‡å—çš„ç‰ˆæœ¬
        prompt = prompt_template.format(
            topic=topic,
            chapter_title=chapter_title,
            chapter_description=chapter_description,
            chapter_number=current_chapter_index + 1,
            total_chapters=len(chapters_to_process),
            previous_chapters_context=previous_chapters_context
            if previous_chapters_context else "è¿™æ˜¯ç¬¬ä¸€ç« ï¼Œæ²¡æœ‰å‰ç½®å†…å®¹ã€‚",
            gathered_data=gathered_data,
            available_sources=available_sources_text,
            context_for_writing=context_for_writing)
        logger.info("ğŸ“ æ ‡å‡†å†™ä½œï¼ŒæœªåŒ…å«æ ·å¼æŒ‡å—")

    # é™åˆ¶ prompt é•¿åº¦
    max_prompt_length = 30000
    if len(prompt) > max_prompt_length:
        logger.warning(
            f"âš ï¸  Writer prompt é•¿åº¦ {len(prompt)} è¶…è¿‡é™åˆ¶ {max_prompt_length}ï¼Œè¿›è¡Œæˆªæ–­"
        )

        # ä¼˜å…ˆä¿ç•™å½“å‰ç« èŠ‚çš„ç ”ç©¶æ•°æ®ï¼Œé€‚å½“ç¼©å‡å·²å®Œæˆç« èŠ‚çš„ä¸Šä¸‹æ–‡
        if len(previous_chapters_context) > 5000:
            # åªä¿ç•™æ¯ç« çš„ç®€çŸ­æ‘˜è¦
            previous_chapters_context = "\n\n".join([
                f"ç¬¬{i+1}ç« æ‘˜è¦:\n{content[:200]}..."
                for i, content in enumerate(completed_chapters_content)
            ])

        # å¦‚æœç ”ç©¶æ•°æ®ä¹Ÿå¤ªé•¿ï¼Œè¿›è¡Œæˆªæ–­
        if len(gathered_data) > 15000:
            gathered_data = gathered_data[:15000] + "\n\n... (ç ”ç©¶æ•°æ®å·²æˆªæ–­)"

        # é‡æ–°æ„å»ºprompt - ä¼˜å…ˆä½¿ç”¨æ”¯æŒå¼•ç”¨çš„ç‰ˆæœ¬
        try:
            # å¯¹äºé•¿ prompt æˆªæ–­ï¼Œä¼˜å…ˆä½¿ç”¨æ”¯æŒå¼•ç”¨çš„ç®€åŒ–ç‰ˆæœ¬
            from ...prompts.writer import PROMPTS
            if "v2_simple_citations" in PROMPTS:
                simple_prompt_template = PROMPTS["v2_simple_citations"]
                logger.debug("âœ… æˆåŠŸè·å– writer v2_simple_citations prompt æ¨¡æ¿")
            else:
                # å›é€€åˆ°é»˜è®¤ç‰ˆæœ¬
                simple_prompt_template = prompt_selector.get_prompt(
                    "prompts", "writer", "default")
                logger.debug("âœ… æˆåŠŸè·å– writer default prompt æ¨¡æ¿")
        except Exception as e:
            logger.error(f"âŒ è·å– writer prompt æ¨¡æ¿å¤±è´¥: {e}")
            # ä½¿ç”¨ç®€åŒ–çš„å¤‡ç”¨æ¨¡æ¿ï¼ˆæ”¯æŒå¼•ç”¨ï¼‰
            simple_prompt_template = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£å†™ä½œä¸“å®¶ã€‚è¯·åŸºäºæä¾›çš„ç ”ç©¶æ•°æ®ï¼Œä¸ºæŒ‡å®šç« èŠ‚æ’°å†™å†…å®¹ã€‚

**æ–‡æ¡£ä¸»é¢˜:** {topic}
**ç« èŠ‚æ ‡é¢˜:** {chapter_title}
**ç« èŠ‚æè¿°:** {chapter_description}
**ç« èŠ‚ç¼–å·:** {chapter_number}/{total_chapters}

**å¯ç”¨ä¿¡æ¯æº:**
{available_sources}

**ç ”ç©¶æ•°æ®:**
{gathered_data}

**å†™ä½œè¦æ±‚:**
1. åŸºäºç ”ç©¶æ•°æ®æ’°å†™å†…å®¹ï¼Œç¡®ä¿ä¿¡æ¯å‡†ç¡®æ€§å’Œå®Œæ•´æ€§
2. ä¿æŒç« èŠ‚ç»“æ„æ¸…æ™°ï¼Œé€»è¾‘è¿è´¯
3. ä½¿ç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€
4. åœ¨å†™ä½œæ—¶ï¼Œå¦‚æœä½¿ç”¨äº†æŸä¸ªä¿¡æ¯æºçš„å†…å®¹ï¼Œè¯·ä½¿ç”¨ç‰¹æ®Šæ ‡è®°ï¼š<sources>[æºID]</sources>
5. ä¾‹å¦‚ï¼š<sources>[1]</sources> è¿™é‡Œä½¿ç”¨äº†æº1çš„ä¿¡æ¯
6. å¦‚æœæ˜¯è‡ªå·±çš„ç»¼åˆæ€»ç»“ï¼Œä½¿ç”¨ï¼š<sources>[]</sources>

è¯·ç«‹å³å¼€å§‹æ’°å†™ç« èŠ‚å†…å®¹ã€‚
"""

        prompt = simple_prompt_template.format(
            topic=topic,
            chapter_title=chapter_title,
            chapter_description=chapter_description,
            chapter_number=current_chapter_index + 1,
            total_chapters=len(chapters_to_process),
            gathered_data=gathered_data)
        logger.info(f"ğŸ“ æˆªæ–­å writer prompt é•¿åº¦: {len(prompt)} å­—ç¬¦")

    logger.debug(f"Invoking LLM with writer prompt:\n{pprint.pformat(prompt)}")

    try:
        # è°ƒç”¨LLMç”Ÿæˆç« èŠ‚å†…å®¹
        response = llm_client.invoke(prompt,
                                     temperature=temperature,
                                     max_tokens=max_tokens,
                                     **extra_params)

        # ç¡®ä¿å“åº”æ ¼å¼æ­£ç¡®
        if not response.strip():
            response = f"## {chapter_title}\n\næ— æ³•ç”Ÿæˆç« èŠ‚å†…å®¹ã€‚"
        elif not response.startswith("##"):
            # å¦‚æœæ²¡æœ‰äºŒçº§æ ‡é¢˜ï¼Œæ·»åŠ ç« èŠ‚æ ‡é¢˜
            response = f"## {chapter_title}\n\n{response}"

        # å®šä¹‰å†…è”å¼•ç”¨å¤„ç†å‡½æ•°
        def _process_citations_inline(
                raw_text: str,
                available_sources: list[Source]) -> tuple[str, list[Source]]:
            """
            å¤„ç†LLMè¾“å‡ºä¸­çš„å¼•ç”¨æ ‡è®°ï¼Œæå–å¼•ç”¨çš„æºå¹¶æ ¼å¼åŒ–æ–‡æœ¬
            
            Args:
                raw_text: LLMçš„åŸå§‹è¾“å‡ºæ–‡æœ¬
                available_sources: å¯ç”¨çš„ä¿¡æ¯æºåˆ—è¡¨
                
            Returns:
                tuple[str, list[Source]]: (å¤„ç†åçš„æ–‡æœ¬, å¼•ç”¨çš„æºåˆ—è¡¨)
            """
            processed_text = raw_text
            cited_sources = []

            # åˆ›å»ºæºIDåˆ°æºå¯¹è±¡çš„æ˜ å°„
            source_map = {source.id: source for source in available_sources}

            def _replace_sources_tag(match):
                """æ›¿æ¢å¼•ç”¨æ ‡è®°çš„è¾…åŠ©å‡½æ•°"""
                try:
                    # æå–æºIDåˆ—è¡¨ï¼Œä¾‹å¦‚ä» [1, 3] ä¸­æå– [1, 3]
                    content = match.group(1).strip()

                    if not content:  # ç©ºæ ‡ç­¾ <sources>[]</sources>
                        logger.debug("  ğŸ“ å¤„ç†ç©ºå¼•ç”¨æ ‡è®°ï¼ˆç»¼åˆåˆ†æï¼‰")
                        return ""  # ç§»é™¤ç©ºæ ‡ç­¾

                    # è§£ææºIDåˆ—è¡¨
                    source_ids = []
                    for id_str in content.split(','):
                        id_str = id_str.strip()
                        if id_str.isdigit():
                            source_ids.append(int(id_str))

                    logger.debug(f"  ğŸ“š è§£æåˆ°æºID: {source_ids}")

                    # æ”¶é›†å¼•ç”¨çš„æºå¹¶ç”Ÿæˆå¼•ç”¨æ ‡è®°
                    citation_markers = []
                    for source_id in source_ids:
                        if source_id in source_map:
                            source = source_map[source_id]
                            cited_sources.append(source)
                            citation_markers.append(f"[{source_id}]")
                            logger.debug(
                                f"    âœ… æ·»åŠ å¼•ç”¨æº: [{source_id}] {source.title}")
                        else:
                            logger.warning(f"    âš ï¸  æœªæ‰¾åˆ°æºID: {source_id}")

                    # è¿”å›æ ¼å¼åŒ–çš„å¼•ç”¨æ ‡è®°
                    return "".join(citation_markers)

                except Exception as e:
                    logger.error(f"âŒ å¤„ç†å¼•ç”¨æ ‡è®°å¤±è´¥: {e}")
                    return ""  # ç§»é™¤æ— æ•ˆæ ‡ç­¾

            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢æ‰€æœ‰å¼•ç”¨æ ‡è®°
            sources_pattern = r'<sources>\[([^\]]*)\]</sources>'
            processed_text = re.sub(sources_pattern, _replace_sources_tag,
                                    processed_text)

            logger.info(f"âœ… å¼•ç”¨å¤„ç†å®Œæˆï¼Œå¼•ç”¨äº† {len(cited_sources)} ä¸ªä¿¡æ¯æº")

            return processed_text, cited_sources

        # å¤„ç†å¼•ç”¨æ ‡è®°
        processed_response, cited_sources = _process_citations_inline(
            response, gathered_sources)

        logger.info(f"âœ… ç« èŠ‚ç”Ÿæˆå®Œæˆï¼Œå¼•ç”¨äº† {len(cited_sources)} ä¸ªä¿¡æ¯æº")
        for source in cited_sources:
            logger.debug(f"  ğŸ“š å¼•ç”¨æº: [{source.id}] {source.title}")

        # è¿”å›å½“å‰ç« èŠ‚çš„å†…å®¹å’Œå¼•ç”¨æº
        return {
            "final_document": processed_response,
            "cited_sources_in_chapter": cited_sources
        }

    except Exception as e:
        # å¦‚æœLLMè°ƒç”¨å¤±è´¥ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
        logger.error(f"Writer node error: {str(e)}")
        error_content = f"""## {chapter_title}

### ç« èŠ‚ç”Ÿæˆé”™è¯¯

ç”±äºæŠ€æœ¯åŸå› ï¼Œæ— æ³•ç”Ÿæˆæœ¬ç« èŠ‚çš„å†…å®¹ã€‚

**é”™è¯¯ä¿¡æ¯:** {str(e)}

**ç« èŠ‚æè¿°:** {chapter_description}

è¯·æ£€æŸ¥ç³»ç»Ÿé…ç½®æˆ–ç¨åé‡è¯•ã€‚
"""
        return {
            "final_document": error_content,
            "cited_sources_in_chapter": []
        }


async def reflection_node(state: ResearchState,
                          llm_client: LLMClient,
                          prompt_selector: PromptSelector,
                          genre: str = "default") -> dict[str, Any]:
    """
    æ™ºèƒ½æŸ¥è¯¢æ‰©å±•èŠ‚ç‚¹
    åˆ†æç°æœ‰çš„æœç´¢æŸ¥è¯¢å’Œå·²æ”¶é›†çš„æ•°æ®ï¼Œç”Ÿæˆæ›´ç²¾ç¡®ã€æ›´ç›¸å…³çš„æœç´¢æŸ¥è¯¢

    Args:
        state: ç ”ç©¶çŠ¶æ€ï¼ŒåŒ…å« topicã€search_queries å’Œ gathered_data
        llm_client: LLMå®¢æˆ·ç«¯å®ä¾‹
        prompt_selector: PromptSelectorå®ä¾‹ï¼Œç”¨äºè·å–promptæ¨¡æ¿
        genre: genreç±»å‹ï¼Œé»˜è®¤ä¸º"default"

    Returns:
        dict: åŒ…å«æ›´æ–°åçš„ search_queries çš„å­—å…¸
    """
    # ä»çŠ¶æ€ä¸­è·å–å¿…è¦ä¿¡æ¯
    topic = state.get("topic", "")
    original_search_queries = state.get("search_queries", [])
    gathered_data = state.get("gathered_data", "")
    gathered_sources = state.get("gathered_sources", [])
    current_chapter_index = state.get("current_chapter_index", 0)
    chapters_to_process = state.get("chapters_to_process", [])

    # è·å–å½“å‰ç« èŠ‚ä¿¡æ¯
    current_chapter = None
    if current_chapter_index < len(chapters_to_process):
        current_chapter = chapters_to_process[current_chapter_index]

    chapter_title = current_chapter.get("chapter_title",
                                        "") if current_chapter else ""
    chapter_description = current_chapter.get("description",
                                              "") if current_chapter else ""

    # ä¼˜å…ˆä½¿ç”¨ gathered_sources çš„æ•°æ®ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ gathered_data
    if gathered_sources and not gathered_data:
        gathered_data = _format_sources_to_text(gathered_sources)
        logger.info(
            f"ğŸ“Š ä» gathered_sources è½¬æ¢ä¸º gathered_dataï¼Œé•¿åº¦: {len(gathered_data)} å­—ç¬¦"
        )

    logger.info("ğŸ¤” å¼€å§‹æ™ºèƒ½æŸ¥è¯¢æ‰©å±•åˆ†æ")
    logger.info(f"ğŸ“‹ ç« èŠ‚: {chapter_title}")
    logger.info(f"ğŸ” åŸå§‹æŸ¥è¯¢æ•°é‡: {len(original_search_queries)}")
    logger.info(f"ğŸ“Š å·²æ”¶é›†æ•°æ®é•¿åº¦: {len(gathered_data)} å­—ç¬¦")
    logger.info(f"ğŸ“š å·²æ”¶é›†æºæ•°é‡: {len(gathered_sources)}")

    # éªŒè¯è¾“å…¥æ•°æ®
    if not topic:
        logger.warning("âŒ ç¼ºå°‘ä¸»é¢˜ä¿¡æ¯ï¼Œæ— æ³•è¿›è¡ŒæŸ¥è¯¢æ‰©å±•")
        return {"search_queries": original_search_queries}

    if not original_search_queries:
        logger.warning("âŒ æ²¡æœ‰åŸå§‹æŸ¥è¯¢ï¼Œæ— æ³•è¿›è¡Œæ‰©å±•")
        return {"search_queries": []}

    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œåˆ†æ
    has_sufficient_data = ((gathered_data and len(gathered_data.strip()) >= 50)
                           or (gathered_sources and len(gathered_sources) > 0))

    if not has_sufficient_data:
        logger.warning("âŒ æ”¶é›†çš„æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆåˆ†æ")
        return {"search_queries": original_search_queries}

    # è·å–æŸ¥è¯¢æ‰©å±•å™¨é…ç½®
    query_expander_config = settings.get_agent_component_config(
        "query_expander")
    if not query_expander_config:
        temperature = 0.7
        max_tokens = 2000
        extra_params = {}
    else:
        temperature = query_expander_config.temperature
        max_tokens = query_expander_config.max_tokens
        extra_params = query_expander_config.extra_params

    # ä½¿ç”¨ PromptSelector è·å– prompt æ¨¡æ¿
    try:
        prompt_template = prompt_selector.get_prompt("chapter_workflow",
                                                     "reflection", genre)
        logger.debug(f"âœ… æˆåŠŸè·å– reflection prompt æ¨¡æ¿ï¼Œgenre: {genre}")
    except Exception as e:
        logger.error(f"âŒ è·å– reflection prompt æ¨¡æ¿å¤±è´¥: {e}")
        # ä½¿ç”¨ prompts/reflection.py ä¸­çš„å¤‡ç”¨æ¨¡æ¿
        try:
            from ...prompts.reflection import PROMPTS
            prompt_template = PROMPTS.get("v1_fallback", PROMPTS["v1_default"])
            logger.debug("âœ… æˆåŠŸè·å– reflection å¤‡ç”¨æ¨¡æ¿")
        except Exception as e2:
            logger.error(f"âŒ è·å– reflection å¤‡ç”¨æ¨¡æ¿ä¹Ÿå¤±è´¥: {e2}")
            # æœ€åçš„å¤‡ç”¨æ–¹æ¡ˆ
            prompt_template = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶ä¸“å®¶å’ŒæŸ¥è¯¢ä¼˜åŒ–å¸ˆã€‚è¯·åˆ†æç°æœ‰çš„æœç´¢æŸ¥è¯¢å’Œå·²æ”¶é›†çš„æ•°æ®ï¼Œç”Ÿæˆæ›´ç²¾ç¡®ã€æ›´ç›¸å…³çš„æœç´¢æŸ¥è¯¢ã€‚

**æ–‡æ¡£ä¸»é¢˜:** {topic}

**å½“å‰ç« èŠ‚ä¿¡æ¯:**
- ç« èŠ‚æ ‡é¢˜: {chapter_title}
- ç« èŠ‚æè¿°: {chapter_description}

**åŸå§‹æœç´¢æŸ¥è¯¢:**
{original_queries}

**å·²æ”¶é›†çš„æ•°æ®æ‘˜è¦:**
{gathered_data_summary}

**ä»»åŠ¡è¦æ±‚:**
1. ä»”ç»†åˆ†æå·²æ”¶é›†çš„æ•°æ®ï¼Œè¯†åˆ«ä¿¡æ¯ç¼ºå£ã€æ¨¡ç³Šä¹‹å¤„æˆ–æ–°çš„æœ‰è¶£æ–¹å‘
2. è€ƒè™‘åŸå§‹æŸ¥è¯¢çš„è¦†ç›–èŒƒå›´å’Œæ·±åº¦
3. ç”Ÿæˆ2-3ä¸ªæ–°çš„ã€é«˜åº¦ç›¸å…³çš„ã€æ›´å…·ä½“çš„æˆ–æ¢ç´¢æ€§çš„æœç´¢æŸ¥è¯¢
4. æ–°æŸ¥è¯¢åº”è¯¥ï¼š
   - å¡«è¡¥ä¿¡æ¯ç¼ºå£
   - æ·±å…¥ç‰¹å®šæ–¹é¢
   - æ¢ç´¢æ–°çš„è§’åº¦æˆ–è§†è§’
   - ä½¿ç”¨æ›´ç²¾ç¡®çš„å…³é”®è¯

**è¾“å‡ºæ ¼å¼:**
è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- new_queries: æ–°çš„æœç´¢æŸ¥è¯¢åˆ—è¡¨ï¼ˆæ•°ç»„ï¼Œ2-3ä¸ªæŸ¥è¯¢ï¼‰
- reasoning: ç®€è¦è¯´æ˜ä¸ºä»€ä¹ˆéœ€è¦è¿™äº›æ–°æŸ¥è¯¢

è¯·ç«‹å³å¼€å§‹åˆ†æå¹¶ç”Ÿæˆæ–°çš„æŸ¥è¯¢ã€‚
"""

    # å‡†å¤‡æ•°æ®æ‘˜è¦ï¼ˆé¿å…promptè¿‡é•¿ï¼‰
    gathered_data_summary = gathered_data
    if len(gathered_data) > 3000:
        gathered_data_summary = gathered_data[:
                                              1500] + "\n\n... (æ•°æ®å·²æˆªæ–­) ...\n\n" + gathered_data[
                                                  -1500:]

    # æ ¼å¼åŒ–åŸå§‹æŸ¥è¯¢
    original_queries_text = "\n".join(
        [f"{i+1}. {query}" for i, query in enumerate(original_search_queries)])

    # æ„å»º prompt
    prompt = prompt_template.format(
        topic=topic,
        chapter_title=chapter_title,
        chapter_description=chapter_description,
        original_queries=original_queries_text,
        gathered_data_summary=gathered_data_summary)

    logger.debug(
        f"Invoking LLM with reflection prompt:\n{pprint.pformat(prompt)}")

    try:
        # è°ƒç”¨ LLM ç”Ÿæˆæ–°çš„æŸ¥è¯¢
        response = llm_client.invoke(prompt,
                                     temperature=temperature,
                                     max_tokens=max_tokens,
                                     **extra_params)

        logger.debug(f"ğŸ” LLMåŸå§‹å“åº”: {repr(response)}")
        logger.debug(f"ğŸ“ å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")

        # è§£æå“åº”ï¼Œæå–æ–°çš„æŸ¥è¯¢
        new_queries = _parse_reflection_response(response)

        if new_queries:
            logger.info(f"âœ… æˆåŠŸç”Ÿæˆ {len(new_queries)} ä¸ªæ–°æŸ¥è¯¢")
            for i, query in enumerate(new_queries, 1):
                logger.debug(f"  {i}. {query}")

            # è¿”å›æ›´æ–°åçš„æŸ¥è¯¢åˆ—è¡¨
            return {"search_queries": new_queries}
        else:
            logger.warning("âš ï¸ æ— æ³•è§£ææ–°æŸ¥è¯¢ï¼Œä¿æŒåŸå§‹æŸ¥è¯¢")
            return {"search_queries": original_search_queries}

    except Exception as e:
        logger.error(f"Reflection node error: {str(e)}")
        logger.warning("âš ï¸ æŸ¥è¯¢æ‰©å±•å¤±è´¥ï¼Œä¿æŒåŸå§‹æŸ¥è¯¢")
        return {"search_queries": original_search_queries}


def _parse_reflection_response(response: str) -> list[str]:
    """
    è§£æ reflection èŠ‚ç‚¹çš„ LLM å“åº”ï¼Œæå–æ–°çš„æœç´¢æŸ¥è¯¢

    Args:
        response: LLM çš„åŸå§‹å“åº”

    Returns:
        list[str]: æ–°çš„æœç´¢æŸ¥è¯¢åˆ—è¡¨
    """
    try:
        # å°è¯•è§£æ JSON æ ¼å¼
        import json
        import re

        # æ¸…ç†å“åº”æ–‡æœ¬
        cleaned_response = response.strip()

        # å°è¯•æå– JSON éƒ¨åˆ†
        json_match = re.search(r'\{.*\}', cleaned_response, re.DOTALL)
        if json_match:
            json_str = json_match.group(0)
            data = json.loads(json_str)

            if 'new_queries' in data and isinstance(data['new_queries'], list):
                queries = data['new_queries']
                # éªŒè¯æŸ¥è¯¢è´¨é‡
                valid_queries = [
                    q.strip() for q in queries
                    if q.strip() and len(q.strip()) > 5
                ]
                if valid_queries:
                    return valid_queries

        # å¦‚æœ JSON è§£æå¤±è´¥ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–æŸ¥è¯¢
        # æŸ¥æ‰¾å¸¸è§çš„æŸ¥è¯¢æ¨¡å¼
        query_patterns = [
            r'(\d+\.\s*)([^\n]+)',  # 1. query
            r'[-â€¢]\s*([^\n]+)',  # - query æˆ– â€¢ query
            r'"([^"]+)"',  # "query"
        ]

        for pattern in query_patterns:
            try:
                matches = re.findall(pattern, cleaned_response, re.MULTILINE)
                if matches:
                    queries = []
                    for match in matches:
                        if isinstance(match, tuple):
                            query = match[1] if len(match) > 1 else match[0]
                        else:
                            query = match

                        query = query.strip()
                        if query and len(query) > 5:
                            queries.append(query)

                    if queries:
                        return queries
            except Exception as e:
                logger.debug(f"æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å¤±è´¥: {e}")
                continue

        # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œå°è¯•ç®€å•çš„è¡Œåˆ†å‰²
        lines = cleaned_response.split('\n')
        queries = []
        for line in lines:
            line = line.strip()
            # è·³è¿‡ç©ºè¡Œã€æ•°å­—è¡Œã€æ ‡é¢˜è¡Œç­‰
            if (line and len(line) > 10 and not line.startswith('#')
                    and not re.match(r'^\d+\.?$', line)
                    and not re.match(r'^[A-Z\s]+$', line)):  # å…¨å¤§å†™å¯èƒ½æ˜¯æ ‡é¢˜
                queries.append(line)

        return queries[:3]  # æœ€å¤šè¿”å›3ä¸ªæŸ¥è¯¢

    except Exception as e:
        logger.error(f"è§£æ reflection å“åº”å¤±è´¥: {e}")
        return []


def _parse_web_search_results(web_results: str, query: str,
                              start_id: int) -> list[Source]:
    """
    è§£æç½‘ç»œæœç´¢ç»“æœï¼Œåˆ›å»º Source å¯¹è±¡åˆ—è¡¨
    
    Args:
        web_results: ç½‘ç»œæœç´¢çš„åŸå§‹ç»“æœå­—ç¬¦ä¸²
        query: æœç´¢æŸ¥è¯¢
        start_id: èµ·å§‹ID
        
    Returns:
        list[Source]: Source å¯¹è±¡åˆ—è¡¨
    """
    sources = []
    current_id = start_id

    try:
        # ç®€å•çš„è§£æé€»è¾‘ï¼šæŒ‰è¡Œåˆ†å‰²ï¼Œæå–æ ‡é¢˜å’ŒURL
        lines = web_results.split('\n')
        current_title = ""
        current_url = ""
        current_content = ""

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # å°è¯•æå–æ ‡é¢˜å’ŒURL
            if line.startswith('http') or line.startswith('https'):
                # è¿™æ˜¯ä¸€ä¸ªURL
                if current_title and current_content:
                    # åˆ›å»ºå‰ä¸€ä¸ªæº
                    source = Source(
                        id=current_id,
                        source_type="webpage",
                        title=current_title,
                        url=current_url,
                        content=current_content[:500] + "..."
                        if len(current_content) > 500 else current_content)
                    sources.append(source)
                    current_id += 1

                current_url = line
                current_title = ""
                current_content = ""
            elif line.startswith('æ ‡é¢˜:') or line.startswith('Title:'):
                current_title = line.split(':', 1)[1].strip()
            elif line.startswith('å†…å®¹:') or line.startswith('Content:'):
                current_content = line.split(':', 1)[1].strip()
            elif not current_title and len(
                    line) > 10 and not line.startswith('==='):
                # å¯èƒ½æ˜¯æ ‡é¢˜
                current_title = line
            elif current_title and len(line) > 20:
                # å¯èƒ½æ˜¯å†…å®¹
                current_content += " " + line

        # å¤„ç†æœ€åä¸€ä¸ªæº
        if current_title and current_content:
            source = Source(
                id=current_id,
                source_type="webpage",
                title=current_title,
                url=current_url,
                content=current_content[:500] +
                "..." if len(current_content) > 500 else current_content)
            sources.append(source)

        # å¦‚æœæ²¡æœ‰è§£æåˆ°ä»»ä½•æºï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤æº
        if not sources:
            source = Source(id=current_id,
                            source_type="webpage",
                            title=f"ç½‘ç»œæœç´¢ç»“æœ - {query}",
                            url="",
                            content=web_results[:500] +
                            "..." if len(web_results) > 500 else web_results)
            sources.append(source)

    except Exception as e:
        logger.error(f"è§£æç½‘ç»œæœç´¢ç»“æœå¤±è´¥: {str(e)}")
        # åˆ›å»ºé»˜è®¤æº
        source = Source(id=start_id,
                        source_type="webpage",
                        title=f"ç½‘ç»œæœç´¢ç»“æœ - {query}",
                        url="",
                        content=web_results[:500] +
                        "..." if len(web_results) > 500 else web_results)
        sources.append(source)

    return sources


def _process_citations(
        raw_text: str,
        available_sources: list[Source],
        global_cited_sources: dict = None) -> tuple[str, list[Source]]:
    """
    å¤„ç†LLMè¾“å‡ºä¸­çš„å¼•ç”¨æ ‡è®°ï¼Œæå–å¼•ç”¨çš„æºå¹¶æ ¼å¼åŒ–æ–‡æœ¬
    
    Args:
        raw_text: LLMçš„åŸå§‹è¾“å‡ºæ–‡æœ¬
        available_sources: å¯ç”¨çš„ä¿¡æ¯æºåˆ—è¡¨
        global_cited_sources: å…¨å±€å·²å¼•ç”¨çš„æºå­—å…¸ï¼Œç”¨äºè¿ç»­ç¼–å·
        
    Returns:
        tuple[str, list[Source]]: (å¤„ç†åçš„æ–‡æœ¬, å¼•ç”¨çš„æºåˆ—è¡¨)
    """
    processed_text = raw_text
    cited_sources = []

    if global_cited_sources is None:
        global_cited_sources = {}

    try:
        # åˆ›å»ºæºIDåˆ°æºå¯¹è±¡çš„æ˜ å°„
        source_map = {source.id: source for source in available_sources}

        # æŸ¥æ‰¾æ‰€æœ‰ <sources>[...]</sources> æ ‡ç­¾
        sources_pattern = r'<sources>\[([^\]]*)\]</sources>'
        matches = re.findall(sources_pattern, processed_text)

        logger.debug(f"ğŸ” æ‰¾åˆ° {len(matches)} ä¸ªå¼•ç”¨æ ‡è®°")

        for match in matches:
            if not match.strip():  # ç©ºæ ‡ç­¾ <sources>[]</sources>
                # æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²ï¼ˆç»¼åˆåˆ†æï¼Œä¸éœ€è¦å¼•ç”¨ï¼‰
                processed_text = processed_text.replace(
                    f'<sources>[{match}]</sources>', '', 1)
                logger.debug("  ğŸ“ å¤„ç†ç©ºå¼•ç”¨æ ‡è®°ï¼ˆç»¼åˆåˆ†æï¼‰")
                continue

            # è§£ææºIDåˆ—è¡¨
            try:
                source_ids = [
                    int(id.strip()) for id in match.split(',')
                    if id.strip().isdigit()
                ]
                logger.debug(f"  ğŸ“š è§£æåˆ°æºID: {source_ids}")

                # æ”¶é›†å¼•ç”¨çš„æºå¹¶åˆ†é…å…¨å±€ç¼–å·
                citation_markers = []
                for source_id in source_ids:
                    if source_id in source_map:
                        source = source_map[source_id]
                        cited_sources.append(source)

                        # åˆ†é…å…¨å±€ç¼–å·
                        if source_id not in global_cited_sources:
                            global_cited_sources[source_id] = source

                        # ä½¿ç”¨å…¨å±€ç¼–å·
                        global_number = list(
                            global_cited_sources.keys()).index(source_id) + 1
                        citation_markers.append(f"[{global_number}]")

                        logger.debug(
                            f"    âœ… æ·»åŠ å¼•ç”¨æº: [{global_number}] {source.title}")
                    else:
                        logger.warning(f"    âš ï¸  æœªæ‰¾åˆ°æºID: {source_id}")

                # æ›¿æ¢ä¸ºæ ¼å¼åŒ–çš„å¼•ç”¨æ ‡è®°
                formatted_citation = "".join(citation_markers)
                processed_text = processed_text.replace(
                    f'<sources>[{match}]</sources>', formatted_citation, 1)

            except ValueError as e:
                logger.error(f"âŒ è§£ææºIDå¤±è´¥: {e}")
                # ç§»é™¤æ— æ•ˆçš„æ ‡ç­¾
                processed_text = processed_text.replace(
                    f'<sources>[{match}]</sources>', '', 1)

        logger.info(f"âœ… å¼•ç”¨å¤„ç†å®Œæˆï¼Œå¼•ç”¨äº† {len(cited_sources)} ä¸ªä¿¡æ¯æº")

    except Exception as e:
        logger.error(f"âŒ å¤„ç†å¼•ç”¨æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        # å¦‚æœå¤„ç†å¤±è´¥ï¼Œè¿”å›åŸå§‹æ–‡æœ¬å’Œç©ºåˆ—è¡¨
        return raw_text, []

    return processed_text, cited_sources


def _format_sources_to_text(sources: list[Source]) -> str:
    """
    å°† Source å¯¹è±¡åˆ—è¡¨æ ¼å¼åŒ–ä¸ºæ–‡æœ¬æ ¼å¼ï¼Œç”¨äºå‘åå…¼å®¹
    
    Args:
        sources: Source å¯¹è±¡åˆ—è¡¨
        
    Returns:
        str: æ ¼å¼åŒ–çš„æ–‡æœ¬
    """
    if not sources:
        return "æ²¡æœ‰æ”¶é›†åˆ°ç›¸å…³æ•°æ®"

    formatted_text = "æ”¶é›†åˆ°çš„ä¿¡æ¯æº:\n\n"

    for i, source in enumerate(sources, 1):
        formatted_text += f"=== ä¿¡æ¯æº {i} ===\n"
        formatted_text += f"æ ‡é¢˜: {source.title}\n"
        if source.url:
            formatted_text += f"URL: {source.url}\n"
        formatted_text += f"ç±»å‹: {source.source_type}\n"
        formatted_text += f"å†…å®¹: {source.content}\n\n"

    return formatted_text


def _parse_es_search_results(es_results: str, query: str,
                             start_id: int) -> list[Source]:
    """
    è§£æESæœç´¢ç»“æœï¼Œåˆ›å»º Source å¯¹è±¡åˆ—è¡¨
    
    Args:
        es_results: ESæœç´¢çš„åŸå§‹ç»“æœå­—ç¬¦ä¸²
        query: æœç´¢æŸ¥è¯¢
        start_id: èµ·å§‹ID
        
    Returns:
        list[Source]: Source å¯¹è±¡åˆ—è¡¨
    """
    sources = []
    current_id = start_id

    try:
        # è§£æESæœç´¢ç»“æœ
        lines = es_results.split('\n')
        current_title = ""
        current_content = ""
        current_url = ""

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # å°è¯•æå–æ–‡æ¡£ä¿¡æ¯
            if line.startswith('æ–‡æ¡£æ ‡é¢˜:') or line.startswith('Title:'):
                current_title = line.split(':', 1)[1].strip()
            elif line.startswith('æ–‡æ¡£å†…å®¹:') or line.startswith('Content:'):
                current_content = line.split(':', 1)[1].strip()
            elif line.startswith('æ–‡æ¡£URL:') or line.startswith('URL:'):
                current_url = line.split(':', 1)[1].strip()
            elif line.startswith('---') or line.startswith('==='):
                # åˆ†éš”ç¬¦ï¼Œå¤„ç†å‰ä¸€ä¸ªæ–‡æ¡£
                if current_title and current_content:
                    source = Source(
                        id=current_id,
                        source_type="es_result",
                        title=current_title,
                        url=current_url,
                        content=current_content[:500] + "..."
                        if len(current_content) > 500 else current_content)
                    sources.append(source)
                    current_id += 1
                    current_title = ""
                    current_content = ""
                    current_url = ""
            elif not current_title and len(line) > 10:
                # å¯èƒ½æ˜¯æ ‡é¢˜
                current_title = line
            elif current_title and len(line) > 20:
                # å¯èƒ½æ˜¯å†…å®¹
                current_content += " " + line

        # å¤„ç†æœ€åä¸€ä¸ªæ–‡æ¡£
        if current_title and current_content:
            source = Source(
                id=current_id,
                source_type="es_result",
                title=current_title,
                url=current_url,
                content=current_content[:500] +
                "..." if len(current_content) > 500 else current_content)
            sources.append(source)

        # å¦‚æœæ²¡æœ‰è§£æåˆ°ä»»ä½•æºï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤æº
        if not sources:
            source = Source(id=start_id,
                            source_type="es_result",
                            title=f"çŸ¥è¯†åº“æœç´¢ç»“æœ - {query}",
                            url="",
                            content=es_results[:500] +
                            "..." if len(es_results) > 500 else es_results)
            sources.append(source)

    except Exception as e:
        logger.error(f"è§£æESæœç´¢ç»“æœå¤±è´¥: {str(e)}")
        # åˆ›å»ºé»˜è®¤æº
        source = Source(id=start_id,
                        source_type="es_result",
                        title=f"çŸ¥è¯†åº“æœç´¢ç»“æœ - {query}",
                        url="",
                        content=es_results[:500] +
                        "..." if len(es_results) > 500 else es_results)
        sources.append(source)

    return sources
