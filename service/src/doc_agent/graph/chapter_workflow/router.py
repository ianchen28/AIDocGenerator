# service/src/doc_agent/graph/router.py
import pprint
from typing import Literal

from loguru import logger

from ...llm_clients.base import LLMClient
from ..state import ResearchState


def supervisor_router(
    state: ResearchState, llm_client: LLMClient
) -> Literal["continue_to_writer", "rerun_researcher"]:
    """
    æ¡ä»¶è·¯ç”±: å†³ç­–ä¸‹ä¸€æ­¥èµ°å‘
    è¯„ä¼°æ”¶é›†çš„ç ”ç©¶æ•°æ®æ˜¯å¦è¶³å¤Ÿæ’°å†™é«˜è´¨é‡æ–‡æ¡£
    Args:
        state: ç ”ç©¶çŠ¶æ€ï¼ŒåŒ…å« topic å’Œ gathered_data
        llm_client: LLMå®¢æˆ·ç«¯å®ä¾‹
    Returns:
        str: "continue_to_writer" å¦‚æœæ•°æ®å……è¶³ï¼Œ"rerun_researcher" å¦‚æœéœ€è¦æ›´å¤šç ”ç©¶
    """
    logger.info("ğŸš€ ====== è¿›å…¥ supervisor_router è·¯ç”±èŠ‚ç‚¹ ======")

    # 1. ä»çŠ¶æ€ä¸­æå– topic å’Œ gathered_data
    topic = state.get("topic", "")
    gathered_data = state.get("gathered_data", "")

    if not topic:
        # å¦‚æœæ²¡æœ‰ä¸»é¢˜ï¼Œé»˜è®¤éœ€è¦é‡æ–°ç ”ç©¶
        logger.warning("âŒ æ²¡æœ‰ä¸»é¢˜ï¼Œè¿”å› rerun_researcher")
        return "rerun_researcher"

    if not gathered_data:
        # å¦‚æœæ²¡æœ‰æ”¶é›†åˆ°æ•°æ®ï¼Œéœ€è¦é‡æ–°ç ”ç©¶
        logger.warning("âŒ æ²¡æœ‰æ”¶é›†åˆ°æ•°æ®ï¼Œè¿”å› rerun_researcher")
        return "rerun_researcher"

    # 2. é¢„åˆ†ææ­¥éª¤ï¼šè®¡ç®—å…ƒæ•°æ®
    # è®¡ç®—æ¥æºæ•°é‡ï¼ˆé€šè¿‡ "===" åˆ†éš”ç¬¦è®¡æ•°ï¼‰
    num_sources = gathered_data.count("===")
    total_length = len(gathered_data)

    logger.info(f"ğŸ“‹ Topic: {topic}")
    logger.info(f"ğŸ“Š Gathered data é•¿åº¦: {total_length} å­—ç¬¦")
    logger.info(f"ğŸ” æ¥æºæ•°é‡: {num_sources}")

    # å¯¼å…¥æç¤ºè¯æ¨¡æ¿
    from ...prompts import SUPERVISOR_PROMPT

    # 3. æ„å»ºç®€åŒ–çš„è¯„ä¼°æç¤ºè¯
    prompt = SUPERVISOR_PROMPT.format(topic=topic,
                                      num_sources=num_sources,
                                      total_length=total_length)

    logger.debug(
        f"Invoking LLM with supervisor prompt:\n{pprint.pformat(prompt)}")

    try:
        # 4. è°ƒç”¨ LLM å®¢æˆ·ç«¯
        # ä½¿ç”¨å°çš„ max_tokensï¼Œå› ä¸ºæœŸæœ›çš„è¾“å‡ºå¾ˆçŸ­
        max_tokens = 10

        logger.info("ğŸ¤– è°ƒç”¨ LLM è¿›è¡Œå†³ç­–åˆ¤æ–­...")
        logger.debug(f"ğŸ“ Prompt é•¿åº¦: {len(prompt)} å­—ç¬¦")
        logger.debug(f"ğŸ”§ å‚æ•°: max_tokens={max_tokens}, temperature=0")

        # æ·»åŠ é‡è¯•æœºåˆ¶
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = llm_client.invoke(prompt,
                                             temperature=0,
                                             max_tokens=max_tokens)
                break  # æˆåŠŸåˆ™è·³å‡ºé‡è¯•å¾ªç¯
            except Exception as e:
                if "400" in str(e) and attempt < max_retries - 1:
                    logger.warning(
                        f"âš ï¸  ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥ (400é”™è¯¯)ï¼Œæ­£åœ¨é‡è¯•...")
                    import time
                    time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
                    continue
                else:
                    raise e  # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥æˆ–å…¶ä»–é”™è¯¯ï¼ŒæŠ›å‡ºå¼‚å¸¸

                # 4. è§£æå“åº” - ç®€åŒ–å¤„ç†é€»è¾‘
        # ç›´æ¥æ£€æŸ¥å“åº”ä¸­æ˜¯å¦åŒ…å« "FINISH" æˆ– "CONTINUE"
        decision = response.strip().upper()
        clean_response = response  # åˆå§‹åŒ– clean_response

        # å¦‚æœå“åº”è¢«æˆªæ–­æˆ–åŒ…å«æ¨ç†è¿‡ç¨‹ï¼Œå°è¯•æå–å†³ç­–å…³é”®è¯
        if "FINISH" not in decision and "CONTINUE" not in decision:
            # å°è¯•ä»å“åº”ä¸­æå–ä»»ä½•å¯èƒ½çš„å†³ç­–è¯
            import re
            # ç§»é™¤å¯èƒ½çš„æ¨ç†æ ‡ç­¾
            clean_response = re.sub(r'<think>.*',
                                    '',
                                    response,
                                    flags=re.IGNORECASE)
            clean_response = re.sub(r'<THINK>.*',
                                    '',
                                    clean_response,
                                    flags=re.IGNORECASE)
            decision = clean_response.strip().upper()

        # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
        logger.debug(f"ğŸ” LLMåŸå§‹å“åº”: '{response}'")
        logger.debug(f"ğŸ” å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
        logger.debug(f"ğŸ” æ¸…ç†åå“åº”: '{clean_response}'")
        logger.debug(f"ğŸ” å¤„ç†åå†³ç­–: '{decision}'")
        logger.debug(f"ğŸ” æ˜¯å¦åŒ…å«FINISH: {'FINISH' in decision}")
        logger.debug(f"ğŸ” æ˜¯å¦åŒ…å«CONTINUE: {'CONTINUE' in decision}")

        # 5. æ ¹æ®å“åº”å†³å®šè·¯ç”±
        if "FINISH" in decision:
            logger.info("âœ… å†³ç­–: FINISH -> continue_to_writer")
            return "continue_to_writer"
        else:
            # å¦‚æœåŒ…å« "CONTINUE" æˆ–å…¶ä»–ä»»ä½•å†…å®¹ï¼Œéƒ½éœ€è¦é‡æ–°ç ”ç©¶
            logger.info("âœ… å†³ç­–: CONTINUE/å…¶ä»– -> rerun_researcher")
            return "rerun_researcher"

    except Exception as e:
        # å¦‚æœ LLM è°ƒç”¨å¤±è´¥ï¼Œé»˜è®¤ç»§ç»­ç ”ç©¶ä»¥ç¡®ä¿å®‰å…¨
        logger.error(f"âŒ Supervisor router error: {str(e)}")
        logger.error(f"âŒ é”™è¯¯ç±»å‹: {type(e).__name__}")
        return "rerun_researcher"
