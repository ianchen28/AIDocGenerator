# service/src/doc_agent/graph/router.py
from typing import Literal
from ..state import ResearchState
from ...llm_clients.base import LLMClient


def supervisor_router(
    state: ResearchState, llm_client: LLMClient
) -> Literal["continue_to_writer", "rerun_researcher"]:
    """
    æ¡ä»¶è·¯ç”±: å†³ç­–ä¸‹ä¸€æ­¥èµ°å‘
    
    è¯„ä¼°æ”¶é›†çš„ç ”ç©¶æ•°æ®æ˜¯å¦è¶³å¤Ÿæ’°å†™é«˜è´¨é‡æ–‡æ¡£
    
    Args:
        state: ç ”ç©¶çŠ¶æ€ï¼ŒåŒ…å« topic å’Œ gathered_data
        llm_client: LLMå®¢æˆ·ç«¯å®ä¾‹
        
    Returns:
        str: "continue_to_writer" å¦‚æœæ•°æ®å……è¶³ï¼Œ"rerun_researcher" å¦‚æœéœ€è¦æ›´å¤šç ”ç©¶
    """
    print("ğŸš€ ====== è¿›å…¥ supervisor_router è·¯ç”±èŠ‚ç‚¹ ======")

    # 1. ä»çŠ¶æ€ä¸­æå– topic å’Œ gathered_data
    topic = state.get("topic", "")
    gathered_data = state.get("gathered_data", "")

    # è®¡ç®—æ•°æ®è´¨é‡æŒ‡æ ‡
    total_length = len(gathered_data)
    data_preview = gathered_data[:200] if gathered_data else ""

    # ç®€å•çš„æ•°æ®è´¨é‡è¯„ä¼°
    has_content = bool(gathered_data and gathered_data.strip())
    has_minimum_length = total_length > 100  # è‡³å°‘100å­—ç¬¦
    has_keywords = any(keyword in gathered_data.lower()
                       for keyword in ['äººå·¥æ™ºèƒ½', 'ç”µåŠ›', 'æŠ€æœ¯', 'åº”ç”¨', 'æ”¿ç­–', 'å‘å±•'])

    print(f"ğŸ“‹ Topic: {topic}")
    print(f"ğŸ“Š Gathered data é•¿åº¦: {total_length} å­—ç¬¦")
    print(f"ğŸ“ Gathered data é¢„è§ˆ: {data_preview}...")
    print(
        f"ğŸ” æ•°æ®è´¨é‡è¯„ä¼°: æœ‰å†…å®¹={has_content}, é•¿åº¦è¶³å¤Ÿ={has_minimum_length}, åŒ…å«å…³é”®è¯={has_keywords}"
    )

    if not topic:
        # å¦‚æœæ²¡æœ‰ä¸»é¢˜ï¼Œé»˜è®¤éœ€è¦é‡æ–°ç ”ç©¶
        print("âŒ æ²¡æœ‰ä¸»é¢˜ï¼Œè¿”å› rerun_researcher")
        return "rerun_researcher"

    if not gathered_data:
        # å¦‚æœæ²¡æœ‰æ”¶é›†åˆ°æ•°æ®ï¼Œéœ€è¦é‡æ–°ç ”ç©¶
        print("âŒ æ²¡æœ‰æ”¶é›†åˆ°æ•°æ®ï¼Œè¿”å› rerun_researcher")
        return "rerun_researcher"

    # å¿«é€Ÿåˆ¤æ–­ï¼šå¦‚æœæ•°æ®æ˜æ˜¾ä¸è¶³ï¼Œç›´æ¥è¿”å› CONTINUE
    if not has_content or not has_minimum_length or not has_keywords:
        print("âš ï¸  æ•°æ®è´¨é‡æ˜æ˜¾ä¸è¶³ï¼Œç›´æ¥è¿”å› rerun_researcher")
        print(f"   - æœ‰å†…å®¹: {has_content}")
        print(f"   - é•¿åº¦è¶³å¤Ÿ: {has_minimum_length}")
        print(f"   - åŒ…å«å…³é”®è¯: {has_keywords}")
        return "rerun_researcher"

    # 2. æ„å»ºæ™ºèƒ½è¯„ä¼°æç¤ºè¯
    prompt = f"""
**ä»»åŠ¡ï¼š** è¯„ä¼°æ”¶é›†çš„ç ”ç©¶æ•°æ®æ˜¯å¦è¶³å¤Ÿæ’°å†™å…³äºã€Œ{topic}ã€çš„é«˜è´¨é‡æ–‡æ¡£ã€‚

**æ•°æ®è´¨é‡æŒ‡æ ‡ï¼š**
- æ•°æ®é•¿åº¦: {total_length} å­—ç¬¦
- æœ‰å®é™…å†…å®¹: {'æ˜¯' if has_content else 'å¦'}
- é•¿åº¦è¶³å¤Ÿ(>100å­—ç¬¦): {'æ˜¯' if has_minimum_length else 'å¦'}
- åŒ…å«ç›¸å…³å…³é”®è¯: {'æ˜¯' if has_keywords else 'å¦'}

**æ•°æ®é¢„è§ˆï¼š**
{gathered_data[:1000] if gathered_data else 'æ— æ•°æ®'}

**è¯„ä¼°æ ‡å‡†ï¼š**
1. æ•°æ®å¿…é¡»åŒ…å«å®é™…å†…å®¹ï¼ˆä¸æ˜¯ç©ºå­—ç¬¦ä¸²æˆ–é”™è¯¯ä¿¡æ¯ï¼‰
2. æ•°æ®é•¿åº¦åº”è¯¥è¶…è¿‡100å­—ç¬¦
3. æ•°æ®åº”è¯¥åŒ…å«ä¸ä¸»é¢˜ç›¸å…³çš„å…³é”®è¯
4. æ•°æ®å†…å®¹åº”è¯¥ä¸ä¸»é¢˜ã€Œ{topic}ã€ç›¸å…³

**å†³ç­–è§„åˆ™ï¼š**
- å¦‚æœæ•°æ®è´¨é‡è‰¯å¥½ï¼ˆæœ‰å†…å®¹ã€é•¿åº¦è¶³å¤Ÿã€åŒ…å«å…³é”®è¯ï¼‰ï¼Œè¿”å› "FINISH"
- å¦‚æœæ•°æ®è´¨é‡ä¸è¶³ï¼ˆæ— å†…å®¹ã€é•¿åº¦ä¸å¤Ÿã€ä¸åŒ…å«å…³é”®è¯ï¼‰ï¼Œè¿”å› "CONTINUE"

è¯·æ ¹æ®ä¸Šè¿°æ ‡å‡†ä¸¥æ ¼åˆ¤æ–­ï¼Œåªèƒ½å›ç­”ä¸€ä¸ªå•è¯ï¼šFINISH æˆ– CONTINUE
"""

    try:
        # 3. è°ƒç”¨ LLM å®¢æˆ·ç«¯
        # æ ¹æ®æ¨¡å‹ç±»å‹è®¾ç½®åˆé€‚çš„max_tokens
        max_tokens = 10
        if hasattr(llm_client, 'reasoning') and llm_client.reasoning:
            # å¦‚æœæ”¯æŒæ¨ç†æ¨¡å¼ï¼Œä½¿ç”¨æ›´å¤§çš„tokené™åˆ¶
            max_tokens = 1000
        if hasattr(llm_client,
                   'model_name') and 'gemini' in llm_client.model_name.lower():
            max_tokens = 5000

        # é™åˆ¶ prompt é•¿åº¦ï¼Œé¿å…è¶…è¿‡æ¨¡å‹è¾“å…¥é™åˆ¶
        max_prompt_length = 30000  # 30K å­—ç¬¦é™åˆ¶
        if len(prompt) > max_prompt_length:
            print(f"âš ï¸  Prompt é•¿åº¦ {len(prompt)} è¶…è¿‡é™åˆ¶ {max_prompt_length}ï¼Œè¿›è¡Œæˆªæ–­")
            # ä¿ç•™å¼€å¤´å’Œç»“å°¾çš„é‡è¦ä¿¡æ¯ï¼Œæˆªæ–­ä¸­é—´çš„ç ”ç©¶èµ„æ–™
            header = prompt[:prompt.find("**å·²æ”¶é›†çš„ç ”ç©¶èµ„æ–™ï¼š**") +
                            len("**å·²æ”¶é›†çš„ç ”ç©¶èµ„æ–™ï¼š**")]
            footer = prompt[prompt.find("**è¯„åˆ¤æ ‡å‡†ï¼ˆå®½æ¾ç‰ˆï¼‰ï¼š**"):]
            # ä» gathered_data ä¸­å–å‰ 10000 å­—ç¬¦
            gathered_data_preview = gathered_data[:10000] + "\n\n... (å†…å®¹å·²æˆªæ–­ï¼Œä¿ç•™å‰10000å­—ç¬¦)"
            prompt = header + "\n\n" + gathered_data_preview + "\n\n" + footer
            print(f"ğŸ“ æˆªæ–­å prompt é•¿åº¦: {len(prompt)} å­—ç¬¦")

        print(f"ğŸ¤– è°ƒç”¨ LLM è¿›è¡Œå†³ç­–åˆ¤æ–­...")
        print(f"ğŸ“ Prompt é•¿åº¦: {len(prompt)} å­—ç¬¦")
        print(f"ğŸ”§ å‚æ•°: max_tokens={max_tokens}, temperature=0")

        # æ·»åŠ é‡è¯•æœºåˆ¶
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = llm_client.invoke(prompt,
                                             temperature=0,
                                             max_tokens=max_tokens)
                break  # æˆåŠŸåˆ™è·³å‡ºé‡è¯•å¾ªç¯
            except Exception as e:
                if "400" in str(e) and attempt < max_retries - 1:
                    print(f"âš ï¸  ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥ (400é”™è¯¯)ï¼Œæ­£åœ¨é‡è¯•...")
                    import time
                    time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
                    continue
                else:
                    raise e  # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥æˆ–å…¶ä»–é”™è¯¯ï¼ŒæŠ›å‡ºå¼‚å¸¸

        # 4. è§£æå“åº” - ä½¿ç”¨ strip() å’Œ upper() å¤„ç†ç©ºç™½å’Œå¤§å°å†™
        decision = response.strip().upper()

        # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
        print(f"ğŸ” LLMåŸå§‹å“åº”: '{response}'")
        print(f"ğŸ” å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
        print(f"ğŸ” å¤„ç†åå†³ç­–: '{decision}'")
        print(f"ğŸ” æ˜¯å¦åŒ…å«FINISH: {'FINISH' in decision}")
        print(f"ğŸ” æ˜¯å¦åŒ…å«CONTINUE: {'CONTINUE' in decision}")

        # 5. æ ¹æ®å“åº”å†³å®šè·¯ç”±
        if "FINISH" in decision:
            print("âœ… å†³ç­–: FINISH -> continue_to_writer")
            return "continue_to_writer"
        else:
            # å¦‚æœåŒ…å« "CONTINUE" æˆ–å…¶ä»–ä»»ä½•å†…å®¹ï¼Œéƒ½éœ€è¦é‡æ–°ç ”ç©¶
            print("âœ… å†³ç­–: CONTINUE/å…¶ä»– -> rerun_researcher")
            return "rerun_researcher"

    except Exception as e:
        # å¦‚æœ LLM è°ƒç”¨å¤±è´¥ï¼Œé»˜è®¤ç»§ç»­ç ”ç©¶ä»¥ç¡®ä¿å®‰å…¨
        print(f"âŒ Supervisor router error: {str(e)}")
        print(f"âŒ é”™è¯¯ç±»å‹: {type(e).__name__}")
        return "rerun_researcher"
