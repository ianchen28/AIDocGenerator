# service/examples/test_decoupled_workflow_pro.py

import asyncio
import json
import os
import pprint
import sys
import uuid
from datetime import datetime

from loguru import logger

# --- è·¯å¾„è®¾ç½® ---
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_dir, '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# --- å¯¼å…¥æ ¸å¿ƒç»„ä»¶ ---
from service.core.config import settings
from service.core.logging_config import setup_logging

# --- ç«‹å³è®¾ç½®æ—¥å¿—é…ç½®ï¼Œé¿å…åç»­åˆå§‹åŒ–æ—¶çš„æ ¼å¼é”™è¯¯ ---
setup_logging(settings)

from service.core.container import container
from service.src.doc_agent.graph.state import ResearchState

# --- æ¨¡æ‹Ÿçš„ä¸Šä¼ æ–‡ä»¶å†…å®¹ (ä¿æŒä¸å˜) ---
STYLE_GUIDE_CONTENT = """
åŒå¿—ä»¬ï¼Œæœ‹å‹ä»¬ï¼ä»Šå¤©æˆ‘ä»¬æ±‡èšä¸€å ‚ï¼Œæ ¸å¿ƒè®®é¢˜æ˜¯åˆ›æ–°ã€‚åˆ›æ–°æ˜¯å¼•é¢†å‘å±•çš„ç¬¬ä¸€åŠ¨åŠ›ï¼Œæ˜¯å»ºè®¾ç°ä»£åŒ–ç»æµä½“ç³»çš„æˆ˜ç•¥æ”¯æ’‘ã€‚æˆ‘ä»¬å¿…é¡»æŠŠåˆ›æ–°æ‘†åœ¨å›½å®¶å‘å±•å…¨å±€çš„æ ¸å¿ƒä½ç½®ã€‚
"""
REQUIREMENTS_CONTENT = """
- æŠ¥å‘Šå¿…é¡»é¦–å…ˆå®šä¹‰ä»€ä¹ˆæ˜¯"å¯è§‚æµ‹æ€§"ã€‚
- å¿…é¡»åŒ…å«ä¸€ä¸ªå…³äº OpenTelemetry æœªæ¥å‘å±•è¶‹åŠ¿çš„ç« èŠ‚ã€‚
- ç»“è®ºéƒ¨åˆ†å¿…é¡»ä¸ºä¸åŒè§„æ¨¡çš„ä¼ä¸šæä¾›æ˜ç¡®çš„æŠ€æœ¯é€‰å‹å»ºè®®ã€‚
"""


async def run_stage_one_outline_generation(
        initial_state: ResearchState) -> dict:
    # ... (æ­¤å‡½æ•°å†…å®¹ä¿æŒä¸å˜) ...
    logger.info("ğŸš€ğŸš€ğŸš€ STAGE 1: Starting Outline Generation Workflow ğŸš€ğŸš€ğŸš€")
    outline_result = None
    try:
        async for step_output in container.outline_graph.astream(
                initial_state):
            node_name = list(step_output.keys())[0]
            logger.info(f"âœ… [Stage 1] Finished step: [ {node_name} ]")
            outline_result = list(step_output.values())[0]
    except Exception as e:
        logger.error(f"âŒ [Stage 1] Error during outline generation: {e}",
                     exception=e)
        return None
    logger.success("âœ…âœ…âœ… STAGE 1: Outline Generation Complete! âœ…âœ…âœ…\n")
    return outline_result.get("document_outline")


async def run_stage_two_document_generation(
        initial_state: ResearchState) -> dict:
    # ... (æ­¤å‡½æ•°å†…å®¹ä¿®æ”¹ä¸ºè¿”å›æœ€ç»ˆçŠ¶æ€) ...
    logger.info("ğŸš€ğŸš€ğŸš€ STAGE 2: Starting Document Generation Workflow ğŸš€ğŸš€ğŸš€")
    final_result_state = None
    try:
        async for step_output in container.document_graph.astream(
                initial_state):
            node_name = list(step_output.keys())[0]
            logger.info(f"âœ… [Stage 2] Finished step: [ {node_name} ]")
            final_result_state = list(step_output.values())[0]
    except Exception as e:
        logger.error(f"âŒ [Stage 2] Error during document generation: {e}",
                     exception=e)
        return None
    logger.success("âœ…âœ…âœ… STAGE 2: Document Generation Complete! âœ…âœ…âœ…\n")
    return final_result_state


async def main():
    """
    ä¸»æ‰§è¡Œå‡½æ•°ï¼Œä¸²è”ä¸¤ä¸ªç‹¬ç«‹çš„æµ‹è¯•é˜¶æ®µï¼Œå¹¶ä¿å­˜æ‰€æœ‰äº§å‡ºã€‚
    """
    # --- 1. ã€æ–°å¢ã€‘è®¾ç½®è¾“å‡ºè·¯å¾„å’Œæ—¥å¿— ---
    run_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = os.path.join(project_root, "output")
    os.makedirs(output_dir, exist_ok=True)

    log_file_path = os.path.join(output_dir,
                                 f"workflow_test_{run_timestamp}.log")

    # æ·»åŠ é¢å¤–çš„æ—¥å¿—æ–‡ä»¶è¾“å‡º
    logger.add(log_file_path, level="DEBUG",
               serialize=True)  # ä½¿ç”¨ serialize=True å¯ä»¥è®©æ—¥å¿—æ–‡ä»¶æ˜¯ JSON æ ¼å¼ï¼Œä¾¿äºæœºå™¨åˆ†æ

    # --- 1.5. ã€æ–°å¢ã€‘ç”Ÿæˆ run_id å¹¶ç»‘å®šåˆ°æ—¥å¿—ä¸Šä¸‹æ–‡ ---
    run_id = f"run-{uuid.uuid4().hex[:8]}"
    logger.info(
        f"ğŸ“ All outputs for this run will be saved with timestamp: {run_timestamp}"
    )
    logger.info(f"ğŸ†” Generated run_id: {run_id}")

    # ç»‘å®š run_id åˆ°æ—¥å¿—ä¸Šä¸‹æ–‡
    with logger.contextualize(run_id=run_id):
        logger.info("ğŸš€ Starting decoupled workflow test with context tracking")

        # --- 2. å‡†å¤‡ç¬¬ä¸€é˜¶æ®µçš„è¾“å…¥ (ä¿æŒä¸å˜) ---
        topic = "ä»¥'å¯è§‚æµ‹æ€§'ä¸ºæ ¸å¿ƒï¼Œå¯¹æ¯”åˆ†æ Prometheus, Zabbix å’Œ OpenTelemetry ä¸‰ç§æŠ€æœ¯æ–¹æ¡ˆåœ¨ç°ä»£äº‘åŸç”Ÿç¯å¢ƒä¸‹çš„ä¼˜ç¼ºç‚¹"
        stage_one_input_state = ResearchState(
            topic=topic,
            style_guide_content=STYLE_GUIDE_CONTENT,
            requirements_content=REQUIREMENTS_CONTENT,
            # ... å…¶ä»–å­—æ®µ ...
            initial_sources=[],
            document_outline={},
            chapters_to_process=[],
            current_chapter_index=0,
            completed_chapters=[],
            final_document="",
            messages=[],
            run_id=run_id,  # ã€æ–°å¢ã€‘æ·»åŠ  run_id åˆ°çŠ¶æ€
        )

        # --- 3. æ‰§è¡Œç¬¬ä¸€é˜¶æ®µ (ä¿æŒä¸å˜) ---
        generated_outline = await run_stage_one_outline_generation(
            stage_one_input_state)
        if not generated_outline:
            logger.error("Aborting test due to failure in Stage 1.")
            return

        logger.info("ğŸ“‹ Generated Outline for Stage 2:")
        pprint.pprint(generated_outline)
        print("-" * 80)

        # --- 4. å‡†å¤‡ç¬¬äºŒé˜¶æ®µçš„è¾“å…¥ (ä¿æŒä¸å˜) ---
        stage_two_input_state = ResearchState(
            topic=topic,
            document_outline=generated_outline,
            style_guide_content=STYLE_GUIDE_CONTENT,
            # ... å…¶ä»–å­—æ®µ ...
            initial_sources=[],
            requirements_content="",
            chapters_to_process=[],
            current_chapter_index=0,
            completed_chapters=[],
            final_document="",
            messages=[],
            run_id=run_id,  # ã€æ–°å¢ã€‘æ·»åŠ  run_id åˆ°çŠ¶æ€
        )

        # --- 5. æ‰§è¡Œç¬¬äºŒé˜¶æ®µ ---
        final_state = await run_stage_two_document_generation(
            stage_two_input_state)

        if not final_state:
            logger.error("Aborting test due to failure in Stage 2.")
            return

        # --- 6. ã€æ–°å¢ã€‘ä¿å­˜æ‰€æœ‰äº§å‡ºæ–‡ä»¶ ---
        logger.info("ğŸ’¾ Saving all workflow outputs...")

        # ä¿å­˜æœ€ç»ˆçŠ¶æ€
        state_file_path = os.path.join(output_dir,
                                       f"workflow_state_{run_timestamp}.json")
        try:
            with open(state_file_path, 'w', encoding='utf-8') as f:
                # Pydantic/TypedDict ä¸èƒ½ç›´æ¥ json.dumpï¼Œéœ€è¦å…ˆè½¬ä¸ºæ™®é€š dict
                # æˆ‘ä»¬ç®€å•åœ°æ‹·è´ä¸€ä¸‹
                serializable_state = dict(final_state)
                json.dump(serializable_state, f, ensure_ascii=False, indent=4)
            logger.success(
                f"   - Full final state saved to: {state_file_path}")
        except Exception as e:
            logger.error(f"   - Failed to save state file: {e}")

        # ä¿å­˜æœ€ç»ˆæ–‡æ¡£
        document_file_path = os.path.join(
            output_dir, f"final_document_{run_timestamp}.md")
        final_document_content = final_state.get("final_document", "")
        try:
            with open(document_file_path, 'w', encoding='utf-8') as f:
                f.write(final_document_content)
            logger.success(
                f"   - Final document saved to: {document_file_path}")
        except Exception as e:
            logger.error(f"   - Failed to save document file: {e}")

        # --- 7. æ‰“å°æœ€ç»ˆæ€»ç»“ ---
        print("\n\n" + "=" * 80)
        logger.success("ğŸ‰ End-to-End Test with Output Saving COMPLETED! ğŸ‰")
        print("=" * 80)
        print("ğŸ“ Output files:")
        print(f"  - ğŸ“ Log: {log_file_path}")
        print(f"  - ğŸ“Š State: {state_file_path}")
        print(f"  - ğŸ“„ Document: {document_file_path}")
        print("=" * 80)


if __name__ == "__main__":
    asyncio.run(main())
