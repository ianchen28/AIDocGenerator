#!/usr/bin/env python3
"""
è§£ç Redisæµä¸­çš„ä¸­æ–‡æ•°æ®
"""

import json
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

import redis
from loguru import logger


def decode_redis_stream_data(job_id: str):
    """è§£ç Redisæµä¸­çš„ä¸­æ–‡æ•°æ®"""
    logger.info(f"ğŸ” è§£ç Redisæµæ•°æ®: {job_id}")

    # è¿æ¥è¿œç¨‹Redis
    r = redis.Redis(host='10.215.149.74',
                    port=26379,
                    password='xJrhp*4mnHxbBWN2grqq',
                    decode_responses=True)

    stream_key = f"job_events:{job_id}"

    try:
        # è·å–æµé•¿åº¦
        stream_length = r.xlen(stream_key)
        logger.info(f"ğŸ“Š æµé•¿åº¦: {stream_length}")

        if stream_length == 0:
            logger.warning("âš ï¸ æµä¸­æ— æ•°æ®")
            return

        # è·å–æ‰€æœ‰æ¶ˆæ¯
        messages = r.xrange(stream_key, "-", "+")

        logger.info(f"ğŸ“‹ è§£ç åçš„æµå†…å®¹:")
        logger.info("=" * 80)

        for msg_id, fields in messages:
            logger.info(f"æ¶ˆæ¯ID: {msg_id}")

            for field, value in fields.items():
                if field == "data":
                    try:
                        # è§£æJSONæ•°æ®
                        data = json.loads(value)
                        logger.info(f"  {field}:")
                        logger.info(
                            f"    {json.dumps(data, ensure_ascii=False, indent=2)}"
                        )
                    except json.JSONDecodeError:
                        logger.info(f"  {field}: {value}")
                else:
                    logger.info(f"  {field}: {value}")

            logger.info("-" * 40)

    except Exception as e:
        logger.error(f"âŒ è§£ç å¤±è´¥: {e}")
    finally:
        r.close()


def decode_specific_message(job_id: str, message_id: str):
    """è§£ç ç‰¹å®šçš„æ¶ˆæ¯"""
    logger.info(f"ğŸ” è§£ç ç‰¹å®šæ¶ˆæ¯: {job_id} -> {message_id}")

    r = redis.Redis(host='10.215.149.74',
                    port=26379,
                    password='xJrhp*4mnHxbBWN2grqq',
                    decode_responses=True)

    stream_key = f"job_events:{job_id}"

    try:
        # è·å–ç‰¹å®šæ¶ˆæ¯
        messages = r.xrange(stream_key, message_id, message_id)

        if not messages:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°æŒ‡å®šæ¶ˆæ¯")
            return

        msg_id, fields = messages[0]
        logger.info(f"æ¶ˆæ¯ID: {msg_id}")

        for field, value in fields.items():
            if field == "data":
                try:
                    data = json.loads(value)
                    logger.info(f"  {field}:")
                    logger.info(
                        f"    {json.dumps(data, ensure_ascii=False, indent=2)}"
                    )
                except json.JSONDecodeError:
                    logger.info(f"  {field}: {value}")
            else:
                logger.info(f"  {field}: {value}")

    except Exception as e:
        logger.error(f"âŒ è§£ç å¤±è´¥: {e}")
    finally:
        r.close()


if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    logger.remove()
    logger.add(
        sys.stderr,
        format=
        "<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>",
        level="INFO")

    if len(sys.argv) > 1:
        job_id = sys.argv[1]
        if len(sys.argv) > 2:
            message_id = sys.argv[2]
            decode_specific_message(job_id, message_id)
        else:
            decode_redis_stream_data(job_id)
    else:
        # é»˜è®¤è§£ç ä½ æµ‹è¯•çš„job_id
        decode_redis_stream_data("1951106983556190200")
