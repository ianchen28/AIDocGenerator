# service/api/endpoints.py
import json
import asyncio

from fastapi import APIRouter, Depends, HTTPException, status, BackgroundTasks
from fastapi.responses import StreamingResponse
from doc_agent.core.logger import logger

# å¯¼å…¥æˆ‘ä»¬æ–°çš„ã€ç»Ÿä¸€çš„æ•°æ®æ¨¡å‹
from doc_agent.schemas import (
    DocumentGenerationRequest,
    DocumentGenerationFromOutlineRequest,
    EditActionRequest,
    OutlineGenerationRequest,
    TaskCreationResponse,  # å¯¼å…¥ç»Ÿä¸€çš„å“åº”æ¨¡å‹
)

# å¯¼å…¥AIç¼–è¾‘å·¥å…·å’Œä»»åŠ¡IDç”Ÿæˆå™¨
from doc_agent.tools.ai_editing_tool import AIEditingTool
from doc_agent.core.task_id_generator import generate_task_id
# å¯¼å…¥Celeryä»»åŠ¡
from workers import tasks

# å¯¼å…¥è¯·æ±‚/å“åº”æ¨¡å‹ (Schemas)
from doc_agent.schemas import (
    OutlineGenerationRequest,
    DocumentGenerationRequest,
    TaskCreationResponse,
)
# å¯¼å…¥æˆ‘ä»¬æ–°çš„æ ¸å¿ƒé€»è¾‘å‡½æ•°
from doc_agent.core.outline_generator import generate_outline_async
from doc_agent.core.document_generator import generate_document_sync
# å¯¼å…¥ä»»åŠ¡IDç”Ÿæˆå™¨
from doc_agent.core.task_id_generator import generate_task_id

# åˆ›å»ºAPIè·¯ç”±å™¨å®ä¾‹
# router = APIRouter()
router = APIRouter(tags=["Generation Jobs & Tasks"])


def get_ai_editing_tool():
    from doc_agent.core.container import Container
    return Container().ai_editing_tool


# =================================================================
#  æ ¸å¿ƒæ¥å£æ”¹é€  (ä½¿ç”¨ FastAPI BackgroundTasks, ä¸å†ä¾èµ– Celery)
# =================================================================


@router.post("/jobs/outline",
             response_model=TaskCreationResponse,
             status_code=status.HTTP_202_ACCEPTED,
             summary="ä»¥èƒŒæ™¯ä»»åŠ¡å½¢å¼ç”Ÿæˆå¤§çº² (éCelery)")
async def generate_outline_endpoint(request: OutlineGenerationRequest,
                                    background_tasks: BackgroundTasks):
    """
    æ¥æ”¶å¤§çº²ç”Ÿæˆè¯·æ±‚ï¼Œå°†å…¶ä½œä¸ºåå°ä»»åŠ¡è¿è¡Œï¼Œå¹¶ç«‹å³è¿”å›ä»»åŠ¡IDã€‚
    è¯¥æ¥å£ä¸ä½¿ç”¨Celeryï¼Œä»»åŠ¡åœ¨FastAPIåº”ç”¨è¿›ç¨‹çš„åå°æ‰§è¡Œã€‚
    """
    logger.info(f"æ”¶åˆ°å¤§çº²ç”Ÿæˆè¯·æ±‚ï¼Œæ­£åœ¨æ·»åŠ åˆ°åå°ä»»åŠ¡ã€‚SessionId: {request.session_id}")
    job_id = generate_task_id()

    background_tasks.add_task(
        generate_outline_async,
        job_id=str(job_id),
        session_id=request.session_id,
        task_prompt=request.task_prompt,
        is_online=request.is_online,
        context_files=request.context_files,
        style_guide_content=request.style_guide_content,
        requirements=request.requirements,
    )

    logger.success(f"å¤§çº²ç”Ÿæˆä»»åŠ¡ {job_id} å·²æäº¤åˆ°åå°ã€‚")
    return TaskCreationResponse(
        redis_stream_key=str(job_id),
        session_id=request.session_id,
    )


@router.post("/jobs/document-from-outline",
             response_model=TaskCreationResponse,
             status_code=status.HTTP_202_ACCEPTED,
             summary="ä»å¤§çº²ç”Ÿæˆæ–‡æ¡£çš„èƒŒæ™¯ä»»åŠ¡ (éCelery)")
async def generate_document_endpoint(request: DocumentGenerationRequest,
                                     background_tasks: BackgroundTasks):
    """
    æ¥æ”¶æ–‡æ¡£ç”Ÿæˆè¯·æ±‚ï¼Œå°†å…¶ä½œä¸ºåå°ä»»åŠ¡è¿è¡Œï¼Œå¹¶ç«‹å³è¿”å›ä»»åŠ¡IDã€‚
    è¯¥æ¥å£ä¸ä½¿ç”¨Celeryï¼Œä»»åŠ¡åœ¨FastAPIåº”ç”¨è¿›ç¨‹çš„åå°æ‰§è¡Œã€‚
    """
    logger.info(f"æ”¶åˆ°æ–‡æ¡£ç”Ÿæˆè¯·æ±‚ï¼Œæ­£åœ¨æ·»åŠ åˆ°åå°ä»»åŠ¡ã€‚JobId: {request.session_id}")
    task_id = generate_task_id()
    session_id = request.session_id
    task_prompt = request.task_prompt
    outline_json_file = request.outline
    context_files = request.context_files
    is_online = request.is_online

    background_tasks.add_task(generate_document_sync,
                              task_id=str(task_id),
                              task_prompt=task_prompt,
                              session_id=session_id,
                              outline_json_file=outline_json_file,
                              context_files=context_files,
                              is_online=is_online)

    logger.success(f"æ–‡æ¡£ç”Ÿæˆä»»åŠ¡ {task_id} å·²æäº¤åˆ°åå°ã€‚")
    return TaskCreationResponse(
        redis_stream_key=str(task_id),
        session_id=session_id,
    )


# @router.post(
#     "/jobs/outline",
#     response_model=TaskCreationResponse,  # ä½¿ç”¨ç»Ÿä¸€æ¨¡å‹
#     response_model_by_alias=True,  # å¼ºåˆ¶æŒ‰åˆ«åè¾“å‡º
#     status_code=status.HTTP_202_ACCEPTED)
# async def generate_outline_from_query(request: OutlineGenerationRequest):
#     logger.info(f"æ”¶åˆ°å¤§çº²ç”Ÿæˆè¯·æ±‚ï¼ŒsessionId: {request.session_id}")
#     task_id = generate_task_id()
#     try:
#         tasks.generate_outline_from_query_task.delay(
#             job_id=task_id,
#             task_prompt=request.task_prompt,
#             is_online=request.is_online,
#             context_files=request.context_files,
#             redis_stream_key=task_id,  # ä¼ é€’ç»™ worker
#         )
#         logger.success(f"å¤§çº²ç”Ÿæˆä»»åŠ¡å·²æäº¤ï¼ŒTask ID: {task_id}")
#         return TaskCreationResponse(
#             redis_stream_key=task_id,
#             session_id=request.session_id,
#         )
#     except Exception as e:
#         logger.error(f"æäº¤å¤§çº²ç”Ÿæˆä»»åŠ¡å¤±è´¥: {e}")
#         raise HTTPException(status_code=500, detail="ä»»åŠ¡æäº¤å¤±è´¥")

# @router.post("/jobs/document-from-outline",
#              response_model=TaskCreationResponse,
#              response_model_by_alias=True,
#              status_code=status.HTTP_202_ACCEPTED)
# async def generate_document_from_outline_json(
#     request: DocumentGenerationFromOutlineRequest, ):
#     logger.info(f"ğŸ“¥ æ”¶åˆ°ä»outline JSONç”Ÿæˆæ–‡æ¡£è¯·æ±‚ï¼ŒjobId: {request.job_id}")
#     logger.info(
#         f"ğŸ“‹ è¯·æ±‚è¯¦æƒ…: sessionId={request.session_id}, outlineé•¿åº¦={len(request.outline_json)}"
#     )

#     task_id = generate_task_id()
#     logger.info(f"ğŸ†” ç”Ÿæˆä»»åŠ¡ID: {task_id}")

#     try:
#         # è¿™ä¸€æ­¥ä¿æŒä¸å˜ï¼Œå› ä¸ºè¯·æ±‚ä½“ä¸­ outline_json æœ¬èº«å°±æ˜¯å­—ç¬¦ä¸²
#         logger.info("ğŸš€ æäº¤Celeryä»»åŠ¡...")
#         result = tasks.generate_document_from_outline_task.delay(
#             job_id=task_id,
#             # ç›´æ¥ä¼ é€’è¯·æ±‚ä¸­çš„ JSON å­—ç¬¦ä¸²
#             outline_json=request.outline_json,
#             session_id=request.session_id,
#         )

#         logger.info(f"ğŸ” Celeryä»»åŠ¡å‘é€ç»“æœ: {result.id}")
#         logger.success(f"âœ… æ–‡æ¡£ç”Ÿæˆä»»åŠ¡å·²æäº¤ï¼ŒTask ID: {task_id}")

#         response_object = TaskCreationResponse(
#             redis_stream_key=task_id,
#             session_id=request.session_id,
#         )
#         logger.info(f"ğŸ“¤ è¿”å›å“åº”: {response_object}")
#         return response_object

#     except Exception as e:
#         logger.error(f"âŒ æäº¤æ–‡æ¡£ç”Ÿæˆä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
#         raise HTTPException(status_code=500, detail="ä»»åŠ¡æäº¤å¤±è´¥")


@router.post(
    "/jobs/document-from-outline-mock",
    response_model=TaskCreationResponse,  # ä½¿ç”¨ç»Ÿä¸€æ¨¡å‹
    response_model_by_alias=True,  # å¼ºåˆ¶æŒ‰åˆ«åè¾“å‡º
    status_code=status.HTTP_202_ACCEPTED)
async def generate_document_from_outline_json_mock(
    request: DocumentGenerationRequest, ):
    logger.info(f"æ”¶åˆ°æ¨¡æ‹Ÿæ–‡æ¡£ç”Ÿæˆè¯·æ±‚ï¼ŒsessionId: {request.session_id}")
    task_id = generate_task_id()
    try:
        # å¯åŠ¨ä¸€ä¸ªä¸ä¼šé˜»å¡ä¸»çº¿ç¨‹çš„åå°æ¨¡æ‹Ÿä»»åŠ¡
        asyncio.create_task(
            simulate_mock_generation_process(task_id, request.session_id))
        logger.success(f"æ¨¡æ‹Ÿåå°ä»»åŠ¡å·²å¯åŠ¨ï¼ŒTask ID: {task_id}")
        # ç«‹å³è¿”å›å“åº”
        return TaskCreationResponse(redis_stream_key=task_id,
                                    session_id=request.session_id)
    except Exception as e:
        logger.error(f"æ¨¡æ‹Ÿæ–‡æ¡£ç”Ÿæˆè¯·æ±‚å¤„ç†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ¨¡æ‹Ÿè¯·æ±‚å¤„ç†å¤±è´¥: {str(e)}")


# =================================================================
# æ¨¡æ‹ŸæœåŠ¡ç«¯ç‚¹ - ç”¨äºæµ‹è¯•
# =================================================================


async def simulate_mock_generation_process(task_id: str, session_id: str):
    """ä¸€ä¸ªå®Œæ•´çš„æ¨¡æ‹Ÿåå°ä»»åŠ¡ï¼ŒåŒ…æ‹¬ Redis æµå‘å¸ƒ"""
    logger.info(
        f"[MOCK] Task {task_id} for session {session_id} has started in the background."
    )

    try:
        # è·å– Redis å®¢æˆ·ç«¯
        import redis.asyncio as redis
        from doc_agent.core.config import settings

        redis_client = redis.from_url(settings.redis_url,
                                      encoding="utf-8",
                                      decode_responses=True)
        await redis_client.ping()
        logger.info("æ¨¡æ‹ŸæœåŠ¡Rediså®¢æˆ·ç«¯è¿æ¥æˆåŠŸ")

        # å‘å¸ƒä»»åŠ¡å¼€å§‹äº‹ä»¶
        await publish_mock_event(
            redis_client, task_id, {
                "eventType": "task_started",
                "taskType": "document_generation",
                "status": "started",
                "outline_title": "äººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•è¶‹åŠ¿"
            })

        # å‘å¸ƒåˆ†æè¿›åº¦
        await publish_mock_event(
            redis_client, task_id, {
                "eventType": "task_progress",
                "taskType": "document_generation",
                "progress": "æ­£åœ¨åˆ†æå¤§çº²ç»“æ„",
                "status": "running",
                "step": "analysis"
            })

        await asyncio.sleep(2)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´

        # æ¨¡æ‹Ÿç« èŠ‚å¤„ç†
        chapters = [{
            "title": "äººå·¥æ™ºèƒ½æ¦‚è¿°",
            "index": 0
        }, {
            "title": "æ ¸å¿ƒæŠ€æœ¯å‘å±•",
            "index": 1
        }, {
            "title": "åº”ç”¨é¢†åŸŸæ‹“å±•",
            "index": 2
        }]

        all_answer_origins = []
        all_web_sources = []

        for chapter in chapters:
            chapter_title = chapter["title"]
            chapter_index = chapter["index"]

            # å‘å¸ƒç« èŠ‚å¼€å§‹äº‹ä»¶
            await publish_mock_event(
                redis_client, task_id, {
                    "eventType": "chapter_started",
                    "taskType": "document_generation",
                    "chapterTitle": chapter_title,
                    "chapterIndex": chapter_index,
                    "totalChapters": len(chapters),
                    "status": "running"
                })

            # æ¨¡æ‹Ÿç« èŠ‚å¤„ç†æ­¥éª¤
            steps = ["planner", "researcher", "supervisor", "writer"]
            for step in steps:
                await publish_mock_event(
                    redis_client, task_id, {
                        "eventType": "chapter_progress",
                        "taskType": "document_generation",
                        "chapterTitle": chapter_title,
                        "step": step,
                        "progress": f"æ­£åœ¨æ‰§è¡Œ{step}æ­¥éª¤",
                        "status": "running"
                    })
                await asyncio.sleep(1)  # æ¨¡æ‹Ÿæ­¥éª¤å¤„ç†æ—¶é—´

            # ç”Ÿæˆç« èŠ‚å†…å®¹
            chapter_content = generate_mock_chapter_content(
                chapter_title, chapter_index)

            # ç”Ÿæˆç« èŠ‚çš„å¼•ç”¨æº
            chapter_sources = generate_mock_sources(chapter_title,
                                                    chapter_index)
            chapter_web_sources = generate_mock_web_sources(
                chapter_title, chapter_index)

            all_answer_origins.extend(chapter_sources)
            all_web_sources.extend(chapter_web_sources)

            # æµå¼è¾“å‡ºç« èŠ‚å†…å®¹
            await publish_mock_event(
                redis_client, task_id, {
                    "eventType": "writer_started",
                    "taskType": "document_generation",
                    "progress": f"å¼€å§‹ç¼–å†™ç« èŠ‚ {chapter_index + 1}",
                    "status": "running"
                })

            await stream_mock_document_content(redis_client, task_id,
                                               chapter_content)

            await asyncio.sleep(1)  # ç« èŠ‚é—´éš”

        # å‘å¸ƒå‚è€ƒæ–‡çŒ®äº‹ä»¶
        citations_data = {
            "answerOrigins": all_answer_origins,
            "webs": all_web_sources
        }

        await publish_mock_event(
            redis_client, task_id, {
                "eventType": "citations_completed",
                "taskType": "document_generation",
                "citations": citations_data,
                "totalAnswerOrigins": len(all_answer_origins),
                "totalWebSources": len(all_web_sources),
                "status": "completed"
            })

        # å‘å¸ƒä»»åŠ¡å®Œæˆäº‹ä»¶
        await publish_mock_event(
            redis_client, task_id, {
                "eventType": "task_completed",
                "taskType": "document_generation",
                "status": "completed",
                "totalChapters": len(chapters),
                "citations": citations_data
            })

        logger.info(
            f"[MOCK] Task {task_id} for session {session_id} has finished.")

    except Exception as e:
        logger.error(
            f"[MOCK] Task {task_id} for session {session_id} failed: {e}")
        try:
            await publish_mock_event(
                redis_client, task_id, {
                    "eventType": "task_failed",
                    "taskType": "document_generation",
                    "status": "failed",
                    "error": str(e)
                })
        except:
            pass


async def publish_mock_event(redis_client, task_id: str, event_data: dict):
    """å‘å¸ƒæ¨¡æ‹Ÿäº‹ä»¶åˆ°Redis Stream"""
    try:
        from datetime import datetime

        # æ„é€  Stream åç§° - ä½¿ç”¨session_idä½œä¸ºæµåç§°ï¼Œä¸mock_document_service.pyä¿æŒä¸€è‡´
        stream_name = task_id

        # å‘å¸ƒäº‹ä»¶åˆ° Redis Stream - ä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„ID
        counter_key = f"stream_counter:{stream_name}"
        i = await redis_client.incr(counter_key)
        # è®© Redis è‡ªåŠ¨ç”Ÿæˆ IDï¼Œä½†åœ¨äº‹ä»¶æ•°æ®ä¸­åŒ…å« sessionId-idx æ ¼å¼
        session_id_idx = f"{task_id}-{i}"  # äº‹ä»¶æ•°æ®ä¸­çš„æ ¼å¼: sessionId-idx

        # å‡†å¤‡äº‹ä»¶æ•°æ®
        event_data["redisId"] = session_id_idx
        event_data["timestamp"] = datetime.now().isoformat()

        await redis_client.xadd(
            stream_name, {"data": json.dumps(event_data, ensure_ascii=False)},
            id=session_id_idx)
        logger.info(f"æ¨¡æ‹Ÿäº‹ä»¶å‘å¸ƒæˆåŠŸ: {event_data.get('eventType', 'unknown')}")
    except Exception as e:
        logger.error(f"æ¨¡æ‹Ÿäº‹ä»¶å‘å¸ƒå¤±è´¥: {e}")


async def stream_mock_document_content(redis_client,
                                       task_id: str,
                                       content: str,
                                       chunk_size: int = 10):
    """æµå¼è¾“å‡ºæ¨¡æ‹Ÿæ–‡æ¡£å†…å®¹åˆ°Redis Stream"""
    try:
        # å°†å†…å®¹æŒ‰å­—ç¬¦åˆ†å‰²æˆtoken
        tokens = list(content)

        for i in range(0, len(tokens), chunk_size):
            chunk = tokens[i:i + chunk_size]
            chunk_text = ''.join(chunk)

            # å‘å¸ƒå†…å®¹æµäº‹ä»¶
            await publish_mock_event(
                redis_client, task_id, {
                    "eventType": "document_content_stream",
                    "taskType": "document_generation",
                    "content": chunk_text,
                    "tokenIndex": i,
                    "totalTokens": len(tokens),
                    "progress": f"{i + len(chunk)}/{len(tokens)}",
                    "status": "streaming"
                })

            # æ¨¡æ‹Ÿtokenç”Ÿæˆçš„æ—¶é—´é—´éš”
            await asyncio.sleep(0.1)

        # å‘å¸ƒæµå¼è¾“å‡ºå®Œæˆäº‹ä»¶
        await publish_mock_event(
            redis_client, task_id, {
                "eventType": "document_content_completed",
                "taskType": "document_generation",
                "totalTokens": len(tokens),
                "status": "completed"
            })

        logger.info(f"æ¨¡æ‹Ÿæ–‡æ¡£å†…å®¹æµå¼è¾“å‡ºå®Œæˆï¼Œå…± {len(tokens)} ä¸ªtoken")

    except Exception as e:
        logger.error(f"æ¨¡æ‹Ÿæµå¼è¾“å‡ºæ–‡æ¡£å†…å®¹å¤±è´¥: {e}")


def generate_mock_chapter_content(chapter_title: str,
                                  chapter_index: int) -> str:
    """ç”Ÿæˆæ¨¡æ‹Ÿçš„ç« èŠ‚å†…å®¹"""
    return f"""## {chapter_title}

è¿™æ˜¯ç¬¬ {chapter_index + 1} ç« çš„æ¨¡æ‹Ÿå†…å®¹ã€‚

### ç« èŠ‚æ¦‚è¿°
æœ¬ç« ä¸»è¦ä»‹ç»äº† {chapter_title} çš„ç›¸å…³å†…å®¹ã€‚

### ä¸»è¦å†…å®¹
1. **èƒŒæ™¯ä»‹ç»**
   - å†å²å‘å±•è„‰ç»œ
   - å½“å‰ç°çŠ¶åˆ†æ

2. **æ ¸å¿ƒè¦ç‚¹**
   - å…³é”®æŠ€æœ¯è¦ç´ 
   - é‡è¦å½±å“å› ç´ 

3. **æ€»ç»“**
é€šè¿‡å¯¹ {chapter_title} çš„å…¨é¢åˆ†æï¼Œæˆ‘ä»¬å‘ç°è¿™ä¸€é¢†åŸŸå…·æœ‰é‡è¦çš„ç ”ç©¶ä»·å€¼ã€‚

### å‚è€ƒæ–‡çŒ®
1. å¼ ä¸‰, æå››. ã€Š{chapter_title}ç ”ç©¶ç»¼è¿°ã€‹. å­¦æœ¯æœŸåˆŠ, 2023.
2. ç‹äº”, èµµå…­. ã€Š{chapter_title}å‘å±•è¶‹åŠ¿åˆ†æã€‹. ç ”ç©¶æŠ¥å‘Š, 2023.
"""


def generate_mock_sources(chapter_title: str, chapter_index: int) -> list:
    """ç”Ÿæˆæ¨¡æ‹Ÿçš„å¼•ç”¨æºåˆ—è¡¨"""
    import time

    sources = []

    # ç”Ÿæˆæ–‡æ¡£ç±»å‹çš„å¼•ç”¨æº
    sources.append({
        "id":
        f"{chapter_index * 3 + 1}",
        "detailId":
        f"{chapter_index * 3 + 1}",
        "originInfo":
        f"å…³äº{chapter_title}çš„é‡è¦ç ”ç©¶æˆæœå’Œæœ€æ–°å‘ç°ã€‚æœ¬æ–‡ä¸»è¦å‚è€ƒäº†ç›¸å…³é¢†åŸŸçš„ç ”ç©¶æ–‡çŒ®ï¼ŒåŒ…æ‹¬ç†è®ºåŸºç¡€ã€å®è¯ç ”ç©¶å’Œåº”ç”¨æ¡ˆä¾‹ã€‚",
        "title":
        f"{chapter_title}ç ”ç©¶ç»¼è¿°.pdf",
        "fileToken":
        f"token_{chapter_index * 3 + 1}_{int(time.time())}",
        "domainId":
        "document",
        "isFeishuSource":
        None,
        "valid":
        "true",
        "metadata":
        json.dumps(
            {
                "file_name": f"{chapter_title}ç ”ç©¶ç»¼è¿°.pdf",
                "locations": [{
                    "pagenum": chapter_index + 1
                }],
                "source": "data_platform"
            },
            ensure_ascii=False)
    })

    # ç”Ÿæˆæ ‡å‡†ç±»å‹çš„å¼•ç”¨æº
    sources.append({
        "id":
        f"{chapter_index * 3 + 2}",
        "detailId":
        f"{chapter_index * 3 + 2}",
        "originInfo":
        f"è¯¦ç»†çš„æŠ€æœ¯åˆ†ææŠ¥å‘Šï¼ŒåŒ…å«{chapter_title}çš„æ ¸å¿ƒæŠ€æœ¯è¦ç‚¹å’Œæ ‡å‡†è§„èŒƒã€‚",
        "title":
        f"GB/T {chapter_index + 1000}-2023 {chapter_title}æŠ€æœ¯æ ‡å‡†.pdf",
        "fileToken":
        f"token_{chapter_index * 3 + 2}_{int(time.time())}",
        "domainId":
        "standard",
        "isFeishuSource":
        None,
        "valid":
        "true",
        "metadata":
        json.dumps(
            {
                "file_name":
                f"GB/T {chapter_index + 1000}-2023 {chapter_title}æŠ€æœ¯æ ‡å‡†.pdf",
                "locations": [{
                    "pagenum": chapter_index + 5
                }],
                "code": f"GB/T {chapter_index + 1000}-2023",
                "gfid": f"GB/T {chapter_index + 1000}-2023",
                "source": "data_platform"
            },
            ensure_ascii=False)
    })

    # ç”Ÿæˆä¹¦ç±ç±»å‹çš„å¼•ç”¨æº
    sources.append({
        "id":
        f"{chapter_index * 3 + 3}",
        "detailId":
        f"{chapter_index * 3 + 3}",
        "originInfo":
        f"æœ€æ–°çš„å­¦æœ¯ç ”ç©¶æˆæœï¼Œä¸º{chapter_title}æä¾›äº†ç†è®ºæ”¯æ’‘å’Œå®è·µæŒ‡å¯¼ã€‚",
        "title":
        f"{chapter_title}æŠ€æœ¯æ‰‹å†Œ.pdf",
        "fileToken":
        f"token_{chapter_index * 3 + 3}_{int(time.time())}",
        "domainId":
        "book",
        "isFeishuSource":
        None,
        "valid":
        "true",
        "metadata":
        json.dumps(
            {
                "file_name": f"{chapter_title}æŠ€æœ¯æ‰‹å†Œ.pdf",
                "locations": [{
                    "pagenum": chapter_index + 10
                }],
                "source": "data_platform"
            },
            ensure_ascii=False)
    })

    return sources


def generate_mock_web_sources(chapter_title: str, chapter_index: int) -> list:
    """ç”Ÿæˆæ¨¡æ‹Ÿçš„ç½‘é¡µå¼•ç”¨æºåˆ—è¡¨"""
    import time

    web_sources = []

    web_sources.append({
        "id":
        f"web_{chapter_index * 2 + 1}",
        "detailId":
        f"web_{chapter_index * 2 + 1}",
        "materialContent":
        f"å…³äº{chapter_title}çš„åœ¨çº¿èµ„æ–™å’Œæœ€æ–°åŠ¨æ€ã€‚åŒ…å«ç›¸å…³æŠ€æœ¯å‘å±•ã€è¡Œä¸šè¶‹åŠ¿å’Œå®é™…åº”ç”¨æ¡ˆä¾‹ã€‚",
        "materialTitle":
        f"{chapter_title}æŠ€æœ¯å‘å±•åŠ¨æ€-æŠ€æœ¯èµ„è®¯",
        "url":
        f"https://example.com/tech/{chapter_title.lower().replace(' ', '-')}",
        "siteName":
        "æŠ€æœ¯èµ„è®¯ç½‘",
        "datePublished":
        "2023å¹´12æœˆ15æ—¥",
        "materialId":
        f"web_{chapter_index * 2 + 1}_{int(time.time())}"
    })

    web_sources.append({
        "id":
        f"web_{chapter_index * 2 + 2}",
        "detailId":
        f"web_{chapter_index * 2 + 2}",
        "materialContent":
        f"{chapter_title}ç›¸å…³çš„ç ”ç©¶æŠ¥å‘Šå’Œè¡Œä¸šåˆ†æã€‚",
        "materialTitle":
        f"{chapter_title}è¡Œä¸šåˆ†ææŠ¥å‘Š-ç ”ç©¶æŠ¥å‘Š",
        "url":
        f"https://research.example.com/report/{chapter_index + 1}",
        "siteName":
        "ç ”ç©¶æŠ¥å‘Šç½‘",
        "datePublished":
        "2023å¹´11æœˆ20æ—¥",
        "materialId":
        f"web_{chapter_index * 2 + 2}_{int(time.time())}"
    })

    return web_sources


# --- å…¶ä»–ç«¯ç‚¹ä¿æŒä¸å˜ ---


@router.get("/health")
async def health_check():
    logger.info("å¥åº·æ£€æŸ¥ç«¯ç‚¹è¢«è°ƒç”¨")
    return {"status": "healthy", "message": "APIæœåŠ¡è¿è¡Œæ­£å¸¸"}


@router.post("/actions/edit", status_code=status.HTTP_200_OK)
async def edit_text(request: EditActionRequest,
                    tool: AIEditingTool = Depends(get_ai_editing_tool)):
    """
    AI æ–‡æœ¬ç¼–è¾‘ç«¯ç‚¹ï¼ˆæµå¼å“åº”ï¼‰

    æ”¯æŒä»¥ä¸‹ç¼–è¾‘æ“ä½œï¼š
    - polish: æ¶¦è‰²æ–‡æœ¬ï¼Œæ”¯æŒå¤šç§é£æ ¼ï¼ˆprofessional, conversational, readable, subtle, academic, literaryï¼‰
    - expand: æ‰©å†™æ–‡æœ¬ï¼Œå¢åŠ æ›´å¤šç»†èŠ‚å’Œæ·±åº¦
    - summarize: ç¼©å†™æ–‡æœ¬ï¼Œæå–å…³é”®è¦ç‚¹
    - continue_writing: ç»­å†™æ–‡æœ¬ï¼ŒåŸºäºä¸Šä¸‹æ–‡ç»§ç»­åˆ›ä½œ
    - custom: è‡ªå®šä¹‰ç¼–è¾‘ï¼Œæ ¹æ®ç”¨æˆ·æŒ‡ä»¤è¿›è¡Œç¼–è¾‘

    è¿”å› Server-Sent Events (SSE) æµå¼å“åº”
    """
    logger.info(f"æ”¶åˆ°æ–‡æœ¬ç¼–è¾‘è¯·æ±‚ï¼Œæ“ä½œç±»å‹: {request.action}")

    async def event_generator():
        """SSE äº‹ä»¶ç”Ÿæˆå™¨"""
        try:
            # å‘é€å¼€å§‹äº‹ä»¶
            yield f"data: {json.dumps({'event': 'start', 'action': request.action}, ensure_ascii=False)}\n\n"

            # è°ƒç”¨ AI ç¼–è¾‘å·¥å…·çš„æµå¼æ–¹æ³•
            async for token in tool.arun(action=request.action,
                                         text=request.text,
                                         command=request.command,
                                         context=request.context,
                                         polish_style=request.polish_style):
                # å°†æ¯ä¸ª token åŒ…è£…æˆ SSE æ ¼å¼
                yield f"data: {json.dumps({'text': token}, ensure_ascii=False)}\n\n"

            # å‘é€ç»“æŸäº‹ä»¶
            yield f"data: {json.dumps({'event': 'end', 'action': request.action}, ensure_ascii=False)}\n\n"

            logger.info(f"æ–‡æœ¬ç¼–è¾‘æµå¼å“åº”å®Œæˆï¼Œæ“ä½œ: {request.action}")

        except ValueError as e:
            # å‘é€é”™è¯¯äº‹ä»¶
            error_data = {'event': 'error', 'error': 'å‚æ•°é”™è¯¯', 'detail': str(e)}
            yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
            logger.error(f"æ–‡æœ¬ç¼–è¾‘å‚æ•°é”™è¯¯: {e}")

        except Exception as e:
            # å‘é€é”™è¯¯äº‹ä»¶
            error_data = {'event': 'error', 'error': 'ç¼–è¾‘å¤±è´¥', 'detail': str(e)}
            yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
            logger.error(f"æ–‡æœ¬ç¼–è¾‘å¤±è´¥: {e}")

    # è¿”å›æµå¼å“åº”
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "Cache-Control",
            "X-Accel-Buffering": "no"  # ç¦ç”¨ Nginx ç¼“å†²
        })
