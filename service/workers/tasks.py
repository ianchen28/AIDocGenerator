import asyncio
import json
from typing import Union

import redis.asyncio as redis
from loguru import logger

# å¯¼å…¥ Celery åº”ç”¨ç¨‹åº
from .celery_app import celery_app


def _get_detailed_progress_message(node_name: str) -> str:
    """
    æ ¹æ®èŠ‚ç‚¹åç§°ç”Ÿæˆè¯¦ç»†çš„è¿›åº¦æ¶ˆæ¯
    
    Args:
        node_name: èŠ‚ç‚¹åç§°
        
    Returns:
        str: è¯¦ç»†çš„è¿›åº¦æ¶ˆæ¯
    """
    progress_messages = {
        "initial_research": "å·²å®Œæˆåˆå§‹ç ”ç©¶é˜¶æ®µï¼Œæ­£åœ¨æ•´ç†æ”¶é›†åˆ°çš„ä¿¡æ¯æºï¼Œä¸ºå¤§çº²ç”Ÿæˆåšå‡†å¤‡...",
        "outline_generation": "å·²å®Œæˆå¤§çº²ç”Ÿæˆé˜¶æ®µï¼Œæ­£åœ¨ä¼˜åŒ–æ–‡æ¡£ç»“æ„å’Œç« èŠ‚å®‰æ’...",
        "planner": "å·²å®Œæˆè®¡åˆ’åˆ¶å®šé˜¶æ®µï¼Œæ­£åœ¨ç¡®å®šç ”ç©¶æ–¹å‘å’Œé‡ç‚¹å†…å®¹...",
        "researcher": "å·²å®Œæˆæ·±å…¥ç ”ç©¶é˜¶æ®µï¼Œæ­£åœ¨åˆ†ææ”¶é›†åˆ°çš„è¯¦ç»†ä¿¡æ¯...",
        "writer": "å·²å®Œæˆå†…å®¹æ’°å†™é˜¶æ®µï¼Œæ­£åœ¨å®Œå–„æ–‡æ¡£å†…å®¹...",
        "reflection": "å·²å®Œæˆåæ€ä¼˜åŒ–é˜¶æ®µï¼Œæ­£åœ¨æ£€æŸ¥å’Œå®Œå–„æ–‡æ¡£è´¨é‡...",
        "supervisor": "å·²å®Œæˆç›‘ç£å†³ç­–é˜¶æ®µï¼Œæ­£åœ¨è¯„ä¼°å½“å‰è¿›åº¦å¹¶ç¡®å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨...",
        "editor": "å·²å®Œæˆç¼–è¾‘ä¼˜åŒ–é˜¶æ®µï¼Œæ­£åœ¨å®Œå–„æ–‡æ¡£æ ¼å¼å’Œå†…å®¹...",
        "generation": "å·²å®Œæˆå†…å®¹ç”Ÿæˆé˜¶æ®µï¼Œæ­£åœ¨åˆ›å»ºæœ€ç»ˆçš„æ–‡æ¡£å†…å®¹...",
        "research": "å·²å®Œæˆç ”ç©¶é˜¶æ®µï¼Œæ­£åœ¨æ•´ç†å’Œåˆ†æç›¸å…³ä¿¡æ¯..."
    }

    return progress_messages.get(node_name, f"å·²å®Œæˆæ­¥éª¤: {node_name}")


# å»¶è¿Ÿå¯¼å…¥containerä»¥é¿å…å¾ªç¯å¯¼å…¥
def get_container():
    """å»¶è¿Ÿå¯¼å…¥containerä»¥é¿å…å¾ªç¯å¯¼å…¥"""
    from doc_agent.core.container import container
    return container


# Redisè¿æ¥ç°åœ¨æ¯æ¬¡éƒ½åˆ›å»ºæ–°çš„ï¼Œé¿å…è¿æ¥è¶…æ—¶é—®é¢˜


async def get_redis_client() -> redis.Redis:
    """è·å–Rediså®¢æˆ·ç«¯å®ä¾‹"""
    try:
        # æ¯æ¬¡éƒ½åˆ›å»ºæ–°çš„è¿æ¥ï¼Œé¿å…è¿æ¥è¶…æ—¶é—®é¢˜
        from doc_agent.core.config import settings
        redis_client = redis.from_url(settings.redis_url,
                                      encoding="utf-8",
                                      decode_responses=True)
        # æµ‹è¯•è¿æ¥
        await redis_client.ping()
        logger.info("Rediså®¢æˆ·ç«¯è¿æ¥æˆåŠŸ")
        return redis_client
    except Exception as e:
        logger.error(f"Redisè¿æ¥å¤±è´¥: {e}")
        raise


@celery_app.task
def generate_outline_from_query_task(job_id: Union[str, int],
                                     task_prompt: str,
                                     is_online: bool = False,
                                     context_files: dict = None,
                                     style_guide_content: str = None,
                                     requirements: str = None,
                                     redis_stream_key: str = None) -> str:
    """
    ä»æŸ¥è¯¢ç”Ÿæˆå¤§çº²çš„å¼‚æ­¥ä»»åŠ¡

    Args:
        job_id: ä½œä¸šID
        task_prompt: ç”¨æˆ·çš„æ ¸å¿ƒæŒ‡ä»¤
        is_online: æ˜¯å¦è°ƒç”¨webæœç´¢
        context_files: ä¸Šä¸‹æ–‡æ–‡ä»¶åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        style_guide_content: é£æ ¼æŒ‡å—å†…å®¹ï¼ˆå¯é€‰ï¼‰
        requirements: ç¼–å†™è¦æ±‚ï¼ˆå¯é€‰ï¼‰
        redis_stream_key: Redisæµkeyï¼ˆå¯é€‰ï¼‰

    Returns:
        ä»»åŠ¡çŠ¶æ€
    """
    logger.info(f"å¤§çº²ç”Ÿæˆä»»åŠ¡å¼€å§‹ - Job ID: {job_id}")

    try:
        # ä½¿ç”¨åŒæ­¥æ–¹å¼è¿è¡Œå¼‚æ­¥å‡½æ•°
        return asyncio.run(
            _generate_outline_from_query_task_async(job_id, task_prompt,
                                                    is_online, context_files,
                                                    style_guide_content,
                                                    requirements,
                                                    redis_stream_key))
    except Exception as e:
        logger.error(f"å¤§çº²ç”Ÿæˆä»»åŠ¡å¤±è´¥: {e}")
        return "FAILED"


async def _generate_outline_from_query_task_async(
        job_id: Union[str, int],
        task_prompt: str,
        is_online: bool = False,
        context_files: dict = None,
        style_guide_content: str = None,
        requirements: str = None,
        redis_stream_key: str = None) -> str:
    """å¼‚æ­¥å¤§çº²ç”Ÿæˆä»»åŠ¡çš„å†…éƒ¨å®ç°"""
    try:
        # è·å–Rediså®¢æˆ·ç«¯å’Œå‘å¸ƒå™¨
        redis = await get_redis_client()
        from doc_agent.core.redis_stream_publisher import RedisStreamPublisher
        publisher = RedisStreamPublisher(redis)

        # å‘å¸ƒä»»åŠ¡å¼€å§‹äº‹ä»¶
        await publisher.publish_task_started(job_id=job_id,
                                             task_type="outline_generation",
                                             task_prompt=task_prompt)

        logger.info(f"Job {job_id}: å¼€å§‹ç”Ÿæˆå¤§çº²ï¼Œä¸»é¢˜: '{task_prompt[:50]}...'")
        logger.info(f"  is_online: {is_online}")
        logger.info(
            f"  context_files: {len(context_files) if context_files else 0} ä¸ªæ–‡ä»¶"
        )
        logger.info(f"  style_guide_content: {bool(style_guide_content)}")
        logger.info(f"  requirements: {bool(requirements)}")
        logger.info(f"  redis_stream_key: {redis_stream_key}")

        # ä½¿ç”¨çœŸæ­£çš„å·¥ä½œæµç”Ÿæˆå¤§çº²
        from doc_agent.core.container import container
        from doc_agent.graph.state import ResearchState
        import uuid

        # åˆ›å»ºåˆå§‹çŠ¶æ€
        run_id = f"run-{uuid.uuid4().hex[:8]}"
        initial_state = ResearchState(
            topic=task_prompt,
            style_guide_content=style_guide_content or "",
            requirements_content=requirements or "",
            initial_sources=[],
            document_outline={},
            chapters_to_process=[],
            current_chapter_index=0,
            current_citation_index=1,
            completed_chapters=[],
            final_document="",
            sources=[],
            all_sources=[],
            cited_sources=[],
            cited_sources_in_chapter=[],
            messages=[],
            run_id=run_id,
        )

        # å‘å¸ƒè¿›åº¦äº‹ä»¶
        await publisher.publish_task_progress(
            job_id=job_id,
            task_type="outline_generation",
            progress="æ­£åœ¨åˆ†ææ‚¨çš„éœ€æ±‚ï¼Œå‡†å¤‡å¼€å§‹å¤§çº²ç”Ÿæˆæµç¨‹...",
            step="analysis")

        # æ‰§è¡ŒçœŸæ­£çš„å·¥ä½œæµ
        outline_result = None
        try:
            logger.info(
                f"ğŸ” [API Task] å¼€å§‹æ‰§è¡Œå·¥ä½œæµï¼Œåˆå§‹çŠ¶æ€: {initial_state.get('topic', 'unknown')}"
            )

            async for step_output in container.outline_graph.astream(
                    initial_state):
                node_name = list(step_output.keys())[0]
                logger.info(f"âœ… [API Task] Finished step: [ {node_name} ]")
                step_result = list(step_output.values())[0]

                # å‘å¸ƒè¿›åº¦äº‹ä»¶
                progress_message = _get_detailed_progress_message(node_name)
                await publisher.publish_task_progress(
                    job_id=job_id,
                    task_type="outline_generation",
                    progress=progress_message,
                    step=node_name)

                logger.info(
                    f"ğŸ” [API Task] Step result keys: {list(step_result.keys()) if step_result else 'None'}"
                )

                if step_result and "document_outline" in step_result:
                    outline_result = step_result.get("document_outline")
                    logger.info(
                        f"âœ… [API Task] æ‰¾åˆ° document_outlineï¼Œç»“æ„: {list(outline_result.keys()) if outline_result else 'None'}"
                    )
                    break
                else:
                    logger.info(f"ğŸ“‹ [API Task] Step {node_name} å®Œæˆï¼Œç­‰å¾…ä¸‹ä¸€ä¸ªæ­¥éª¤...")

        except Exception as e:
            logger.error(f"âŒ [API Task] Error during outline generation: {e}")
            logger.error(
                f"âŒ [API Task] Exception details: {type(e).__name__}: {str(e)}"
            )
            import traceback
            logger.error(f"âŒ [API Task] Traceback: {traceback.format_exc()}")
            raise e

        if not outline_result:
            # å¦‚æœå·¥ä½œæµå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
            logger.warning("âš ï¸  å·¥ä½œæµå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨å¤§çº²ç”Ÿæˆæ–¹æ¡ˆ")
            outline_result = {
                "title":
                f"åŸºäº'{task_prompt}'çš„æŠ€æœ¯æ–‡æ¡£",
                "nodes": [{
                    "id": "node_1",
                    "title": "å¼•è¨€",
                    "content_summary": f"ä»‹ç»{task_prompt}çš„åŸºæœ¬æ¦‚å¿µå’ŒèƒŒæ™¯"
                }, {
                    "id": "node_2",
                    "title": "æŠ€æœ¯åŸç†",
                    "content_summary": f"è¯¦ç»†è§£é‡Š{task_prompt}çš„æ ¸å¿ƒåŸç†"
                }, {
                    "id": "node_3",
                    "title": "åº”ç”¨åœºæ™¯",
                    "content_summary": f"å±•ç¤º{task_prompt}çš„å®é™…åº”ç”¨æ¡ˆä¾‹"
                }]
            }

        # å‘å¸ƒå¤§çº²ç”Ÿæˆå®Œæˆäº‹ä»¶
        await publisher.publish_outline_generated(job_id, outline_result)

        # ä¿å­˜å¤§çº²ç»“æœåˆ°Redis
        outline_json = json.dumps(outline_result, ensure_ascii=False)
        await redis.set(f"job_result:{job_id}", outline_json, ex=3600)  # 1å°æ—¶è¿‡æœŸ

        # å‘å¸ƒä»»åŠ¡å®Œæˆäº‹ä»¶
        await publisher.publish_task_completed(
            job_id=job_id,
            task_type="outline_generation",
            result={"outline": outline_result},
            duration="7s")

        logger.info(f"Job {job_id}: å¤§çº²ç”Ÿæˆå®Œæˆ")

        return "COMPLETED"

    except Exception as e:
        logger.error(f"Job {job_id}: å¤§çº²ç”Ÿæˆå¤±è´¥: {e}")

        # å‘å¸ƒä»»åŠ¡å¤±è´¥äº‹ä»¶
        try:
            await publisher.publish_task_failed(job_id=job_id,
                                                task_type="outline_generation",
                                                error=str(e))
        except Exception as publish_error:
            logger.error(f"å‘å¸ƒå¤±è´¥äº‹ä»¶æ—¶å‡ºé”™: {publish_error}")

        return "FAILED"


@celery_app.task
def generate_document_from_outline_task(job_id: Union[str, int],
                                        outline: dict,
                                        session_id: str = None) -> str:
    """
    ä»å¤§çº²ç”Ÿæˆæ–‡æ¡£çš„å¼‚æ­¥ä»»åŠ¡

    Args:
        job_id: ä½œä¸šID
        outline: ç»“æ„åŒ–çš„å¤§çº²å¯¹è±¡
        session_id: ä¼šè¯IDï¼Œç”¨äºè¿½è¸ª

    Returns:
        ä»»åŠ¡çŠ¶æ€
    """
    logger.info(f"æ–‡æ¡£ç”Ÿæˆä»»åŠ¡å¼€å§‹ - Job ID: {job_id}, Session ID: {session_id}")

    try:
        # ä½¿ç”¨åŒæ­¥æ–¹å¼è¿è¡Œå¼‚æ­¥å‡½æ•°
        return asyncio.run(
            _generate_document_from_outline_task_async(job_id, outline,
                                                       session_id))
    except Exception as e:
        logger.error(f"æ–‡æ¡£ç”Ÿæˆä»»åŠ¡å¤±è´¥: {e}")
        return "FAILED"


async def _generate_document_from_outline_task_async(job_id: Union[str, int],
                                                     outline: dict,
                                                     session_id: str = None
                                                     ) -> str:
    """å¼‚æ­¥æ–‡æ¡£ç”Ÿæˆä»»åŠ¡çš„å†…éƒ¨å®ç°"""
    try:
        # è·å–Rediså®¢æˆ·ç«¯å’Œå‘å¸ƒå™¨
        redis = await get_redis_client()
        from doc_agent.core.redis_stream_publisher import RedisStreamPublisher
        publisher = RedisStreamPublisher(redis)

        # å‘å¸ƒä»»åŠ¡å¼€å§‹äº‹ä»¶
        await publisher.publish_task_started(job_id=job_id,
                                             task_type="document_generation",
                                             outline_title=outline.get(
                                                 "title", "æœªçŸ¥æ ‡é¢˜"))

        logger.info(
            f"Job {job_id}: å¼€å§‹ç”Ÿæˆæ–‡æ¡£ï¼Œå¤§çº²æ ‡é¢˜: '{outline.get('title', 'æœªçŸ¥æ ‡é¢˜')}'")

        # åˆå§‹åŒ–çŠ¶æ€ï¼Œå°†outlineé›†æˆåˆ°äºŒé˜¶æ®µå·¥ä½œæµç¨‹
        from doc_agent.graph.state import ResearchState

        # æ„å»ºåˆå§‹çŠ¶æ€
        initial_state = ResearchState(run_id=str(job_id),
                                      topic=outline.get("title", "æŠ€æœ¯æ–‡æ¡£"),
                                      style_guide_content=None,
                                      requirements_content=None,
                                      initial_sources=[],
                                      document_outline=outline,
                                      chapters_to_process=[],
                                      current_chapter_index=0,
                                      completed_chapters=[],
                                      final_document="",
                                      research_plan="",
                                      search_queries=[],
                                      gathered_sources=[],
                                      sources=[],
                                      all_sources=[],
                                      current_citation_index=1,
                                      cited_sources=[],
                                      cited_sources_in_chapter=[],
                                      messages=[])

        # ä»outlineä¸­æå–ç« èŠ‚ä¿¡æ¯
        chapters = []
        for i, node in enumerate(outline.get("nodes", [])):
            chapter_info = {
                "chapter_title": node.get("title", f"ç« èŠ‚ {i+1}"),
                "description": node.get("content_summary", ""),
                "node_id": node.get("id", f"node_{i+1}"),
                "children": node.get("children", [])
            }
            chapters.append(chapter_info)

        initial_state["chapters_to_process"] = chapters
        logger.info(f"Job {job_id}: æå–åˆ° {len(chapters)} ä¸ªç« èŠ‚")

        # å‘å¸ƒè¿›åº¦äº‹ä»¶
        await publisher.publish_task_progress(job_id=job_id,
                                              task_type="document_generation",
                                              progress="æ­£åœ¨åˆ†æå¤§çº²ç»“æ„",
                                              step="analysis")

        # è·å–å®¹å™¨å’Œç« èŠ‚å·¥ä½œæµå›¾
        container = get_container()
        chapter_workflow = container.chapter_graph

        if not chapter_workflow:
            logger.error(f"Job {job_id}: ç« èŠ‚å·¥ä½œæµå›¾æœªæ‰¾åˆ°")
            raise Exception("ç« èŠ‚å·¥ä½œæµå›¾æœªåˆå§‹åŒ–")

        # å¼€å§‹å¤„ç†æ¯ä¸ªç« èŠ‚
        completed_chapters = []
        all_sources = []
        cited_sources = []

        for chapter_index, chapter in enumerate(chapters):
            logger.info(
                f"Job {job_id}: å¼€å§‹å¤„ç†ç« èŠ‚ {chapter_index + 1}/{len(chapters)}: {chapter['chapter_title']}"
            )

            # æ›´æ–°å½“å‰ç« èŠ‚ç´¢å¼•
            current_state = initial_state.copy()
            current_state["current_chapter_index"] = chapter_index
            current_state["chapters_to_process"] = chapters
            current_state["completed_chapters"] = completed_chapters
            current_state["all_sources"] = all_sources
            current_state["cited_sources"] = cited_sources

            # å‘å¸ƒç« èŠ‚å¤„ç†è¿›åº¦
            await publisher.publish_task_progress(
                job_id=job_id,
                task_type="document_generation",
                progress=f"æ­£åœ¨å¤„ç†ç« èŠ‚: {chapter['chapter_title']}",
                step=f"chapter_{chapter_index + 1}")

            try:
                # æ‰§è¡Œç« èŠ‚å·¥ä½œæµ
                result = await chapter_workflow.ainvoke(current_state)

                # æå–ç« èŠ‚ç»“æœ
                if "final_document" in result:
                    chapter_content = result["final_document"]
                else:
                    chapter_content = f"ç« èŠ‚ {chapter['chapter_title']} çš„å†…å®¹..."

                # æ”¶é›†å¼•ç”¨æº
                chapter_sources = result.get("cited_sources_in_chapter", [])
                all_sources.extend(chapter_sources)
                cited_sources.extend(chapter_sources)

                # ä¿å­˜ç« èŠ‚ç»“æœ
                completed_chapter = {
                    "title": chapter["chapter_title"],
                    "content": chapter_content,
                    "sources": chapter_sources,
                    "chapter_index": chapter_index
                }
                completed_chapters.append(completed_chapter)

                logger.info(
                    f"Job {job_id}: ç« èŠ‚ {chapter['chapter_title']} å¤„ç†å®Œæˆ")

            except Exception as chapter_error:
                logger.error(
                    f"Job {job_id}: ç« èŠ‚ {chapter['chapter_title']} å¤„ç†å¤±è´¥: {chapter_error}"
                )
                # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªç« èŠ‚
                continue

        # åˆå¹¶æ‰€æœ‰ç« èŠ‚å†…å®¹
        final_document_parts = []
        final_document_parts.append(f"# {outline.get('title', 'æŠ€æœ¯æ–‡æ¡£')}\n\n")

        for chapter in completed_chapters:
            final_document_parts.append(f"## {chapter['title']}\n\n")
            final_document_parts.append(f"{chapter['content']}\n\n")

        final_document = "".join(final_document_parts)

        # ç”Ÿæˆæœ€ç»ˆæ–‡æ¡£ç»“æœ
        document_result = {
            "title": outline.get("title", "æŠ€æœ¯æ–‡æ¡£"),
            "content": final_document,
            "word_count": len(final_document.split()),
            "char_count": len(final_document),
            "sections": len(completed_chapters),
            "generated_at": str(asyncio.get_event_loop().time()),
            "chapters": completed_chapters,
            "sources": all_sources
        }

        # å‘å¸ƒæ–‡æ¡£ç”Ÿæˆå®Œæˆäº‹ä»¶
        await publisher.publish_document_generated(job_id, document_result)

        # ä¿å­˜æ–‡æ¡£ç»“æœåˆ°Redis
        document_json = json.dumps(document_result, ensure_ascii=False)
        await redis.set(f"job_result:{job_id}", document_json,
                        ex=3600)  # 1å°æ—¶è¿‡æœŸ

        # å‘å¸ƒä»»åŠ¡å®Œæˆäº‹ä»¶
        await publisher.publish_task_completed(
            job_id=job_id,
            task_type="document_generation",
            result={"document": document_result},
            duration="completed")

        logger.info(f"Job {job_id}: æ–‡æ¡£ç”Ÿæˆå®Œæˆï¼Œå…±å¤„ç† {len(completed_chapters)} ä¸ªç« èŠ‚")

        return "COMPLETED"

    except Exception as e:
        logger.error(f"Job {job_id}: æ–‡æ¡£ç”Ÿæˆå¤±è´¥: {e}")

        # å‘å¸ƒä»»åŠ¡å¤±è´¥äº‹ä»¶
        try:
            await publisher.publish_task_failed(
                job_id=job_id, task_type="document_generation", error=str(e))
        except Exception as publish_error:
            logger.error(f"å‘å¸ƒå¤±è´¥äº‹ä»¶æ—¶å‡ºé”™: {publish_error}")

        return "FAILED"


@celery_app.task
def get_job_status(job_id: Union[str, int]) -> dict:
    """
    è·å–ä»»åŠ¡çŠ¶æ€

    Args:
        job_id: ä»»åŠ¡ID

    Returns:
        ä»»åŠ¡çŠ¶æ€å­—å…¸
    """
    try:
        # ä½¿ç”¨åŒæ­¥æ–¹å¼è¿è¡Œå¼‚æ­¥å‡½æ•°
        return asyncio.run(_get_job_status_async(job_id))
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
        return {"status": "error", "error": str(e)}


async def _get_job_status_async(job_id: Union[str, int]) -> dict:
    """å¼‚æ­¥è·å–ä»»åŠ¡çŠ¶æ€çš„å†…éƒ¨å®ç°"""
    try:
        redis = await get_redis_client()
        job_data = await redis.hgetall(f"job:{job_id}")

        if not job_data:
            return {"status": "not_found"}

        return job_data

    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
        return {"status": "error", "error": str(e)}


# Celery ä»»åŠ¡å¯¼å…¥
from .celery_app import celery_app


@celery_app.task
def run_main_workflow(job_id: str, topic: str, genre: str = "default") -> str:
    """
    ä¸»è¦çš„å¼‚æ­¥å·¥ä½œæµå‡½æ•°
    ä½¿ç”¨çœŸå®çš„å›¾æ‰§è¡Œå™¨å’ŒRediså›è°ƒå¤„ç†å™¨

    Args:
        job_id: ä»»åŠ¡ID
        topic: æ–‡æ¡£ä¸»é¢˜
        genre: æ–‡æ¡£ç±»å‹ï¼Œç”¨äºé€‰æ‹©ç›¸åº”çš„promptç­–ç•¥

    Returns:
        ä»»åŠ¡çŠ¶æ€
    """
    logger.info(f"ä¸»å·¥ä½œæµå¼€å§‹ - Job ID: {job_id}, Topic: {topic}, Genre: {genre}")

    try:
        # ä½¿ç”¨åŒæ­¥æ–¹å¼è¿è¡Œå¼‚æ­¥å‡½æ•°
        return asyncio.run(_run_main_workflow_async(job_id, topic, genre))
    except Exception as e:
        logger.error(f"ä¸»å·¥ä½œæµä»»åŠ¡å¤±è´¥: {e}")
        return "FAILED"


async def _run_main_workflow_async(job_id: str,
                                   topic: str,
                                   genre: str = "default") -> str:
    """å¼‚æ­¥ä¸»å·¥ä½œæµä»»åŠ¡çš„å†…éƒ¨å®ç°"""
    try:
        # è·å–Rediså®¢æˆ·ç«¯
        redis = await get_redis_client()

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿›è¡Œä¸­
        await redis.hset(f"job:{job_id}",
                         mapping={
                             "status": "processing",
                             "topic": topic,
                             "genre": genre,
                             "started_at": str(asyncio.get_event_loop().time())
                         })

        # è·å–å¸¦æœ‰Rediså›è°ƒå¤„ç†å™¨çš„å›¾æ‰§è¡Œå™¨
        logger.info(f"Job {job_id}: è·å–å›¾æ‰§è¡Œå™¨...")
        container = get_container()
        runnable = container.get_graph_runnable_for_job(job_id, genre)

        # åˆ›å»ºåˆå§‹çŠ¶æ€
        from doc_agent.graph.state import ResearchState
        import uuid

        run_id = f"run-{uuid.uuid4().hex[:8]}"
        initial_state = ResearchState(
            topic=topic,
            style_guide_content="",
            requirements_content="",
            initial_sources=[],
            document_outline={},
            chapters_to_process=[],
            current_chapter_index=0,
            current_citation_index=1,
            completed_chapters=[],
            final_document="",
            sources=[],
            all_sources=[],
            cited_sources=[],
            cited_sources_in_chapter=[],
            messages=[],
            run_id=run_id,
        )

        logger.info(f"Job {job_id}: å¼€å§‹æ‰§è¡Œå·¥ä½œæµ...")

        # æ‰§è¡Œå·¥ä½œæµ
        async for step_output in runnable.astream(initial_state):
            node_name = list(step_output.keys())[0]
            logger.info(f"Job {job_id}: å®Œæˆæ­¥éª¤ [{node_name}]")

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
        await redis.hset(f"job:{job_id}",
                         mapping={
                             "status": "completed",
                             "completed_at":
                             str(asyncio.get_event_loop().time())
                         })

        logger.info(f"Job {job_id}: å·¥ä½œæµæ‰§è¡Œå®Œæˆ")
        return "COMPLETED"

    except Exception as e:
        logger.error(f"Job {job_id}: å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
        try:
            await redis.hset(f"job:{job_id}",
                             mapping={
                                 "status": "failed",
                                 "error": str(e),
                                 "failed_at":
                                 str(asyncio.get_event_loop().time())
                             })
        except Exception as update_error:
            logger.error(f"æ›´æ–°ä»»åŠ¡çŠ¶æ€å¤±è´¥: {update_error}")

        return "FAILED"


@celery_app.task
def test_celery_task(message: str) -> str:
    """
    æµ‹è¯• Celery ä»»åŠ¡
    
    Args:
        message: æµ‹è¯•æ¶ˆæ¯
        
    Returns:
        str: è¿”å›çš„æ¶ˆæ¯
    """
    logger.info(f"æ‰§è¡Œ Celery æµ‹è¯•ä»»åŠ¡: {message}")
    return f"Celery ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ: {message}"


@celery_app.task
def generate_document_celery(job_id: Union[str, int], topic: str) -> dict:
    """
    ä½¿ç”¨ Celery æ‰§è¡Œæ–‡æ¡£ç”Ÿæˆä»»åŠ¡
    
    Args:
        job_id: ä½œä¸šID
        topic: æ–‡æ¡£ä¸»é¢˜
        
    Returns:
        dict: æ‰§è¡Œç»“æœ
    """
    logger.info(f"å¼€å§‹ Celery æ–‡æ¡£ç”Ÿæˆä»»åŠ¡: {job_id}, ä¸»é¢˜: {topic}")

    try:
        # è¿™é‡Œå¯ä»¥è°ƒç”¨ç°æœ‰çš„æ–‡æ¡£ç”Ÿæˆé€»è¾‘
        # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿç»“æœ
        result = {
            "job_id": job_id,
            "status": "completed",
            "topic": topic,
            "document_length": 5000,
            "chapters": 5
        }

        logger.info(f"Celery æ–‡æ¡£ç”Ÿæˆä»»åŠ¡å®Œæˆ: {job_id}")
        return result

    except Exception as e:
        logger.error(f"Celery æ–‡æ¡£ç”Ÿæˆä»»åŠ¡å¤±è´¥: {job_id}, é”™è¯¯: {e}")
        return {"job_id": job_id, "status": "failed", "error": str(e)}


@celery_app.task
def process_files_task(context_id: str, files: list[dict]) -> str:
    """
    å¤„ç†ä¸Šä¼ æ–‡ä»¶çš„å¼‚æ­¥ä»»åŠ¡
    
    Args:
        context_id: ä¸Šä¸‹æ–‡ID
        files: æ–‡ä»¶åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡ä»¶åŒ…å« file_id, file_name, storage_url, file_type
        
    Returns:
        ä»»åŠ¡çŠ¶æ€
    """
    logger.info(f"æ–‡ä»¶å¤„ç†ä»»åŠ¡å¼€å§‹ - Context ID: {context_id}, æ–‡ä»¶æ•°é‡: {len(files)}")

    try:
        # ä½¿ç”¨åŒæ­¥æ–¹å¼è¿è¡Œå¼‚æ­¥å‡½æ•°
        return asyncio.run(_process_files_task_async(context_id, files))
    except Exception as e:
        logger.error(f"æ–‡ä»¶å¤„ç†ä»»åŠ¡å¤±è´¥: {e}")
        return "FAILED"


async def _process_files_task_async(context_id: str, files: list[dict]) -> str:
    """å¼‚æ­¥æ–‡ä»¶å¤„ç†ä»»åŠ¡çš„å†…éƒ¨å®ç°"""
    try:
        # è·å–Rediså®¢æˆ·ç«¯
        redis_client = await get_redis_client()

        # åˆå§‹åŒ–å†…å®¹åˆ—è¡¨
        style_contents = []
        requirements_contents = []

        # éå†æ‰€æœ‰æ–‡ä»¶
        for file_info in files:
            file_id = file_info.get("file_id")
            file_name = file_info.get("file_name")
            storage_url = file_info.get("storage_url")
            file_type = file_info.get("file_type", "content")  # é»˜è®¤ä¸ºcontent

            logger.info(f"å¤„ç†æ–‡ä»¶: {file_name} (ç±»å‹: {file_type})")

            if file_type == "content":
                # ä¿æŒç°æœ‰çš„å‘é‡ç´¢å¼•é€»è¾‘
                logger.info(f"å¤„ç†å†…å®¹æ–‡ä»¶: {file_name}")
                # TODO: å®ç°å‘é‡ç´¢å¼•é€»è¾‘
                # è¿™é‡Œåº”è¯¥è°ƒç”¨ç°æœ‰çš„å‘é‡ç´¢å¼•å¤„ç†å‡½æ•°

            elif file_type == "style":
                # è¯»å–æ ·å¼æŒ‡å—å†…å®¹
                logger.info(f"å¤„ç†æ ·å¼æŒ‡å—æ–‡ä»¶: {file_name}")
                try:
                    content = read_file_content(storage_url)
                    style_contents.append(content)
                    logger.info(f"æ ·å¼æŒ‡å—æ–‡ä»¶å¤„ç†å®Œæˆ: {file_name}")
                except Exception as e:
                    logger.error(f"å¤„ç†æ ·å¼æŒ‡å—æ–‡ä»¶å¤±è´¥: {file_name}, é”™è¯¯: {e}")

            elif file_type == "requirements":
                # è¯»å–éœ€æ±‚æ–‡æ¡£å†…å®¹
                logger.info(f"å¤„ç†éœ€æ±‚æ–‡æ¡£æ–‡ä»¶: {file_name}")
                try:
                    content = read_file_content(storage_url)
                    requirements_contents.append(content)
                    logger.info(f"éœ€æ±‚æ–‡æ¡£æ–‡ä»¶å¤„ç†å®Œæˆ: {file_name}")
                except Exception as e:
                    logger.error(f"å¤„ç†éœ€æ±‚æ–‡æ¡£æ–‡ä»¶å¤±è´¥: {file_name}, é”™è¯¯: {e}")

            else:
                logger.warning(f"æœªçŸ¥æ–‡ä»¶ç±»å‹: {file_type}, æ–‡ä»¶: {file_name}")

        # å­˜å‚¨æ ·å¼æŒ‡å—å†…å®¹åˆ°Redis
        if style_contents:
            style_guide_content = "\n\n".join(style_contents)
            await redis_client.hset(f"context:{context_id}",
                                    "style_guide_content", style_guide_content)
            logger.info(f"æ ·å¼æŒ‡å—å†…å®¹å·²å­˜å‚¨åˆ°Redis, é•¿åº¦: {len(style_guide_content)} å­—ç¬¦")

        # å­˜å‚¨éœ€æ±‚æ–‡æ¡£å†…å®¹åˆ°Redis
        if requirements_contents:
            requirements_content = "\n\n".join(requirements_contents)
            await redis_client.hset(f"context:{context_id}",
                                    "requirements_content",
                                    requirements_content)
            logger.info(f"éœ€æ±‚æ–‡æ¡£å†…å®¹å·²å­˜å‚¨åˆ°Redis, é•¿åº¦: {len(requirements_content)} å­—ç¬¦")

        # æ›´æ–°ä¸Šä¸‹æ–‡çŠ¶æ€ä¸ºå°±ç»ª
        await redis_client.hset(f"context:{context_id}", "status", "READY")

        logger.info(f"æ–‡ä»¶å¤„ç†ä»»åŠ¡å®Œæˆ - Context ID: {context_id}")
        return "SUCCESS"

    except Exception as e:
        logger.error(f"æ–‡ä»¶å¤„ç†ä»»åŠ¡å¤±è´¥ - Context ID: {context_id}, é”™è¯¯: {e}")
        return "FAILED"


def read_file_content(storage_url: str) -> str:
    """
    è¯»å–æ–‡ä»¶å†…å®¹çš„å·¥å…·å‡½æ•°
    
    Args:
        storage_url: æ–‡ä»¶å­˜å‚¨URL
        
    Returns:
        æ–‡ä»¶å†…å®¹å­—ç¬¦ä¸²
    """
    # TODO: å®ç°å®é™…çš„æ–‡ä»¶è¯»å–é€»è¾‘
    # è¿™é‡Œåº”è¯¥æ ¹æ®storage_urlè¯»å–æ–‡ä»¶å†…å®¹
    # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿå†…å®¹
    logger.info(f"è¯»å–æ–‡ä»¶å†…å®¹: {storage_url}")
    return f"æ¨¡æ‹Ÿæ–‡ä»¶å†…å®¹ - {storage_url}"
