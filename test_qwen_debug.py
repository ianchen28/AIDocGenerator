#!/usr/bin/env python3
"""
æµ‹è¯• Qwen LLM çš„ 400 é”™è¯¯è¯Šæ–­
"""

import asyncio
import sys
import os

# æ·»åŠ  service ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'service'))

from service.core.env_loader import setup_environment
from service.src.doc_agent.llm_clients import get_llm_client
from service.core.config import settings


def test_qwen_connection():
    """æµ‹è¯• Qwen LLM è¿æ¥"""
    print("ğŸ” å¼€å§‹è¯Šæ–­ Qwen LLM è¿æ¥é—®é¢˜...")

    # è®¾ç½®ç¯å¢ƒ
    setup_environment()

    # è·å–æ¨¡å‹é…ç½®
    model_config = settings.get_model_config("qwen_2_5_235b_a22b")
    print(f"ğŸ“‹ æ¨¡å‹é…ç½®:")
    print(f"   URL: {model_config.url}")
    print(f"   Model ID: {model_config.model_id}")
    print(f"   API Key: {model_config.api_key}")
    print(f"   Reasoning: {model_config.reasoning}")

    # åˆ›å»ºå®¢æˆ·ç«¯
    try:
        llm_client = get_llm_client("qwen_2_5_235b_a22b")
        print("âœ… LLM å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ LLM å®¢æˆ·ç«¯åˆ›å»ºå¤±è´¥: {e}")
        return

    # æµ‹è¯•ç®€å•è¯·æ±‚
    simple_prompt = "ä½ å¥½ï¼Œè¯·å›ç­”ï¼š1+1ç­‰äºå‡ ï¼Ÿ"
    print(f"\nğŸ§ª æµ‹è¯•ç®€å•è¯·æ±‚:")
    print(f"   Prompt: {simple_prompt}")
    print(f"   Prompt é•¿åº¦: {len(simple_prompt)} å­—ç¬¦")

    try:
        response = llm_client.invoke(simple_prompt,
                                     temperature=0,
                                     max_tokens=10)
        print(f"âœ… ç®€å•è¯·æ±‚æˆåŠŸ: {response}")
    except Exception as e:
        print(f"âŒ ç®€å•è¯·æ±‚å¤±è´¥: {e}")
        return

    # æµ‹è¯•ä¸­ç­‰é•¿åº¦è¯·æ±‚
    medium_prompt = "è¯·è¯¦ç»†ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½åœ¨ä¸­å›½ç”µåŠ›è¡Œä¸šçš„åº”ç”¨è¶‹åŠ¿ï¼ŒåŒ…æ‹¬æŠ€æœ¯å‘å±•ã€æ”¿ç­–æ”¯æŒã€å®é™…æ¡ˆä¾‹ç­‰æ–¹é¢ã€‚" * 10
    print(f"\nğŸ§ª æµ‹è¯•ä¸­ç­‰é•¿åº¦è¯·æ±‚:")
    print(f"   Prompt é•¿åº¦: {len(medium_prompt)} å­—ç¬¦")

    try:
        response = llm_client.invoke(medium_prompt,
                                     temperature=0,
                                     max_tokens=50)
        print(f"âœ… ä¸­ç­‰é•¿åº¦è¯·æ±‚æˆåŠŸ: {response}")
    except Exception as e:
        print(f"âŒ ä¸­ç­‰é•¿åº¦è¯·æ±‚å¤±è´¥: {e}")
        return

    # æµ‹è¯•é•¿è¯·æ±‚ï¼ˆæ¨¡æ‹Ÿ supervisor_router çš„ promptï¼‰
    long_prompt = "**è§’è‰²ï¼š** ä½ æ˜¯ä¸€ä½ç ”ç©¶ä¸»ç®¡ï¼Œéœ€è¦åˆ¤æ–­ä¸‹æ–¹èµ„æ–™æ˜¯å¦è¶³å¤Ÿæ’°å†™å®Œæ•´æ–‡æ¡£ã€‚\n\n**ä¸»é¢˜ï¼š**ã€Œäººå·¥æ™ºèƒ½åœ¨ä¸­å›½ç”µåŠ›è¡Œä¸šçš„åº”ç”¨è¶‹åŠ¿å’Œæ”¿ç­–æ”¯æŒã€\n\n**å·²æ”¶é›†çš„ç ”ç©¶èµ„æ–™ï¼š**" + "è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„ç ”ç©¶èµ„æ–™å†…å®¹ã€‚" * 1000
    print(f"\nğŸ§ª æµ‹è¯•é•¿è¯·æ±‚:")
    print(f"   Prompt é•¿åº¦: {len(long_prompt)} å­—ç¬¦")

    try:
        response = llm_client.invoke(long_prompt, temperature=0, max_tokens=10)
        print(f"âœ… é•¿è¯·æ±‚æˆåŠŸ: {response}")
    except Exception as e:
        print(f"âŒ é•¿è¯·æ±‚å¤±è´¥: {e}")
        print(f"ğŸ’¡ å»ºè®®: å¯èƒ½æ˜¯ prompt é•¿åº¦è¶…è¿‡äº†æ¨¡å‹çš„æœ€å¤§è¾“å…¥é™åˆ¶")
        return

    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")


if __name__ == "__main__":
    test_qwen_connection()
